{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT STATEMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.3)\n",
      "Requirement already satisfied: sklearn in ./.local/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: xgboost in ./.local/lib/python3.6/site-packages (1.3.1)\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.6/site-packages (3.3.3)\n",
      "Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0+nv)\n",
      "Requirement already satisfied: keras==2.2.4 in ./.local/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: fastai in ./.local/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (2.8.1)\n",
      "Requirement already satisfied: scikit-optimize in ./.local/lib/python3.6/site-packages (0.8.1)\n",
      "Requirement already satisfied: scikit-learn==0.21 in ./.local/lib/python3.6/site-packages (0.21.0)\n",
      "Requirement already satisfied: graphviz in ./.local/lib/python3.6/site-packages (0.16)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./.local/lib/python3.6/site-packages (from pandas) (2020.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.local/lib/python3.6/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.local/lib/python3.6/site-packages (from matplotlib) (8.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in ./.local/lib/python3.6/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.11.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.14.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.9.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.27.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.34.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.9.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (5.3.1)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in ./.local/lib/python3.6/site-packages (from fastai) (1.0.0)\n",
      "Requirement already satisfied: torch<1.8,>=1.7.0 in ./.local/lib/python3.6/site-packages (from fastai) (1.7.1)\n",
      "Requirement already satisfied: torchvision<0.9,>=0.8 in ./.local/lib/python3.6/site-packages (from fastai) (0.8.2)\n",
      "Requirement already satisfied: spacy in ./.local/lib/python3.6/site-packages (from fastai) (2.3.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.23.0)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from fastai) (20.0.2)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.6/site-packages (from fastai) (20.8)\n",
      "Requirement already satisfied: fastcore<1.4,>=1.3.8 in ./.local/lib/python3.6/site-packages (from fastai) (1.3.18)\n",
      "Requirement already satisfied: pyaml>=16.9 in ./.local/lib/python3.6/site-packages (from scikit-optimize) (20.4.0)\n",
      "Requirement already satisfied: joblib>=0.11 in ./.local/lib/python3.6/site-packages (from scikit-optimize) (1.0.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.11.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (46.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in ./.local/lib/python3.6/site-packages (from torch<1.8,>=1.7.0->fastai) (0.8)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.6/site-packages (from torch<1.8,>=1.7.0->fastai) (3.7.4.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (1.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (2.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (7.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (4.43.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (0.7.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (0.8.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (1.0.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.25.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->fastai) (1.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!TMPDIR=/home/mgill/ pip install --cache-dir=/home/mgill/ --build /home/mgill/ pandas numpy sklearn xgboost matplotlib tensorflow==2.1.0 keras==2.2.4 fastai python-dateutil scikit-optimize scikit-learn==0.21 graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import tensorflow as tf; print(tf.__version__)\n",
    "import keras; print(keras.__version__)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from random import randint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Activation\n",
    "from math import sqrt\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import skopt\n",
    "from skopt.searchcv import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import interp\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_prep_data(tt_file, ho_file):\n",
    "    imp = SimpleImputer(missing_values='./.', strategy='most_frequent')\n",
    "    my_list = []\n",
    "    x = 0 \n",
    "    for chunk in pd.read_csv(tt_file, chunksize=10000, index_col=\"Unnamed: 0\"):\n",
    "        x=x+10000\n",
    "        chunk = chunk.T\n",
    "        if 'Value' in chunk.columns:\n",
    "            #does the selecting of pheno array for application ML\n",
    "            chunk[\"Value\"] = pd.to_numeric(chunk[\"Value\"], downcast=\"float\")\n",
    "            tt_pheno = chunk[\"Value\"].to_numpy()\n",
    "            #reshapes it so its not a 1D array\n",
    "            print(tt_pheno.shape)\n",
    "            tt_pheno = np.reshape(tt_pheno,(len(tt_pheno),1))\n",
    "            print(tt_pheno.shape)\n",
    "            chunk = chunk.drop(columns=['Value'])\n",
    "        headers = chunk.columns\n",
    "        row_idx = chunk.index\n",
    "        chunk = imp.fit_transform(chunk) #SHOULD TURN ./. into the most common for each column\n",
    "        #since imputing makes a numpy array have to turn back into PD for label encoding\n",
    "        chunk = pd.DataFrame(data = chunk, index = row_idx, columns = headers)\n",
    "        my_list.append(chunk)\n",
    "        print(x)\n",
    "    tt_vcf = pd.concat(my_list, axis = 1)\n",
    "    my_list = []\n",
    "    x=0\n",
    "    for chunk in pd.read_csv(ho_file, chunksize=10000, index_col=\"Unnamed: 0\"):\n",
    "        x=x+10000\n",
    "        chunk = chunk.T\n",
    "        if 'Value' in chunk.columns:\n",
    "            #does the selecting of pheno array for application ML\n",
    "            chunk[\"Value\"] = pd.to_numeric(chunk[\"Value\"], downcast=\"float\")\n",
    "            ho_pheno = chunk[\"Value\"].to_numpy()\n",
    "            #reshapes it so its not a 1D array\n",
    "            print(ho_pheno.shape)\n",
    "            ho_pheno = np.reshape(ho_pheno,(len(ho_pheno),1))\n",
    "            print(ho_pheno.shape)\n",
    "            chunk = chunk.drop(columns=['Value'])\n",
    "        headers = chunk.columns\n",
    "        row_idx = chunk.index\n",
    "        chunk = imp.fit_transform(chunk) #SHOULD TURN ./. into the most common for each column\n",
    "        #since imputing makes a numpy array have to turn back into PD for label encoding\n",
    "        chunk = pd.DataFrame(data = chunk, index = row_idx, columns = headers)\n",
    "        my_list.append(chunk)\n",
    "        print(x)\n",
    "    ho_vcf = pd.concat(my_list, axis = 1)\n",
    "    print(tt_vcf.shape)\n",
    "    print(ho_vcf.shape)\n",
    "    print(tt_pheno.shape)\n",
    "            \n",
    "    return tt_vcf, ho_vcf, tt_pheno, ho_pheno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "(617,)\n",
      "(617, 1)\n",
      "60000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "(155,)\n",
      "(155, 1)\n",
      "60000\n",
      "(617, 58574)\n",
      "(155, 58574)\n",
      "(617, 1)\n",
      "0.0\n",
      "(617, 1)\n",
      "(155, 1)\n"
     ]
    }
   ],
   "source": [
    "tt_vcf, ho_vcf, tt_pheno, ho_pheno = new_prep_data(\"PoC_Merged_filtered.csv_train_testQTL_SNPS.csv\", \"PoC_Merged_filtered.csv_holdoutQTL_SNPS.csv\")\n",
    "r_t = tt_pheno.ravel()\n",
    "r_h = ho_pheno.ravel()\n",
    "print(r_t[10])\n",
    "i = 0\n",
    "for x in r_t:\n",
    "    if(x==0.5):\n",
    "        r_t[i]=2.0\n",
    "    i = i+1\n",
    "i = 0\n",
    "for x in r_h:\n",
    "    if(x==0.5):\n",
    "        r_h[i]=2.0\n",
    "    i = i+1\n",
    "r_t = np.reshape(r_t,(len(r_t),1))\n",
    "r_h = np.reshape(r_h,(len(r_h),1))\n",
    "tt_pheno = r_t\n",
    "ho_pheno = r_h\n",
    "print(tt_pheno.shape)\n",
    "print(ho_pheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if it hasn't been run and saved before\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "ohe = ohe.fit(tt_vcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ohe, open(\"PoC_QTL_ohe.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if need or have new holdout data etc.\n",
    "ohe = pickle.load(open(\"PoC_QTL_ohe.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 58574)\n",
      "(617, 170396)\n",
      "(155, 58574)\n",
      "(155, 170396)\n"
     ]
    }
   ],
   "source": [
    "print(tt_vcf.shape)\n",
    "tt_vcf = ohe.transform(tt_vcf)\n",
    "print(tt_vcf.shape)\n",
    "print(ho_vcf.shape)\n",
    "ho_vcf = ohe.transform(ho_vcf)\n",
    "print(ho_vcf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_snp_from_header(ohe,snp_num):\n",
    "    count = 0\n",
    "    snp = \"Not found\"\n",
    "    found = False\n",
    "    i = 0\n",
    "    while i < len(ohe.categories_) and (found == False):\n",
    "        j = 0\n",
    "        while j < len(ohe.categories_[i]):\n",
    "            if(count == snp_num):\n",
    "                snp = ohe.categories_[i][j]\n",
    "                found = True\n",
    "                break\n",
    "            count = count + 1\n",
    "            j = j + 1\n",
    "        i = i + 1\n",
    "    return snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C/C\n"
     ]
    }
   ],
   "source": [
    "## TESTING IF IT WORKS\n",
    "my_snp = find_snp_from_header(ohe, 62793)\n",
    "print(my_snp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 170396)\n",
      "(617, 1)\n",
      "(124, 170396)\n",
      "seed is 3336\n"
     ]
    }
   ],
   "source": [
    "##ONLY NEED TO DO IF OPTIMISING\n",
    "\n",
    "print(tt_vcf.shape)\n",
    "print(tt_pheno.shape)\n",
    "seed = randint(0,5000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tt_vcf, tt_pheno, test_size=0.2, random_state=seed)\n",
    "print(X_test.shape)\n",
    "print(\"seed is \" + str(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "space ={'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "        'min_child_weight': Integer(0, 10),\n",
    "        'max_depth': Integer(0, 50),\n",
    "        'max_delta_step': Integer(0, 20),\n",
    "        'subsample': Real(0.01, 1.0, 'uniform'),\n",
    "        'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n",
    "        'colsample_bylevel': Real(0.01, 1.0, 'uniform'),\n",
    "        'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n",
    "        'gamma': Real(1e-9, 0.5, 'log-uniform'),\n",
    "        'min_child_weight': Integer(0, 5),\n",
    "        'n_estimators': Integer(50, 200),\n",
    "        'scale_pos_weight': Real(1e-6, 500, 'log-uniform')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_step(optim_result):\n",
    "    \"\"\"\n",
    "    Callback meant to view scores after\n",
    "    each iteration while performing Bayesian\n",
    "    Optimization in Skopt\"\"\"\n",
    "    score = xgb_bayes_search.best_score_\n",
    "    print(\"best score: %s\" % score)\n",
    "    if score >= 0.98:\n",
    "        print('Interrupting!')\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/mgill/.local/lib/python3.6/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:15:37] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:15:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216, score=0.880, total=  44.2s\n",
      "[CV] colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   44.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:16:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:16:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216, score=0.818, total=  41.3s\n",
      "[CV] colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:17:03] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:17:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216, score=0.828, total=  40.8s\n",
      "[CV] colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216 \n",
      "[09:17:43] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:17:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216, score=0.837, total=  38.0s\n",
      "[CV] colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216 \n",
      "[09:18:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:18:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216, score=0.825, total=  44.1s\n",
      "[CV] colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859 \n",
      "[09:19:06] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:19:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859, score=0.910, total=  53.1s\n",
      "[CV] colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859 \n",
      "[09:19:59] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:20:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859, score=0.838, total=  52.9s\n",
      "[CV] colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859 \n",
      "[09:20:52] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:20:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859, score=0.889, total=  52.0s\n",
      "[CV] colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859 \n",
      "[09:21:44] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:21:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859, score=0.837, total=  51.9s\n",
      "[CV] colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859 \n",
      "[09:22:36] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:22:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859, score=0.856, total=  52.2s\n",
      "[CV] colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597 \n",
      "[09:23:28] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:23:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597, score=0.910, total=  24.5s\n",
      "[CV] colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597 \n",
      "[09:23:52] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:23:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597, score=0.838, total=  25.0s\n",
      "[CV] colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597 \n",
      "[09:24:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:24:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597, score=0.869, total=  23.9s\n",
      "[CV] colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597 \n",
      "[09:24:41] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:24:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597, score=0.827, total=  21.2s\n",
      "[CV] colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597 \n",
      "[09:25:03] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:25:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597, score=0.897, total=  23.4s\n",
      "[CV] colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:25:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:25:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827, score=0.890, total=  11.1s\n",
      "[CV] colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827 \n",
      "[09:25:37] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:25:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827, score=0.798, total=  11.1s\n",
      "[CV] colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827 \n",
      "[09:25:48] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:25:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827, score=0.859, total=  11.2s\n",
      "[CV] colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827 \n",
      "[09:26:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:26:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827, score=0.837, total=  11.4s\n",
      "[CV] colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827 \n",
      "[09:26:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:26:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827, score=0.887, total=  11.1s\n",
      "[CV] colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085 \n",
      "[09:26:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:26:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085, score=0.870, total=  57.9s\n",
      "[CV] colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085 \n",
      "[09:27:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:27:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085, score=0.798, total= 1.0min\n",
      "[CV] colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085 \n",
      "[09:28:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:28:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085, score=0.808, total=  59.0s\n",
      "[CV] colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085 \n",
      "[09:29:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:29:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085, score=0.827, total= 1.0min\n",
      "[CV] colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085 \n",
      "[09:30:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:30:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085, score=0.835, total= 1.0min\n",
      "[CV] colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985 \n",
      "[09:31:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:31:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985, score=0.560, total=   9.5s\n",
      "[CV] colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985 \n",
      "[09:31:35] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:31:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985, score=0.586, total=   9.6s\n",
      "[CV] colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985 \n",
      "[09:31:45] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:31:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985, score=0.677, total=   9.7s\n",
      "[CV] colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:31:54] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:31:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985, score=0.612, total=  10.9s\n",
      "[CV] colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985 \n",
      "[09:32:05] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:32:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985, score=0.753, total=   9.4s\n",
      "[CV] colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774 \n",
      "[09:32:15] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:32:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774, score=0.910, total=  31.5s\n",
      "[CV] colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774 \n",
      "[09:32:46] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:32:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774, score=0.808, total=  31.6s\n",
      "[CV] colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774 \n",
      "[09:33:18] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:33:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774, score=0.859, total=  31.4s\n",
      "[CV] colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774 \n",
      "[09:33:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:33:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774, score=0.837, total=  32.5s\n",
      "[CV] colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774 \n",
      "[09:34:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:34:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774, score=0.876, total=  32.4s\n",
      "[CV] colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868 \n",
      "[09:34:54] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:34:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868, score=0.910, total=  31.9s\n",
      "[CV] colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868 \n",
      "[09:35:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:35:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868, score=0.848, total=  29.1s\n",
      "[CV] colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868 \n",
      "[09:35:56] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:35:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868, score=0.848, total=  28.1s\n",
      "[CV] colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868 \n",
      "[09:36:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:36:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868, score=0.837, total=  25.6s\n",
      "[CV] colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868 \n",
      "[09:36:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:36:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868, score=0.866, total=  30.4s\n",
      "[CV] colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377 \n",
      "[09:37:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:37:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377, score=0.880, total=  22.6s\n",
      "[CV] colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:37:42] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:37:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377, score=0.828, total=  21.0s\n",
      "[CV] colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377 \n",
      "[09:38:03] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:38:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377, score=0.798, total=  25.6s\n",
      "[CV] colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377 \n",
      "[09:38:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:38:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377, score=0.857, total=  23.3s\n",
      "[CV] colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377 \n",
      "[09:38:52] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:38:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377, score=0.814, total=  21.7s\n",
      "[CV] colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144 \n",
      "[09:39:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:39:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144, score=0.890, total=  18.7s\n",
      "[CV] colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144 \n",
      "[09:39:33] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:39:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144, score=0.808, total=  17.4s\n",
      "[CV] colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144 \n",
      "[09:39:50] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:39:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144, score=0.848, total=  17.9s\n",
      "[CV] colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144 \n",
      "[09:40:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:40:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144, score=0.837, total=  17.5s\n",
      "[CV] colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144 \n",
      "[09:40:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:40:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144, score=0.876, total=  17.9s\n",
      "[CV] colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086 \n",
      "[09:40:44] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:40:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086, score=0.860, total=  32.4s\n",
      "[CV] colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086 \n",
      "[09:41:16] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:41:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086, score=0.818, total=  38.4s\n",
      "[CV] colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086 \n",
      "[09:41:55] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:41:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086, score=0.818, total=  34.3s\n",
      "[CV] colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086 \n",
      "[09:42:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:42:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086, score=0.847, total=  37.3s\n",
      "[CV] colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:43:06] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:43:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086, score=0.845, total=  35.6s\n",
      "[CV] colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889 \n",
      "[09:43:42] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:43:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889, score=0.900, total=  29.2s\n",
      "[CV] colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889 \n",
      "[09:44:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:44:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889, score=0.818, total=  28.9s\n",
      "[CV] colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889 \n",
      "[09:44:40] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:44:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889, score=0.838, total=  32.0s\n",
      "[CV] colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889 \n",
      "[09:45:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:45:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889, score=0.837, total=  34.5s\n",
      "[CV] colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889 \n",
      "[09:45:47] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:45:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889, score=0.876, total=  29.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 30.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.8681541582150102\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.07385324490376632, max_delta_step=12, max_depth=0, min_child_weight=5, n_estimators=67, reg_alpha=1.0, reg_lambda=5.023273816526789e-08, scale_pos_weight=1e-06, subsample=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/mgill/.local/lib/python3.6/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:46:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:46:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.07385324490376632, max_delta_step=12, max_depth=0, min_child_weight=5, n_estimators=67, reg_alpha=1.0, reg_lambda=5.023273816526789e-08, scale_pos_weight=1e-06, subsample=1.0, score=0.580, total=  11.2s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.07385324490376632, max_delta_step=12, max_depth=0, min_child_weight=5, n_estimators=67, reg_alpha=1.0, reg_lambda=5.023273816526789e-08, scale_pos_weight=1e-06, subsample=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:46:36] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:46:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.07385324490376632, max_delta_step=12, max_depth=0, min_child_weight=5, n_estimators=67, reg_alpha=1.0, reg_lambda=5.023273816526789e-08, scale_pos_weight=1e-06, subsample=1.0, score=0.586, total=   9.9s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.07385324490376632, max_delta_step=12, max_depth=0, min_child_weight=5, n_estimators=67, reg_alpha=1.0, reg_lambda=5.023273816526789e-08, scale_pos_weight=1e-06, subsample=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   21.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:46:46] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:46:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.07385324490376632, max_delta_step=12, max_depth=0, min_child_weight=5, n_estimators=67, reg_alpha=1.0, reg_lambda=5.023273816526789e-08, scale_pos_weight=1e-06, subsample=1.0, score=0.586, total=  10.7s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.07385324490376632, max_delta_step=12, max_depth=0, min_child_weight=5, n_estimators=67, reg_alpha=1.0, reg_lambda=5.023273816526789e-08, scale_pos_weight=1e-06, subsample=1.0 \n",
      "[09:46:57] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:46:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.07385324490376632, max_delta_step=12, max_depth=0, min_child_weight=5, n_estimators=67, reg_alpha=1.0, reg_lambda=5.023273816526789e-08, scale_pos_weight=1e-06, subsample=1.0, score=0.582, total=   9.8s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.07385324490376632, max_delta_step=12, max_depth=0, min_child_weight=5, n_estimators=67, reg_alpha=1.0, reg_lambda=5.023273816526789e-08, scale_pos_weight=1e-06, subsample=1.0 \n",
      "[09:47:07] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:47:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.07385324490376632, max_delta_step=12, max_depth=0, min_child_weight=5, n_estimators=67, reg_alpha=1.0, reg_lambda=5.023273816526789e-08, scale_pos_weight=1e-06, subsample=1.0, score=0.588, total=  10.3s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.0004220541874780832, learning_rate=0.04600576240195131, max_delta_step=20, max_depth=17, min_child_weight=1, n_estimators=143, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:47:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:47:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.0004220541874780832, learning_rate=0.04600576240195131, max_delta_step=20, max_depth=17, min_child_weight=1, n_estimators=143, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.720, total=  15.9s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.0004220541874780832, learning_rate=0.04600576240195131, max_delta_step=20, max_depth=17, min_child_weight=1, n_estimators=143, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:47:33] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:47:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.0004220541874780832, learning_rate=0.04600576240195131, max_delta_step=20, max_depth=17, min_child_weight=1, n_estimators=143, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.707, total=  16.3s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.0004220541874780832, learning_rate=0.04600576240195131, max_delta_step=20, max_depth=17, min_child_weight=1, n_estimators=143, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:47:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:47:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.0004220541874780832, learning_rate=0.04600576240195131, max_delta_step=20, max_depth=17, min_child_weight=1, n_estimators=143, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.687, total=  15.3s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.0004220541874780832, learning_rate=0.04600576240195131, max_delta_step=20, max_depth=17, min_child_weight=1, n_estimators=143, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:48:05] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:48:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.0004220541874780832, learning_rate=0.04600576240195131, max_delta_step=20, max_depth=17, min_child_weight=1, n_estimators=143, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.704, total=  16.2s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.0004220541874780832, learning_rate=0.04600576240195131, max_delta_step=20, max_depth=17, min_child_weight=1, n_estimators=143, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:48:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:48:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.0004220541874780832, learning_rate=0.04600576240195131, max_delta_step=20, max_depth=17, min_child_weight=1, n_estimators=143, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.691, total=  16.0s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9446923854635458, gamma=1e-09, learning_rate=0.09543696827179805, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=0.6775813303897988 \n",
      "[09:48:37] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:48:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9446923854635458, gamma=1e-09, learning_rate=0.09543696827179805, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=0.6775813303897988, score=0.580, total=   9.9s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9446923854635458, gamma=1e-09, learning_rate=0.09543696827179805, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=0.6775813303897988 \n",
      "[09:48:47] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:48:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9446923854635458, gamma=1e-09, learning_rate=0.09543696827179805, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=0.6775813303897988, score=0.586, total=  10.3s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9446923854635458, gamma=1e-09, learning_rate=0.09543696827179805, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=0.6775813303897988 \n",
      "[09:48:57] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:48:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9446923854635458, gamma=1e-09, learning_rate=0.09543696827179805, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=0.6775813303897988, score=0.586, total=  10.2s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9446923854635458, gamma=1e-09, learning_rate=0.09543696827179805, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=0.6775813303897988 \n",
      "[09:49:07] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:49:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9446923854635458, gamma=1e-09, learning_rate=0.09543696827179805, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=0.6775813303897988, score=0.582, total=   9.2s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9446923854635458, gamma=1e-09, learning_rate=0.09543696827179805, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=0.6775813303897988 \n",
      "[09:49:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:49:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9446923854635458, gamma=1e-09, learning_rate=0.09543696827179805, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=0.6775813303897988, score=0.588, total=  10.4s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=1.0 \n",
      "[09:49:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:49:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=1.0, score=0.890, total=  14.5s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=1.0 \n",
      "[09:49:42] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:49:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=1.0, score=0.838, total=  14.2s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=1.0 \n",
      "[09:49:56] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:49:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=1.0, score=0.859, total=  13.4s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=1.0 \n",
      "[09:50:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:50:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=1.0, score=0.837, total=  13.7s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=1.0 \n",
      "[09:50:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:50:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=1e-06, subsample=1.0, score=0.887, total=  13.4s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.2686464976410889, max_delta_step=20, max_depth=40, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=6.639786763923772e-08, scale_pos_weight=2.1407317883464803e-06, subsample=1.0 \n",
      "[09:50:36] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:50:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.2686464976410889, max_delta_step=20, max_depth=40, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=6.639786763923772e-08, scale_pos_weight=2.1407317883464803e-06, subsample=1.0, score=0.900, total=  12.1s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.2686464976410889, max_delta_step=20, max_depth=40, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=6.639786763923772e-08, scale_pos_weight=2.1407317883464803e-06, subsample=1.0 \n",
      "[09:50:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:50:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.2686464976410889, max_delta_step=20, max_depth=40, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=6.639786763923772e-08, scale_pos_weight=2.1407317883464803e-06, subsample=1.0, score=0.838, total=  11.4s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.2686464976410889, max_delta_step=20, max_depth=40, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=6.639786763923772e-08, scale_pos_weight=2.1407317883464803e-06, subsample=1.0 \n",
      "[09:51:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:51:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.2686464976410889, max_delta_step=20, max_depth=40, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=6.639786763923772e-08, scale_pos_weight=2.1407317883464803e-06, subsample=1.0, score=0.859, total=  11.5s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.2686464976410889, max_delta_step=20, max_depth=40, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=6.639786763923772e-08, scale_pos_weight=2.1407317883464803e-06, subsample=1.0 \n",
      "[09:51:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:51:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.2686464976410889, max_delta_step=20, max_depth=40, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=6.639786763923772e-08, scale_pos_weight=2.1407317883464803e-06, subsample=1.0, score=0.847, total=  11.2s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.2686464976410889, max_delta_step=20, max_depth=40, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=6.639786763923772e-08, scale_pos_weight=2.1407317883464803e-06, subsample=1.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:51:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:51:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.2686464976410889, max_delta_step=20, max_depth=40, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=6.639786763923772e-08, scale_pos_weight=2.1407317883464803e-06, subsample=1.0, score=0.887, total=  10.8s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.16219460855181925, gamma=1e-09, learning_rate=0.07499764003328939, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=6.884435185948308e-06, subsample=0.7585527231591308 \n",
      "[09:51:34] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:51:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.16219460855181925, gamma=1e-09, learning_rate=0.07499764003328939, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=6.884435185948308e-06, subsample=0.7585527231591308, score=0.580, total=  26.7s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.16219460855181925, gamma=1e-09, learning_rate=0.07499764003328939, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=6.884435185948308e-06, subsample=0.7585527231591308 \n",
      "[09:52:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:52:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.16219460855181925, gamma=1e-09, learning_rate=0.07499764003328939, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=6.884435185948308e-06, subsample=0.7585527231591308, score=0.586, total=  25.8s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.16219460855181925, gamma=1e-09, learning_rate=0.07499764003328939, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=6.884435185948308e-06, subsample=0.7585527231591308 \n",
      "[09:52:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:52:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.16219460855181925, gamma=1e-09, learning_rate=0.07499764003328939, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=6.884435185948308e-06, subsample=0.7585527231591308, score=0.586, total=  26.9s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.16219460855181925, gamma=1e-09, learning_rate=0.07499764003328939, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=6.884435185948308e-06, subsample=0.7585527231591308 \n",
      "[09:52:53] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:52:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.16219460855181925, gamma=1e-09, learning_rate=0.07499764003328939, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=6.884435185948308e-06, subsample=0.7585527231591308, score=0.582, total=  26.1s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.16219460855181925, gamma=1e-09, learning_rate=0.07499764003328939, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=6.884435185948308e-06, subsample=0.7585527231591308 \n",
      "[09:53:19] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:53:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.16219460855181925, gamma=1e-09, learning_rate=0.07499764003328939, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=6.884435185948308e-06, subsample=0.7585527231591308, score=0.588, total=  26.9s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.025687348390009163, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=64, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:53:46] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:53:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.025687348390009163, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=64, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.890, total=  22.9s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.025687348390009163, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=64, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:54:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:54:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.025687348390009163, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=64, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.808, total=  20.6s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.025687348390009163, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=64, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:54:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:54:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.025687348390009163, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=64, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.808, total=  21.3s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.025687348390009163, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=64, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:54:51] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:54:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.025687348390009163, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=64, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.837, total=  22.1s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.025687348390009163, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=64, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:55:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:55:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.025687348390009163, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=64, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.835, total=  21.7s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.08655027472951894, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.5930002447690905 \n",
      "[09:55:35] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:55:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.08655027472951894, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.5930002447690905, score=0.920, total=  24.7s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.08655027472951894, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.5930002447690905 \n",
      "[09:56:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:56:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.08655027472951894, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.5930002447690905, score=0.828, total=  25.3s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.08655027472951894, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.5930002447690905 \n",
      "[09:56:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:56:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.08655027472951894, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.5930002447690905, score=0.879, total=  24.5s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.08655027472951894, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.5930002447690905 \n",
      "[09:56:50] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:56:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.08655027472951894, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.5930002447690905, score=0.816, total=  24.8s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.08655027472951894, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.5930002447690905 \n",
      "[09:57:15] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:57:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.08655027472951894, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.5930002447690905, score=0.887, total=  24.8s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.1146079012583086, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.7210413880227217 \n",
      "[09:57:39] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:57:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.1146079012583086, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.7210413880227217, score=0.690, total=   8.3s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.1146079012583086, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.7210413880227217 \n",
      "[09:57:48] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:57:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.1146079012583086, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.7210413880227217, score=0.677, total=   7.4s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.1146079012583086, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.7210413880227217 \n",
      "[09:57:55] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:57:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.1146079012583086, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.7210413880227217, score=0.667, total=   7.6s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.1146079012583086, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.7210413880227217 \n",
      "[09:58:03] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:58:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.1146079012583086, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.7210413880227217, score=0.673, total=   7.2s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.1146079012583086, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.7210413880227217 \n",
      "[09:58:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:58:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.1146079012583086, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.7210413880227217, score=0.670, total=   7.4s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:58:18] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:58:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.580, total=  19.8s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:58:37] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:58:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.586, total=  20.8s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:58:58] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:58:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.586, total=  21.2s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:59:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:59:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.582, total=  19.9s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[09:59:39] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:59:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.588, total=  20.9s\n",
      "[CV] colsample_bylevel=0.6756169052076102, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.788354085349178 \n",
      "[10:00:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:00:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.6756169052076102, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.788354085349178, score=0.580, total=  26.4s\n",
      "[CV] colsample_bylevel=0.6756169052076102, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.788354085349178 \n",
      "[10:00:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:00:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.6756169052076102, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.788354085349178, score=0.586, total=  25.2s\n",
      "[CV] colsample_bylevel=0.6756169052076102, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.788354085349178 \n",
      "[10:00:52] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:00:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.6756169052076102, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.788354085349178, score=0.586, total=  25.0s\n",
      "[CV] colsample_bylevel=0.6756169052076102, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.788354085349178 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:01:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:01:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.6756169052076102, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.788354085349178, score=0.582, total=  25.8s\n",
      "[CV] colsample_bylevel=0.6756169052076102, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.788354085349178 \n",
      "[10:01:43] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:01:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.6756169052076102, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.01, max_delta_step=20, max_depth=0, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.788354085349178, score=0.588, total=  25.3s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=9.31925368563796e-06, learning_rate=0.01, max_delta_step=0, max_depth=32, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[10:02:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:02:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=9.31925368563796e-06, learning_rate=0.01, max_delta_step=0, max_depth=32, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.880, total= 3.2min\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=9.31925368563796e-06, learning_rate=0.01, max_delta_step=0, max_depth=32, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[10:05:18] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:05:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=9.31925368563796e-06, learning_rate=0.01, max_delta_step=0, max_depth=32, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.788, total= 2.9min\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=9.31925368563796e-06, learning_rate=0.01, max_delta_step=0, max_depth=32, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[10:08:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:08:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=9.31925368563796e-06, learning_rate=0.01, max_delta_step=0, max_depth=32, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.808, total= 3.3min\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=9.31925368563796e-06, learning_rate=0.01, max_delta_step=0, max_depth=32, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[10:11:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:11:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=9.31925368563796e-06, learning_rate=0.01, max_delta_step=0, max_depth=32, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.786, total= 2.8min\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=9.31925368563796e-06, learning_rate=0.01, max_delta_step=0, max_depth=32, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[10:14:18] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:14:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=9.31925368563796e-06, learning_rate=0.01, max_delta_step=0, max_depth=32, min_child_weight=0, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.845, total= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 31.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.8681541582150102\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] colsample_bylevel=0.7539856612396592, colsample_bytree=1.0, gamma=5.892350703005532e-07, learning_rate=0.08952178082527781, max_delta_step=1, max_depth=36, min_child_weight=1, n_estimators=106, reg_alpha=3.777875653000374e-08, reg_lambda=0.025468207657755936, scale_pos_weight=1.3834669532987018, subsample=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/mgill/.local/lib/python3.6/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:17:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:17:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7539856612396592, colsample_bytree=1.0, gamma=5.892350703005532e-07, learning_rate=0.08952178082527781, max_delta_step=1, max_depth=36, min_child_weight=1, n_estimators=106, reg_alpha=3.777875653000374e-08, reg_lambda=0.025468207657755936, scale_pos_weight=1.3834669532987018, subsample=1.0, score=0.900, total=  59.0s\n",
      "[CV] colsample_bylevel=0.7539856612396592, colsample_bytree=1.0, gamma=5.892350703005532e-07, learning_rate=0.08952178082527781, max_delta_step=1, max_depth=36, min_child_weight=1, n_estimators=106, reg_alpha=3.777875653000374e-08, reg_lambda=0.025468207657755936, scale_pos_weight=1.3834669532987018, subsample=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   59.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:18:48] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:18:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7539856612396592, colsample_bytree=1.0, gamma=5.892350703005532e-07, learning_rate=0.08952178082527781, max_delta_step=1, max_depth=36, min_child_weight=1, n_estimators=106, reg_alpha=3.777875653000374e-08, reg_lambda=0.025468207657755936, scale_pos_weight=1.3834669532987018, subsample=1.0, score=0.838, total=  59.1s\n",
      "[CV] colsample_bylevel=0.7539856612396592, colsample_bytree=1.0, gamma=5.892350703005532e-07, learning_rate=0.08952178082527781, max_delta_step=1, max_depth=36, min_child_weight=1, n_estimators=106, reg_alpha=3.777875653000374e-08, reg_lambda=0.025468207657755936, scale_pos_weight=1.3834669532987018, subsample=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:19:47] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:19:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7539856612396592, colsample_bytree=1.0, gamma=5.892350703005532e-07, learning_rate=0.08952178082527781, max_delta_step=1, max_depth=36, min_child_weight=1, n_estimators=106, reg_alpha=3.777875653000374e-08, reg_lambda=0.025468207657755936, scale_pos_weight=1.3834669532987018, subsample=1.0, score=0.869, total= 1.0min\n",
      "[CV] colsample_bylevel=0.7539856612396592, colsample_bytree=1.0, gamma=5.892350703005532e-07, learning_rate=0.08952178082527781, max_delta_step=1, max_depth=36, min_child_weight=1, n_estimators=106, reg_alpha=3.777875653000374e-08, reg_lambda=0.025468207657755936, scale_pos_weight=1.3834669532987018, subsample=1.0 \n",
      "[10:20:47] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:20:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7539856612396592, colsample_bytree=1.0, gamma=5.892350703005532e-07, learning_rate=0.08952178082527781, max_delta_step=1, max_depth=36, min_child_weight=1, n_estimators=106, reg_alpha=3.777875653000374e-08, reg_lambda=0.025468207657755936, scale_pos_weight=1.3834669532987018, subsample=1.0, score=0.827, total=  59.9s\n",
      "[CV] colsample_bylevel=0.7539856612396592, colsample_bytree=1.0, gamma=5.892350703005532e-07, learning_rate=0.08952178082527781, max_delta_step=1, max_depth=36, min_child_weight=1, n_estimators=106, reg_alpha=3.777875653000374e-08, reg_lambda=0.025468207657755936, scale_pos_weight=1.3834669532987018, subsample=1.0 \n",
      "[10:21:47] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:21:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7539856612396592, colsample_bytree=1.0, gamma=5.892350703005532e-07, learning_rate=0.08952178082527781, max_delta_step=1, max_depth=36, min_child_weight=1, n_estimators=106, reg_alpha=3.777875653000374e-08, reg_lambda=0.025468207657755936, scale_pos_weight=1.3834669532987018, subsample=1.0, score=0.887, total=  57.1s\n",
      "[CV] colsample_bylevel=0.3288861044516802, colsample_bytree=0.5471941705860428, gamma=1.8725286198986933e-06, learning_rate=0.03593943082189275, max_delta_step=10, max_depth=19, min_child_weight=1, n_estimators=84, reg_alpha=2.609154014915572e-07, reg_lambda=157.06200136337932, scale_pos_weight=0.010093787540742112, subsample=0.6493425997136414 \n",
      "[10:22:44] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:22:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3288861044516802, colsample_bytree=0.5471941705860428, gamma=1.8725286198986933e-06, learning_rate=0.03593943082189275, max_delta_step=10, max_depth=19, min_child_weight=1, n_estimators=84, reg_alpha=2.609154014915572e-07, reg_lambda=157.06200136337932, scale_pos_weight=0.010093787540742112, subsample=0.6493425997136414, score=0.900, total=  27.2s\n",
      "[CV] colsample_bylevel=0.3288861044516802, colsample_bytree=0.5471941705860428, gamma=1.8725286198986933e-06, learning_rate=0.03593943082189275, max_delta_step=10, max_depth=19, min_child_weight=1, n_estimators=84, reg_alpha=2.609154014915572e-07, reg_lambda=157.06200136337932, scale_pos_weight=0.010093787540742112, subsample=0.6493425997136414 \n",
      "[10:23:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:23:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3288861044516802, colsample_bytree=0.5471941705860428, gamma=1.8725286198986933e-06, learning_rate=0.03593943082189275, max_delta_step=10, max_depth=19, min_child_weight=1, n_estimators=84, reg_alpha=2.609154014915572e-07, reg_lambda=157.06200136337932, scale_pos_weight=0.010093787540742112, subsample=0.6493425997136414, score=0.818, total=  27.8s\n",
      "[CV] colsample_bylevel=0.3288861044516802, colsample_bytree=0.5471941705860428, gamma=1.8725286198986933e-06, learning_rate=0.03593943082189275, max_delta_step=10, max_depth=19, min_child_weight=1, n_estimators=84, reg_alpha=2.609154014915572e-07, reg_lambda=157.06200136337932, scale_pos_weight=0.010093787540742112, subsample=0.6493425997136414 \n",
      "[10:23:39] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:23:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3288861044516802, colsample_bytree=0.5471941705860428, gamma=1.8725286198986933e-06, learning_rate=0.03593943082189275, max_delta_step=10, max_depth=19, min_child_weight=1, n_estimators=84, reg_alpha=2.609154014915572e-07, reg_lambda=157.06200136337932, scale_pos_weight=0.010093787540742112, subsample=0.6493425997136414, score=0.838, total=  29.3s\n",
      "[CV] colsample_bylevel=0.3288861044516802, colsample_bytree=0.5471941705860428, gamma=1.8725286198986933e-06, learning_rate=0.03593943082189275, max_delta_step=10, max_depth=19, min_child_weight=1, n_estimators=84, reg_alpha=2.609154014915572e-07, reg_lambda=157.06200136337932, scale_pos_weight=0.010093787540742112, subsample=0.6493425997136414 \n",
      "[10:24:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:24:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  colsample_bylevel=0.3288861044516802, colsample_bytree=0.5471941705860428, gamma=1.8725286198986933e-06, learning_rate=0.03593943082189275, max_delta_step=10, max_depth=19, min_child_weight=1, n_estimators=84, reg_alpha=2.609154014915572e-07, reg_lambda=157.06200136337932, scale_pos_weight=0.010093787540742112, subsample=0.6493425997136414, score=0.847, total=  28.3s\n",
      "[CV] colsample_bylevel=0.3288861044516802, colsample_bytree=0.5471941705860428, gamma=1.8725286198986933e-06, learning_rate=0.03593943082189275, max_delta_step=10, max_depth=19, min_child_weight=1, n_estimators=84, reg_alpha=2.609154014915572e-07, reg_lambda=157.06200136337932, scale_pos_weight=0.010093787540742112, subsample=0.6493425997136414 \n",
      "[10:24:37] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:24:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3288861044516802, colsample_bytree=0.5471941705860428, gamma=1.8725286198986933e-06, learning_rate=0.03593943082189275, max_delta_step=10, max_depth=19, min_child_weight=1, n_estimators=84, reg_alpha=2.609154014915572e-07, reg_lambda=157.06200136337932, scale_pos_weight=0.010093787540742112, subsample=0.6493425997136414, score=0.845, total=  27.4s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9509401432447667, gamma=1.0781982427101056e-09, learning_rate=1.0, max_delta_step=3, max_depth=42, min_child_weight=0, n_estimators=61, reg_alpha=0.011329180733964227, reg_lambda=1.754093517794009e-05, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[10:25:04] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:25:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9509401432447667, gamma=1.0781982427101056e-09, learning_rate=1.0, max_delta_step=3, max_depth=42, min_child_weight=0, n_estimators=61, reg_alpha=0.011329180733964227, reg_lambda=1.754093517794009e-05, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.610, total=  15.3s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9509401432447667, gamma=1.0781982427101056e-09, learning_rate=1.0, max_delta_step=3, max_depth=42, min_child_weight=0, n_estimators=61, reg_alpha=0.011329180733964227, reg_lambda=1.754093517794009e-05, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[10:25:19] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:25:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9509401432447667, gamma=1.0781982427101056e-09, learning_rate=1.0, max_delta_step=3, max_depth=42, min_child_weight=0, n_estimators=61, reg_alpha=0.011329180733964227, reg_lambda=1.754093517794009e-05, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.535, total=  15.1s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9509401432447667, gamma=1.0781982427101056e-09, learning_rate=1.0, max_delta_step=3, max_depth=42, min_child_weight=0, n_estimators=61, reg_alpha=0.011329180733964227, reg_lambda=1.754093517794009e-05, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[10:25:35] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:25:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9509401432447667, gamma=1.0781982427101056e-09, learning_rate=1.0, max_delta_step=3, max_depth=42, min_child_weight=0, n_estimators=61, reg_alpha=0.011329180733964227, reg_lambda=1.754093517794009e-05, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.535, total=  15.4s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9509401432447667, gamma=1.0781982427101056e-09, learning_rate=1.0, max_delta_step=3, max_depth=42, min_child_weight=0, n_estimators=61, reg_alpha=0.011329180733964227, reg_lambda=1.754093517794009e-05, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[10:25:50] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:25:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9509401432447667, gamma=1.0781982427101056e-09, learning_rate=1.0, max_delta_step=3, max_depth=42, min_child_weight=0, n_estimators=61, reg_alpha=0.011329180733964227, reg_lambda=1.754093517794009e-05, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.531, total=  15.9s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9509401432447667, gamma=1.0781982427101056e-09, learning_rate=1.0, max_delta_step=3, max_depth=42, min_child_weight=0, n_estimators=61, reg_alpha=0.011329180733964227, reg_lambda=1.754093517794009e-05, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[10:26:06] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:26:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9509401432447667, gamma=1.0781982427101056e-09, learning_rate=1.0, max_delta_step=3, max_depth=42, min_child_weight=0, n_estimators=61, reg_alpha=0.011329180733964227, reg_lambda=1.754093517794009e-05, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.526, total=  16.4s\n",
      "[CV] colsample_bylevel=0.30190692068544567, colsample_bytree=0.7993091105672656, gamma=5.330557652608038e-08, learning_rate=0.046594183737523644, max_delta_step=12, max_depth=28, min_child_weight=1, n_estimators=127, reg_alpha=8.67469405516486e-08, reg_lambda=0.1510489799379124, scale_pos_weight=0.016998104884807793, subsample=0.5972460475608784 \n",
      "[10:26:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:26:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.30190692068544567, colsample_bytree=0.7993091105672656, gamma=5.330557652608038e-08, learning_rate=0.046594183737523644, max_delta_step=12, max_depth=28, min_child_weight=1, n_estimators=127, reg_alpha=8.67469405516486e-08, reg_lambda=0.1510489799379124, scale_pos_weight=0.016998104884807793, subsample=0.5972460475608784, score=0.910, total=  45.1s\n",
      "[CV] colsample_bylevel=0.30190692068544567, colsample_bytree=0.7993091105672656, gamma=5.330557652608038e-08, learning_rate=0.046594183737523644, max_delta_step=12, max_depth=28, min_child_weight=1, n_estimators=127, reg_alpha=8.67469405516486e-08, reg_lambda=0.1510489799379124, scale_pos_weight=0.016998104884807793, subsample=0.5972460475608784 \n",
      "[10:27:07] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:27:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.30190692068544567, colsample_bytree=0.7993091105672656, gamma=5.330557652608038e-08, learning_rate=0.046594183737523644, max_delta_step=12, max_depth=28, min_child_weight=1, n_estimators=127, reg_alpha=8.67469405516486e-08, reg_lambda=0.1510489799379124, scale_pos_weight=0.016998104884807793, subsample=0.5972460475608784, score=0.818, total=  43.7s\n",
      "[CV] colsample_bylevel=0.30190692068544567, colsample_bytree=0.7993091105672656, gamma=5.330557652608038e-08, learning_rate=0.046594183737523644, max_delta_step=12, max_depth=28, min_child_weight=1, n_estimators=127, reg_alpha=8.67469405516486e-08, reg_lambda=0.1510489799379124, scale_pos_weight=0.016998104884807793, subsample=0.5972460475608784 \n",
      "[10:27:51] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:27:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.30190692068544567, colsample_bytree=0.7993091105672656, gamma=5.330557652608038e-08, learning_rate=0.046594183737523644, max_delta_step=12, max_depth=28, min_child_weight=1, n_estimators=127, reg_alpha=8.67469405516486e-08, reg_lambda=0.1510489799379124, scale_pos_weight=0.016998104884807793, subsample=0.5972460475608784, score=0.869, total=  43.2s\n",
      "[CV] colsample_bylevel=0.30190692068544567, colsample_bytree=0.7993091105672656, gamma=5.330557652608038e-08, learning_rate=0.046594183737523644, max_delta_step=12, max_depth=28, min_child_weight=1, n_estimators=127, reg_alpha=8.67469405516486e-08, reg_lambda=0.1510489799379124, scale_pos_weight=0.016998104884807793, subsample=0.5972460475608784 \n",
      "[10:28:34] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:28:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.30190692068544567, colsample_bytree=0.7993091105672656, gamma=5.330557652608038e-08, learning_rate=0.046594183737523644, max_delta_step=12, max_depth=28, min_child_weight=1, n_estimators=127, reg_alpha=8.67469405516486e-08, reg_lambda=0.1510489799379124, scale_pos_weight=0.016998104884807793, subsample=0.5972460475608784, score=0.837, total= 1.0min\n",
      "[CV] colsample_bylevel=0.30190692068544567, colsample_bytree=0.7993091105672656, gamma=5.330557652608038e-08, learning_rate=0.046594183737523644, max_delta_step=12, max_depth=28, min_child_weight=1, n_estimators=127, reg_alpha=8.67469405516486e-08, reg_lambda=0.1510489799379124, scale_pos_weight=0.016998104884807793, subsample=0.5972460475608784 \n",
      "[10:29:37] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:29:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.30190692068544567, colsample_bytree=0.7993091105672656, gamma=5.330557652608038e-08, learning_rate=0.046594183737523644, max_delta_step=12, max_depth=28, min_child_weight=1, n_estimators=127, reg_alpha=8.67469405516486e-08, reg_lambda=0.1510489799379124, scale_pos_weight=0.016998104884807793, subsample=0.5972460475608784, score=0.907, total=  48.2s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9512076537730382, gamma=0.00015405419782536298, learning_rate=0.9985313547288144, max_delta_step=17, max_depth=43, min_child_weight=0, n_estimators=69, reg_alpha=0.12392806740765744, reg_lambda=6.835623754460854e-05, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[10:30:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:30:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9512076537730382, gamma=0.00015405419782536298, learning_rate=0.9985313547288144, max_delta_step=17, max_depth=43, min_child_weight=0, n_estimators=69, reg_alpha=0.12392806740765744, reg_lambda=6.835623754460854e-05, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.410, total=  16.0s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9512076537730382, gamma=0.00015405419782536298, learning_rate=0.9985313547288144, max_delta_step=17, max_depth=43, min_child_weight=0, n_estimators=69, reg_alpha=0.12392806740765744, reg_lambda=6.835623754460854e-05, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[10:30:41] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:30:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9512076537730382, gamma=0.00015405419782536298, learning_rate=0.9985313547288144, max_delta_step=17, max_depth=43, min_child_weight=0, n_estimators=69, reg_alpha=0.12392806740765744, reg_lambda=6.835623754460854e-05, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.545, total=  15.6s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9512076537730382, gamma=0.00015405419782536298, learning_rate=0.9985313547288144, max_delta_step=17, max_depth=43, min_child_weight=0, n_estimators=69, reg_alpha=0.12392806740765744, reg_lambda=6.835623754460854e-05, scale_pos_weight=499.99999999999994, subsample=0.01 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:30:56] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:30:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9512076537730382, gamma=0.00015405419782536298, learning_rate=0.9985313547288144, max_delta_step=17, max_depth=43, min_child_weight=0, n_estimators=69, reg_alpha=0.12392806740765744, reg_lambda=6.835623754460854e-05, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.323, total=  15.9s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9512076537730382, gamma=0.00015405419782536298, learning_rate=0.9985313547288144, max_delta_step=17, max_depth=43, min_child_weight=0, n_estimators=69, reg_alpha=0.12392806740765744, reg_lambda=6.835623754460854e-05, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[10:31:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:31:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9512076537730382, gamma=0.00015405419782536298, learning_rate=0.9985313547288144, max_delta_step=17, max_depth=43, min_child_weight=0, n_estimators=69, reg_alpha=0.12392806740765744, reg_lambda=6.835623754460854e-05, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.490, total=  16.5s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.9512076537730382, gamma=0.00015405419782536298, learning_rate=0.9985313547288144, max_delta_step=17, max_depth=43, min_child_weight=0, n_estimators=69, reg_alpha=0.12392806740765744, reg_lambda=6.835623754460854e-05, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[10:31:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:31:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.9512076537730382, gamma=0.00015405419782536298, learning_rate=0.9985313547288144, max_delta_step=17, max_depth=43, min_child_weight=0, n_estimators=69, reg_alpha=0.12392806740765744, reg_lambda=6.835623754460854e-05, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.485, total=  15.7s\n",
      "[CV] colsample_bylevel=0.8143193811563679, colsample_bytree=0.3532780413721867, gamma=2.3998299038450225e-05, learning_rate=0.025140873949347133, max_delta_step=8, max_depth=50, min_child_weight=4, n_estimators=105, reg_alpha=1e-09, reg_lambda=3.347033813906026e-08, scale_pos_weight=54.87250609379015, subsample=0.24849725385892932 \n",
      "[10:31:45] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:31:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8143193811563679, colsample_bytree=0.3532780413721867, gamma=2.3998299038450225e-05, learning_rate=0.025140873949347133, max_delta_step=8, max_depth=50, min_child_weight=4, n_estimators=105, reg_alpha=1e-09, reg_lambda=3.347033813906026e-08, scale_pos_weight=54.87250609379015, subsample=0.24849725385892932, score=0.880, total=  31.3s\n",
      "[CV] colsample_bylevel=0.8143193811563679, colsample_bytree=0.3532780413721867, gamma=2.3998299038450225e-05, learning_rate=0.025140873949347133, max_delta_step=8, max_depth=50, min_child_weight=4, n_estimators=105, reg_alpha=1e-09, reg_lambda=3.347033813906026e-08, scale_pos_weight=54.87250609379015, subsample=0.24849725385892932 \n",
      "[10:32:16] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:32:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8143193811563679, colsample_bytree=0.3532780413721867, gamma=2.3998299038450225e-05, learning_rate=0.025140873949347133, max_delta_step=8, max_depth=50, min_child_weight=4, n_estimators=105, reg_alpha=1e-09, reg_lambda=3.347033813906026e-08, scale_pos_weight=54.87250609379015, subsample=0.24849725385892932, score=0.828, total=  32.8s\n",
      "[CV] colsample_bylevel=0.8143193811563679, colsample_bytree=0.3532780413721867, gamma=2.3998299038450225e-05, learning_rate=0.025140873949347133, max_delta_step=8, max_depth=50, min_child_weight=4, n_estimators=105, reg_alpha=1e-09, reg_lambda=3.347033813906026e-08, scale_pos_weight=54.87250609379015, subsample=0.24849725385892932 \n",
      "[10:32:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:32:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8143193811563679, colsample_bytree=0.3532780413721867, gamma=2.3998299038450225e-05, learning_rate=0.025140873949347133, max_delta_step=8, max_depth=50, min_child_weight=4, n_estimators=105, reg_alpha=1e-09, reg_lambda=3.347033813906026e-08, scale_pos_weight=54.87250609379015, subsample=0.24849725385892932, score=0.828, total=  29.8s\n",
      "[CV] colsample_bylevel=0.8143193811563679, colsample_bytree=0.3532780413721867, gamma=2.3998299038450225e-05, learning_rate=0.025140873949347133, max_delta_step=8, max_depth=50, min_child_weight=4, n_estimators=105, reg_alpha=1e-09, reg_lambda=3.347033813906026e-08, scale_pos_weight=54.87250609379015, subsample=0.24849725385892932 \n",
      "[10:33:19] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:33:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  colsample_bylevel=0.8143193811563679, colsample_bytree=0.3532780413721867, gamma=2.3998299038450225e-05, learning_rate=0.025140873949347133, max_delta_step=8, max_depth=50, min_child_weight=4, n_estimators=105, reg_alpha=1e-09, reg_lambda=3.347033813906026e-08, scale_pos_weight=54.87250609379015, subsample=0.24849725385892932, score=0.847, total=  29.2s\n",
      "[CV] colsample_bylevel=0.8143193811563679, colsample_bytree=0.3532780413721867, gamma=2.3998299038450225e-05, learning_rate=0.025140873949347133, max_delta_step=8, max_depth=50, min_child_weight=4, n_estimators=105, reg_alpha=1e-09, reg_lambda=3.347033813906026e-08, scale_pos_weight=54.87250609379015, subsample=0.24849725385892932 \n",
      "[10:33:48] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:33:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8143193811563679, colsample_bytree=0.3532780413721867, gamma=2.3998299038450225e-05, learning_rate=0.025140873949347133, max_delta_step=8, max_depth=50, min_child_weight=4, n_estimators=105, reg_alpha=1e-09, reg_lambda=3.347033813906026e-08, scale_pos_weight=54.87250609379015, subsample=0.24849725385892932, score=0.835, total=  29.5s\n",
      "[CV] colsample_bylevel=0.9912826447840224, colsample_bytree=0.7519043314965428, gamma=4.6413403874832227e-07, learning_rate=1.0, max_delta_step=20, max_depth=47, min_child_weight=0, n_estimators=146, reg_alpha=7.041110301735712e-05, reg_lambda=8.586731837579599e-06, scale_pos_weight=499.99999999999994, subsample=0.22967133705661355 \n",
      "[10:34:18] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:34:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9912826447840224, colsample_bytree=0.7519043314965428, gamma=4.6413403874832227e-07, learning_rate=1.0, max_delta_step=20, max_depth=47, min_child_weight=0, n_estimators=146, reg_alpha=7.041110301735712e-05, reg_lambda=8.586731837579599e-06, scale_pos_weight=499.99999999999994, subsample=0.22967133705661355, score=0.660, total= 1.1min\n",
      "[CV] colsample_bylevel=0.9912826447840224, colsample_bytree=0.7519043314965428, gamma=4.6413403874832227e-07, learning_rate=1.0, max_delta_step=20, max_depth=47, min_child_weight=0, n_estimators=146, reg_alpha=7.041110301735712e-05, reg_lambda=8.586731837579599e-06, scale_pos_weight=499.99999999999994, subsample=0.22967133705661355 \n",
      "[10:35:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:35:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9912826447840224, colsample_bytree=0.7519043314965428, gamma=4.6413403874832227e-07, learning_rate=1.0, max_delta_step=20, max_depth=47, min_child_weight=0, n_estimators=146, reg_alpha=7.041110301735712e-05, reg_lambda=8.586731837579599e-06, scale_pos_weight=499.99999999999994, subsample=0.22967133705661355, score=0.667, total= 1.1min\n",
      "[CV] colsample_bylevel=0.9912826447840224, colsample_bytree=0.7519043314965428, gamma=4.6413403874832227e-07, learning_rate=1.0, max_delta_step=20, max_depth=47, min_child_weight=0, n_estimators=146, reg_alpha=7.041110301735712e-05, reg_lambda=8.586731837579599e-06, scale_pos_weight=499.99999999999994, subsample=0.22967133705661355 \n",
      "[10:36:32] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:36:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9912826447840224, colsample_bytree=0.7519043314965428, gamma=4.6413403874832227e-07, learning_rate=1.0, max_delta_step=20, max_depth=47, min_child_weight=0, n_estimators=146, reg_alpha=7.041110301735712e-05, reg_lambda=8.586731837579599e-06, scale_pos_weight=499.99999999999994, subsample=0.22967133705661355, score=0.646, total= 1.1min\n",
      "[CV] colsample_bylevel=0.9912826447840224, colsample_bytree=0.7519043314965428, gamma=4.6413403874832227e-07, learning_rate=1.0, max_delta_step=20, max_depth=47, min_child_weight=0, n_estimators=146, reg_alpha=7.041110301735712e-05, reg_lambda=8.586731837579599e-06, scale_pos_weight=499.99999999999994, subsample=0.22967133705661355 \n",
      "[10:37:36] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:37:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9912826447840224, colsample_bytree=0.7519043314965428, gamma=4.6413403874832227e-07, learning_rate=1.0, max_delta_step=20, max_depth=47, min_child_weight=0, n_estimators=146, reg_alpha=7.041110301735712e-05, reg_lambda=8.586731837579599e-06, scale_pos_weight=499.99999999999994, subsample=0.22967133705661355, score=0.704, total= 1.2min\n",
      "[CV] colsample_bylevel=0.9912826447840224, colsample_bytree=0.7519043314965428, gamma=4.6413403874832227e-07, learning_rate=1.0, max_delta_step=20, max_depth=47, min_child_weight=0, n_estimators=146, reg_alpha=7.041110301735712e-05, reg_lambda=8.586731837579599e-06, scale_pos_weight=499.99999999999994, subsample=0.22967133705661355 \n",
      "[10:38:47] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:38:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9912826447840224, colsample_bytree=0.7519043314965428, gamma=4.6413403874832227e-07, learning_rate=1.0, max_delta_step=20, max_depth=47, min_child_weight=0, n_estimators=146, reg_alpha=7.041110301735712e-05, reg_lambda=8.586731837579599e-06, scale_pos_weight=499.99999999999994, subsample=0.22967133705661355, score=0.680, total= 1.0min\n",
      "[CV] colsample_bylevel=0.5370129134973161, colsample_bytree=0.24303510355541716, gamma=0.055794982544355706, learning_rate=0.10844984994907986, max_delta_step=2, max_depth=31, min_child_weight=5, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.48881082865425923 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:39:47] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:39:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5370129134973161, colsample_bytree=0.24303510355541716, gamma=0.055794982544355706, learning_rate=0.10844984994907986, max_delta_step=2, max_depth=31, min_child_weight=5, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.48881082865425923, score=0.880, total=  33.5s\n",
      "[CV] colsample_bylevel=0.5370129134973161, colsample_bytree=0.24303510355541716, gamma=0.055794982544355706, learning_rate=0.10844984994907986, max_delta_step=2, max_depth=31, min_child_weight=5, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.48881082865425923 \n",
      "[10:40:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:40:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5370129134973161, colsample_bytree=0.24303510355541716, gamma=0.055794982544355706, learning_rate=0.10844984994907986, max_delta_step=2, max_depth=31, min_child_weight=5, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.48881082865425923, score=0.798, total=  33.1s\n",
      "[CV] colsample_bylevel=0.5370129134973161, colsample_bytree=0.24303510355541716, gamma=0.055794982544355706, learning_rate=0.10844984994907986, max_delta_step=2, max_depth=31, min_child_weight=5, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.48881082865425923 \n",
      "[10:40:54] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:40:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5370129134973161, colsample_bytree=0.24303510355541716, gamma=0.055794982544355706, learning_rate=0.10844984994907986, max_delta_step=2, max_depth=31, min_child_weight=5, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.48881082865425923, score=0.808, total=  35.1s\n",
      "[CV] colsample_bylevel=0.5370129134973161, colsample_bytree=0.24303510355541716, gamma=0.055794982544355706, learning_rate=0.10844984994907986, max_delta_step=2, max_depth=31, min_child_weight=5, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.48881082865425923 \n",
      "[10:41:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:41:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5370129134973161, colsample_bytree=0.24303510355541716, gamma=0.055794982544355706, learning_rate=0.10844984994907986, max_delta_step=2, max_depth=31, min_child_weight=5, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.48881082865425923, score=0.837, total=  33.7s\n",
      "[CV] colsample_bylevel=0.5370129134973161, colsample_bytree=0.24303510355541716, gamma=0.055794982544355706, learning_rate=0.10844984994907986, max_delta_step=2, max_depth=31, min_child_weight=5, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.48881082865425923 \n",
      "[10:42:03] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:42:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5370129134973161, colsample_bytree=0.24303510355541716, gamma=0.055794982544355706, learning_rate=0.10844984994907986, max_delta_step=2, max_depth=31, min_child_weight=5, n_estimators=200, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.48881082865425923, score=0.825, total=  34.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 24.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.8681541582150102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgill/.local/lib/python3.6/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:38] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:42:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "OrderedDict([('colsample_bylevel', 0.25617325301227906), ('colsample_bytree', 0.7083937150495909), ('gamma', 2.41812432168581e-07), ('learning_rate', 0.13965555720269418), ('max_delta_step', 10), ('max_depth', 27), ('min_child_weight', 1), ('n_estimators', 76), ('reg_alpha', 3.178148842971562e-08), ('reg_lambda', 0.005381781269387993), ('scale_pos_weight', 0.23835043249575294), ('subsample', 0.9559763235078597)])\n"
     ]
    }
   ],
   "source": [
    "xgbcl = xgb.XGBClassifier(objective='multi:softmax', num_class=3)\n",
    "xgb_bayes_search = BayesSearchCV(xgbcl, space, n_iter=32, # specify how many iterations\n",
    "                                    scoring=None, n_jobs=1, cv=5, verbose=3, random_state=42, n_points=12,\n",
    "                                 refit=True)\n",
    "xgb_bayes_search.fit(X_train, y_train.ravel(), callback = on_step)\n",
    "print(xgb_bayes_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('colsample_bylevel', 0.25617325301227906), ('colsample_bytree', 0.7083937150495909), ('gamma', 2.41812432168581e-07), ('learning_rate', 0.13965555720269418), ('max_delta_step', 10), ('max_depth', 27), ('min_child_weight', 1), ('n_estimators', 76), ('reg_alpha', 3.178148842971562e-08), ('reg_lambda', 0.005381781269387993), ('scale_pos_weight', 0.23835043249575294), ('subsample', 0.9559763235078597)])\n"
     ]
    }
   ],
   "source": [
    "print(xgb_bayes_search.best_params_)\n",
    "model = xgb.XGBClassifier(**xgb_bayes_search.best_params_, objective='multi:softmax', num_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##if not in same run as optimisation\n",
    "best_params = OrderedDict([('colsample_bylevel', 1.0), ('colsample_bytree', 1.0), ('gamma', 1e-09), ('learning_rate', 0.06083328540655388), ('max_delta_step', 0), ('max_depth', 50), ('min_child_weight', 2), ('n_estimators', 50), ('reg_alpha', 1e-09), ('reg_lambda', 0.003070765266077698), ('scale_pos_weight', 1e-06), ('subsample', 0.7786806739694166)])\n",
    "model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO MORE STUFF HERE when OPTIMISED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_k_fold(m, x, y, k, hx, hy):\n",
    "    #model: xgboost model, should be with the best params available\n",
    "    #x: input data (eg. all samples and SNPS)\n",
    "    #y: labels\n",
    "    #k: number of folds for cross validation\n",
    "    cv = StratifiedKFold(n_splits=k,shuffle=False)\n",
    "    fig1 = plt.figure(figsize=[12,12])\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    results = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    high = 0\n",
    "    best = m\n",
    "    i = 1\n",
    "    for train,test in cv.split(x,y):\n",
    "        prediction = m.fit(x[train],y[train].ravel()).predict_proba(x[test])\n",
    "        print(\"variables for auroc curve done. Processing fold accuracy + checking best model\")\n",
    "        y_pred = m.predict(x[test])\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        #sees how accurate the model was when testing the test set\n",
    "        accuracy = accuracy_score(y[test], predictions)\n",
    "        pcent = accuracy * 100.0\n",
    "        print(\"The accuracy of this model is\" + str(pcent))\n",
    "        if(pcent > high):\n",
    "            high = pcent\n",
    "            best = m\n",
    "       # fpr, tpr, t = roc_curve(y[test], prediction[:, 1])\n",
    "       # tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "       # roc_auc = auc(fpr, tpr)\n",
    "       # aucs.append(roc_auc)\n",
    "        results.append(pcent)\n",
    "       # plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "        i= i+1\n",
    "\n",
    "   # plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "   # mean_tpr = np.mean(tprs, axis=0)\n",
    "   # mean_auc = auc(mean_fpr, mean_tpr)\n",
    "   # plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "    #         label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "    \n",
    "    holdout_pred = best.predict(hx)\n",
    "    predictions = [round(value) for value in holdout_pred]\n",
    "    #sees how accurate the model was when testing the test set\n",
    "    accuracy = accuracy_score(hy, predictions)\n",
    "    pcent = accuracy * 100.0\n",
    "    print(pcent)\n",
    "    xgb_predictions = best.predict(hx)\n",
    "    xgb_probs = best.predict_proba(hx)[:, 1]\n",
    "    #model_fpr, model_tpr, my_roccy = evaluate_model(xgb_predictions, xgb_probs, hy)\n",
    "    #plt.plot(model_fpr, model_tpr, 'r', label = 'Holdout Data'+my_roccy, lw=2)\n",
    "    \n",
    "    #plt.xlabel('False Positive Rate')\n",
    "    #plt.ylabel('True Positive Rate')\n",
    "    #plt.title('ROC Flower Colour Training Model & Holdout Data')\n",
    "    #plt.legend(loc=\"lower right\")\n",
    "    #plt.show()\n",
    "\n",
    "    print(\"Training Testing Accuracy: %.2f%% (%.2f%%)\" % (np.mean(results), np.std(results)))\n",
    "    print(\"Holdout Accuracy: %.2f%%\" % (pcent))\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 170396)\n",
      "(617, 1)\n",
      "(155, 170396)\n",
      "(155, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgill/.local/lib/python3.6/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:04] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:43:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is85.71428571428571\n",
      "[10:43:31] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:43:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is92.06349206349206\n",
      "[10:44:01] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:44:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is93.65079365079364\n",
      "[10:44:31] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:44:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is90.47619047619048\n",
      "[10:45:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:45:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is90.32258064516128\n",
      "[10:45:28] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:45:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is88.52459016393442\n",
      "[10:46:01] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:46:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is91.80327868852459\n",
      "[10:46:33] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:46:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is78.68852459016394\n",
      "[10:47:02] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:47:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is76.66666666666667\n",
      "[10:47:32] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:47:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is70.0\n",
      "85.80645161290322\n",
      "Training Testing Accuracy: 85.79% (7.55%)\n",
      "Holdout Accuracy: 85.81%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(tt_vcf.shape)\n",
    "print(tt_pheno.shape)\n",
    "print(ho_vcf.shape)\n",
    "print(ho_pheno.shape)\n",
    " #if optimised in same session, other enter manually below\n",
    "#this function should average out 10 folds and training, with inital params optimised\n",
    "#average accuracy and std should be calculated along with a nice AUROC graph of train/test models\n",
    "#best model should be extracted for use on holdout set\n",
    "best_model = eval_k_fold(model, tt_vcf, tt_pheno, 10, ho_vcf, ho_pheno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(best_model, open(\"PoC_kfold_10_XGB_QTL.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only load if not generated in same session\n",
    "best_model = pickle.load(open(\"PoC_kfold_10_XGB_QTL.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO SNPS of importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "#best_model = pickle.load(open(\"FC_kfold_10_tt_from_all.pickle.dat\", \"rb\"))\n",
    "plt.figure(figsize = (20, 20))\n",
    "plot_importance(best_model, max_num_features=15, importance_type='gain', height=0.3)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my X value is: 28943\n",
      "(617, 28943)\n",
      "my header list is: 28943\n",
      "my X value is: 57392\n",
      "(617, 28449)\n",
      "my header list is: 57392\n",
      "my X value is: 86590\n",
      "(617, 29198)\n",
      "my header list is: 86590\n",
      "my X value is: 116146\n",
      "(617, 29556)\n",
      "my header list is: 116146\n",
      "my X value is: 145140\n",
      "(617, 28994)\n",
      "my header list is: 145140\n",
      "dropping value so it doesn't include that in headers\n",
      "my X value is: 170396\n",
      "(617, 25256)\n",
      "my header list is: 170396\n",
      "170396\n"
     ]
    }
   ],
   "source": [
    "#This function essentially returns an array of dataframe headers the length of OHE'd input SNPs for training data\n",
    "#EG. It will be able to determine that feature 357310 is Gm13_17683957 but not what allele it is\n",
    "#eg. feature 357309 357310 and 357311 may all be one hot encoded versions of all possible values of Gm13_17683957\n",
    "#iterating through the saved OHE will by able to determine what specific allele the feature is but cannot determine\n",
    "#what SNP header it belongs to. Therefore combining these two methods you can determine both allele and SNP\n",
    "snp = []\n",
    "imp = SimpleImputer(missing_values='./.', strategy='most_frequent')\n",
    "fs_ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "x = 0\n",
    "n_headers = []\n",
    "le = LabelEncoder()\n",
    "#while (i < 10):\n",
    "for chunk in pd.read_csv(\"PoC_Merged_filtered.csv_train_testQTL_SNPS.csv\", chunksize=10000, index_col=\"Unnamed: 0\"):\n",
    "    chunk = chunk.T\n",
    "    if 'Value' in chunk.columns:\n",
    "        print(\"dropping value so it doesn't include that in headers\")\n",
    "        chunk = chunk.drop(columns=['Value'])\n",
    "    headers = chunk.columns\n",
    "    row_idx = chunk.index\n",
    "    chunk = imp.fit_transform(chunk) #SHOULD TURN ./. into the most common for each column\n",
    "    #since imputing makes a numpy array have to turn back into PD for label encoding\n",
    "    chunk = pd.DataFrame(data = chunk, index = row_idx, columns = headers)\n",
    "    chunk = chunk.apply(lambda col: le.fit_transform(col))\n",
    "    c_headers = chunk.columns\n",
    "    y = 0\n",
    "    for column in chunk:\n",
    "        d = (chunk[column].nunique())\n",
    "        n_headers.extend([c_headers[y] for i in range(d)])\n",
    "        #print(n_headers)\n",
    "        #print(l)\n",
    "        #n_headers.append(c_headers[y] * d)\n",
    "        #print(n_headers)\n",
    "        y = y + 1\n",
    "    #to double check that it would indeed be one hot encoded with this amount of columns\n",
    "    chunk = fs_ohe.fit_transform(chunk)\n",
    "    x = x + chunk.shape[1]\n",
    "    print(\"my X value is: \" + str(x))\n",
    "    print(chunk.shape)\n",
    "    print(\"my header list is: \" + str(len(n_headers)))\n",
    "print(len(n_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gm05_11968079 (A/A)\n",
      "Gm01_55053811 (T/T)\n",
      "Gm02_41961414 (A/A)\n",
      "Gm17_6746570 (C/C)\n",
      "Gm08_183702 (G/G)\n",
      "Gm19_6434610 (A/A)\n",
      "Gm07_17356456 (A/A)\n",
      "Gm02_49207571 (C/C)\n",
      "Gm12_18950685 (A/A)\n",
      "Gm13_16076593 (G/G)\n",
      "Gm07_45624847 (C/C)\n",
      "Gm07_42559917 (T/T)\n",
      "Gm05_6303805 (A/A)\n",
      "Gm19_4824644 (A/A)\n",
      "Gm03_642218 (C/C)\n",
      "15\n",
      "15\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(10, 10), dpi=100, facecolor='w', edgecolor='k')\n",
    "fs = [136052,30299,54586,479986,211027,561565,198737,59473,336396,356471,210689,209071,133882,560181,61728]\n",
    "scores = [39.5,38.1,34.4,32.3,32.2,29.2,26.2,23.7,23.5,22.113,22.107,21.58,21.56,19.2,18.6]\n",
    "snp_label = []\n",
    "for jj in fs:\n",
    "    jj_allele = find_snp_from_header(ohe, jj)\n",
    "    this_snp = (n_headers[jj] + ' ('+str(jj_allele)+')')\n",
    "    print(this_snp)\n",
    "    snp_label.append(this_snp)\n",
    "snp_label.reverse()\n",
    "scores.reverse()\n",
    "print(len(scores))\n",
    "print(len(snp_label))\n",
    "plt.barh(snp_label,scores)\n",
    "plt.title('SNP Importance XGBoost Pod Colour')\n",
    "plt.ylabel('SNP Label')\n",
    "plt.xlabel('Relative F_Score (GAIN)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = best_model.get_booster().get_score(importance_type=\"gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_f_header(fn,n_headers,ohe):\n",
    "    fn = fn[1:]\n",
    "    fn = int(fn)\n",
    "    allele = find_snp_from_header(ohe, fn)\n",
    "    this_snp = (n_headers[fn] + ' ('+str(allele)+')')\n",
    "    return this_snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n"
     ]
    }
   ],
   "source": [
    "#convert feature to actual SNP name\n",
    "i = 0\n",
    "new_dict = {}\n",
    "for key in my_dict:\n",
    "    new_key = rename_f_header(key, n_headers, ohe)\n",
    "    new_dict[new_key] = my_dict[key]\n",
    "    i = i + 1\n",
    "    print(str(i))\n",
    "    if(my_dict):\n",
    "        continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gm03_642300 (T/T)      84.748451\n",
      "Gm01_55047009 (A/A)    37.480871\n",
      "Gm01_54543776 (C/C)     0.706393\n",
      "Gm03_632020 (A/A)       5.790105\n",
      "Gm07_45619468 (C/C)     8.493835\n",
      "                         ...    \n",
      "Gm02_41613819 (A/A)     0.044076\n",
      "Gm07_42668674 (T/T)     0.053545\n",
      "Gm19_39270651 (A/A)     0.059054\n",
      "Gm02_42040940 (T/T)     0.029073\n",
      "Gm03_749993 (G/G)       0.090292\n",
      "Length: 1844, dtype: float64\n",
      "                     F_Score(GAIN)\n",
      "Gm03_642300 (T/T)        84.748451\n",
      "Gm01_55047009 (A/A)      37.480871\n",
      "Gm01_54543776 (C/C)       0.706393\n",
      "Gm03_632020 (A/A)         5.790105\n",
      "Gm07_45619468 (C/C)       8.493835\n",
      "...                            ...\n",
      "Gm02_41613819 (A/A)       0.044076\n",
      "Gm07_42668674 (T/T)       0.053545\n",
      "Gm19_39270651 (A/A)       0.059054\n",
      "Gm02_42040940 (T/T)       0.029073\n",
      "Gm03_749993 (G/G)         0.090292\n",
      "\n",
      "[1844 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "new_fi = pd.Series(new_dict)\n",
    "print(new_fi)\n",
    "df = new_fi.to_frame()\n",
    "df = df.rename(columns = {0:'F_Score(GAIN)'})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(10, 10), dpi=100, facecolor='w', edgecolor='k')\n",
    "indexes = df.nlargest(20, \"F_Score(GAIN)\").index\n",
    "values = df.nlargest(20, \"F_Score(GAIN)\").values.ravel()\n",
    "indexes = indexes[::-1]\n",
    "values = values[::-1]\n",
    "plt.barh(indexes, values)\n",
    "plt.title('SNP Importance XGBoost Pod Colour')\n",
    "plt.ylabel('SNP Label')\n",
    "plt.xlabel('Relative F_Score (GAIN)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9YAAANICAYAAAAihXeBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdeVzU1f4/8NcgMCADiLKJIAgkIIogpBc1cR+JwsquphaKS2BaailEcVMzRa+3zDTRciOXXCqTXHGjUCdBL+OCiuC+YrkMCIgK5/eHPz5fP86wOVdNez0fj/O4M2c/M9TtPefz+RyFEEKAiIiIiIiIiB6KyZOeABEREREREdHTjIE1ERERERERkREYWBMREREREREZgYE1ERERERERkREYWBMREREREREZgYE1ERERERERkREYWBMREREREREZgYE1ERERERERkREYWBMREREREREZgYE1ERERET0xnTt3RufOnZ/I2Onp6VAoFEhPT38i4xPRs4OBNRERPfUOHTqE119/He7u7rCwsECTJk3Qo0cPzJ49W1bPw8MDCoUC7777rl4flf+B/cMPP0h5S5YsgUKhkJKFhQWaN2+OUaNGoaCgoMZ5KRQKjBo1yvgFPiF79uzBxIkTcePGjSc9FaPdunUL3t7e8PX1xe3bt/XKw8PDYWtri4sXL8ryr1y5gg8//BCtWrWCSqWChYUFvL29ER0djV27dsnqPvj3olAo4OjoiC5dumDTpk2PdH21UVJSgokTJ9Y6iKz8Z6IymZmZwdPTE1FRUTh58uSjnWwVysvLsXjxYnTu3BkNGzaEUqmEh4cHoqOjsW/fvicyJyIigIE1ERE95fbs2YOQkBAcOHAAw4cPx5w5czBs2DCYmJhg1qxZBtt8++23egFUdT799FMsXboUc+bMQfv27ZGcnIzQ0FCUlJT8r5bxl7Rnzx5MmjTpmQisLSwskJycjNzcXCQlJcnKVq5cic2bN2PKlClwcXGR8jMzM+Hv748vv/wSwcHBmD59OubMmYN+/fohMzMTL7zwAn777Te9sSr/Xr777jvExcXhjz/+wIsvvoj169c/8nVWp6SkBJMmTarz7ux7772HpUuX4ptvvkFERARWrVqF559/vk7/DP0vlJaW4qWXXsKQIUMghMBHH32E5ORkREVFQaPRoG3btjh//vxjnRMRUSXTJz0BIiIiY0yZMgW2trbIyspCgwYNZGVXrlzRq+/v74/c3FxMmzYNX331Va3GCA8PR0hICABg2LBhaNSoEb744gusW7cO/fv3N3oNfzXFxcWwsrJ60tP4n+vRowcGDBiApKQk9O/fH82bN8eNGzcwduxYPP/883jnnXekutevX8crr7wCU1NTaLVa+Pr6yvr67LPPsHLlSlhaWuqNc//fCwAMHToUTk5O+P777/HSSy89ugU+Ii+88AJef/11AEB0dDSaN2+O9957DykpKUhISHhs8xg/fjw2b96MmTNnYsyYMbKyCRMmYObMmY9tLsa6e/cuKioqYG5u/qSnQkT/I9yxJiKip9qJEyfg7++vF1QDgKOjo16eh4cHoqKi6rxrfb+uXbsCAE6dOlWndpWX1q5evRqTJk1CkyZNYG1tjddffx06nQ5lZWUYM2YMHB0doVKpEB0djbKyMlkflZeXL1++HD4+PrCwsEBwcLDBndPs7GyEh4fDxsYGKpUK3bp1w++//y6rU3n58q+//op33nkHjo6OcHV1xcSJEzF+/HgAQLNmzaTLgU+fPg0AWLx4Mbp27QpHR0colUq0aNECycnJenPw8PDASy+9hF27dqFt27awsLCAp6cnvvvuO726lUGuh4cHlEolXF1dERUVhT///FOqU1ZWhgkTJsDb2xtKpRJubm6Ii4vT+5yqMnPmTNSvXx+xsbEAgA8//BB//PEH5s+fDxOT//vPonnz5uHSpUv48ssv9YJq4N730L9/fzz//PM1jtmgQQNYWlrC1FS+n1FcXIwPPvgAbm5uUCqV8PHxwX/+8x8IIWT17t69i8mTJ8PLy0u69Pmjjz7SW/O+ffugVqthb28PS0tLNGvWDEOGDAEAnD59Gg4ODgCASZMmSd/nxIkTa/7QHmDo73/u3Lnw9/eHUqmEi4sLRo4cafBKh2+++QZeXl6wtLRE27ZtkZGRUasxz58/j/nz56NHjx56QTUA1KtXD+PGjYOrq6uUV5u//6qsWbMGwcHBsLS0hL29Pd58801cuHBBVqeqe8MHDx4MDw8P6f3p06ehUCjwn//8B19++aX0PR45cqRWcyGipwN3rImI6Knm7u4OjUaDw4cPo2XLlrVq8/HHH+O7776r0671/U6cOAEAaNSoUZ3bAkBSUhIsLS3x4YcfIj8/H7Nnz4aZmRlMTExw/fp1TJw4Eb///juWLFmCZs2a4ZNPPpG1//XXX7Fq1Sq89957UCqVmDt3Lnr16oXMzEzpM8jJycELL7wAGxsbxMXFwczMDPPnz0fnzp3x66+/ol27drI+33nnHTg4OOCTTz5BcXExwsPDcfz4cXz//feYOXMm7O3tAUAKzpKTk+Hv74/IyEiYmpril19+wTvvvIOKigqMHDlS1nd+fj5ef/11DB06FIMGDcKiRYswePBgBAcHw9/fHwBw8+ZNvPDCCzh69CiGDBmCNm3a4M8//0RqairOnz8Pe3t7VFRUIDIyErt27cLbb78NPz8/HDp0CDNnzsTx48fx888/1/jZOzo6Ytq0aYiJicG7776Lb775BmPGjEFQUJCs3i+//AJLS0u89tprtf9i/z+dToc///wTQghcuXIFs2fPxs2bN/Hmm29KdYQQiIyMxM6dOzF06FAEBgZiy5YtGD9+PC5cuCDbfR02bBhSUlLw+uuv44MPPsDevXuRlJSEo0ePYu3atQDuXZ3Rs2dPODg44MMPP0SDBg1w+vRp/PTTT9L3lpycjBEjRuDVV1+V1hUQEFDn9T349z9x4kRMmjQJ3bt3x4gRI5Cbm4vk5GRkZWVh9+7dMDMzAwAsXLgQMTExaN++PcaMGYOTJ08iMjISDRs2hJubW7Vjbtq0CXfv3sVbb71VqznW9e//fkuWLEF0dDSef/55JCUloaCgALNmzcLu3buRnZ1t8Ee82li8eDFu3bqFt99+G0qlEg0bNnyofojoL0oQERE9xdLS0kS9evVEvXr1RGhoqIiLixNbtmwRt2/f1qvr7u4uIiIihBBCREdHCwsLC3Hx4kUhhBA7d+4UAMSaNWuk+osXLxYAxLZt28Qff/whzp07J1auXCkaNWokLC0txfnz56udGwAxcuRI6X3lGC1btpTNr3///kKhUIjw8HBZ+9DQUOHu7q7XJwCxb98+Ke/MmTPCwsJCvPrqq1LeK6+8IszNzcWJEyekvIsXLwpra2vRqVMnvTV27NhR3L17VzbWjBkzBABx6tQpvbWVlJTo5anVauHp6SnLc3d3FwDEb7/9JuVduXJFKJVK8cEHH0h5n3zyiQAgfvrpJ71+KyoqhBBCLF26VJiYmIiMjAxZ+bx58wQAsXv3br22hlRUVIgOHToIAMLNzU0UFRXp1bGzsxOBgYF6+YWFheKPP/6Q0s2bN6Wyys/ywaRUKsWSJUtk/fz8888CgPjss89k+a+//rpQKBQiPz9fCCGEVqsVAMSwYcNk9caNGycAiB07dgghhFi7dq0AILKysqpc9x9//CEAiAkTJlT/Af1/lX+vixYtEn/88Ye4ePGi2LBhg/Dw8BAKhUJkZWWJK1euCHNzc9GzZ09RXl4utZ0zZ47UVgghbt++LRwdHUVgYKAoKyuT6n3zzTcCgAgLC6t2LmPHjhUARHZ2dq3mXtu//8o17ty5UzbPli1bitLSUqne+vXrBQDxySefSHlhYWEG5z1o0CDZP7enTp0SAISNjY24cuVKreZPRE8fXgpORERPtR49ekCj0SAyMhIHDhzAv//9b6jVajRp0gSpqalVtktMTMTdu3cxbdq0Gsfo3r07HBwc4ObmhjfeeAMqlQpr165FkyZNHmrOUVFR0i4eALRr1w5CCOmy3fvzz507h7t378ryQ0NDERwcLL1v2rQpevfujS1btqC8vBzl5eVIS0vDK6+8Ak9PT6le48aNMWDAAOzatQuFhYWyPocPH4569erVeg3331tcuUMbFhaGkydPQqfTyeq2aNECL7zwgvTewcEBPj4+sidL//jjj2jdujVeffVVvbEUCgWAe5fn+vn5wdfXF3/++aeUKi9N3rlzZ63mrlAopN3C0NBQqFQqvTqFhYUG89966y04ODhIKT4+Xq/O119/ja1bt2Lr1q1YtmwZunTpgmHDhkm7xwCwceNG1KtXD++9956s7QcffAAhhPQU8Y0bNwIA3n//fb16ALBhwwYAkHZR169fjzt37tTqc6itIUOGwMHBAS4uLoiIiEBxcTFSUlIQEhKCbdu24fbt2xgzZozsUvrhw4fDxsZGmt++fftw5coVxMbGyu4rHjx4MGxtbWucQ+Xfq7W1dY11H+bvv1LlPN955x1YWFhI+REREfD19ZXW8zD69OkjXfFBRM8eBtZERPTUe/755/HTTz/h+vXryMzMREJCAoqKivD6669XeR+jp6cn3nrrLXzzzTe4dOlStf1XBko7d+7EkSNHcPLkSajV6oeeb9OmTWXvKwOLBy+HtbW1RUVFhV6g+txzz+n12bx5c5SUlOCPP/7AH3/8gZKSEvj4+OjV8/PzQ0VFBc6dOyfLb9asWZ3WsHv3bnTv3h1WVlZo0KABHBwc8NFHHwGA3nwfXC8A2NnZ4fr169L7EydO1Hgpf15eHnJycmSBrYODA5o3bw7A8MPqDPnpp5/wyy+/oGXLllizZo3B+3ytra1x8+ZNvfxPP/1UCpqr0rZtW3Tv3h3du3fHwIEDsWHDBrRo0QKjRo2Sjvo6c+YMXFxc9AJFPz8/qbzyf01MTODt7S2r5+zsjAYNGkj1wsLC0KdPH0yaNAn29vbo3bs3Fi9eXOt7z6vzySefYOvWrdixYwcOHjyIixcvSpdkV47/4N+aubk5PD09ZesA9P92K4/wqomNjQ0AoKioqMa6D/P3X6mq9QCAr6+vVP4w6vrPGBE9XXiPNRERPTPMzc3x/PPP4/nnn0fz5s0RHR2NNWvWYMKECQbrf/zxx1i6dCmmT5+OV155pcp+27ZtK3vKs7Gq2hmuKl888DCrR8HQ062rcuLECXTr1g2+vr744osv4ObmBnNzc2zcuBEzZ85ERUWFrP7/al0VFRVo1aoVvvjiC4PlNd2nC9wLzN577z0EBwdj586dCAgIwIgRI5CdnS27isDX1xcHDhzAnTt3ZPkPc0+yiYkJunTpglmzZiEvL0+6r7wuKnftqyv/4Ycf8Pvvv+OXX37Bli1bMGTIEHz++ef4/fffDe6+11arVq3QvXv3h27/v1D5ALlDhw4hMDDwic6lkkKhMPg3XF5ebrB+Xf4ZI6KnD3esiYjomVQZCFe3G+3l5YU333wT8+fPr3HX+q8kLy9PL+/48eOoX7++tItbv3595Obm6tU7duwYTExMahWEVhXM/fLLLygrK0NqaipiYmLw4osvonv37kYFDl5eXjh8+HCNda5du4Zu3bpJO8L3J0O7jA9KTEzEpUuXMH/+fFhbW2P27NnIycnB559/Lqv30ksvobS0VHo4mLEqL+ev3AV3d3fHxYsX9XZgjx07JpVX/m9FRYXed15QUIAbN25I9Sr94x//wJQpU7Bv3z4sX74cOTk5WLlyJYCag/OHUTn+g39rt2/fxqlTp2TrAPT/du/cuVOrp+uHh4ejXr16WLZsWY11jfn7r2o9lXn3f952dnYGn3xuzK42ET29GFgTEdFTbefOnQZ3jSrvTa0p2EpMTMSdO3fw73//+5HM71HQaDT473//K70/d+4c1q1bh549e6JevXqoV68eevbsiXXr1knHYwH3grEVK1agY8eO0qW11ak8y/rB4KFyB/r+z12n02Hx4sUPvaY+ffrgwIEDBgPZynH69u2LCxcu4Ntvv9WrU1paiuLi4mrH2L9/P77++muMGjVKukf9pZdewquvvorJkyfLAqIRI0bAyckJY8eOxfHjx6ucU23cuXMHaWlpMDc3ly71fvHFF1FeXo45c+bI6s6cORMKhQLh4eFSPQD48ssvZfUqd+0jIiIA3Dt3+8E5Ve7sVl4OXr9+fQD636cxunfvDnNzc3z11Vey8RcuXAidTifNLyQkBA4ODpg3b550OTxw7wnctZmPm5sbhg8fjrS0NMyePVuvvKKiAp9//jnOnz9v1N9/SEgIHB0dMW/ePNll9Js2bcLRo0el9QD3fug5duwY/vjjDynvwIED2L17d43rIaJnDy8FJyKip9q7776LkpISvPrqq/D19cXt27exZ88erFq1Ch4eHoiOjq62feWudUpKymOasfFatmwJtVotO24LuHc+caXPPvsMW7duRceOHfHOO+/A1NQU8+fPR1lZWa1/RKgMPj/++GO88cYbMDMzw8svv4yePXvC3NwcL7/8MmJiYnDz5k18++23cHR0fOid//Hjx+OHH37AP//5TwwZMgTBwcG4du0aUlNTMW/ePLRu3RpvvfUWVq9ejdjYWOzcuRMdOnRAeXk5jh07htWrV2PLli1VXrJfXl6Ot99+G87Ozvjss89kZbNmzUKLFi3w7rvvSg+8a9iwIdauXYuXX34ZrVu3xhtvvIHnn38eZmZmOHfuHNasWQPA8P3jmzZtknaer1y5ghUrViAvLw8ffvihFNC9/PLL6NKlCz7++GOcPn0arVu3RlpaGtatW4cxY8bAy8sLANC6dWsMGjQI33zzDW7cuIGwsDBkZmYiJSUFr7zyCrp06QIASElJwdy5c/Hqq6/Cy8sLRUVF+Pbbb2FjYyMF55aWlmjRogVWrVqF5s2bo2HDhmjZsmWtj6kzxMHBAQkJCZg0aRJ69eqFyMhI5ObmYu7cuXj++eelI8bMzMzw2WefISYmBl27dkW/fv1w6tQpLF68uFb3WAPA559/jhMnTuC9997DTz/9hJdeegl2dnY4e/Ys1qxZg2PHjuGNN94A8PB//2ZmZpg+fTqio6MRFhaG/v37S8dteXh4YOzYsVLdIUOG4IsvvoBarcbQoUNx5coVzJs3D/7+/lU+HI2InmFP6GnkRERE/xObNm0SQ4YMEb6+vkKlUglzc3Ph7e0t3n33XVFQUCCre/9xW/fLy8sT9erVq/K4reqOMKoOqjhu6/4xqhtnwoQJAoD4448/9PpctmyZeO6554RSqRRBQUHScUH3++9//yvUarVQqVSifv36okuXLmLPnj21GrvS5MmTRZMmTYSJiYns6K3U1FQREBAgLCwshIeHh5g+fbpYtGiR3vFcVX3mho4qunr1qhg1apRo0qSJMDc3F66urmLQoEHizz//lOrcvn1bTJ8+Xfj7+wulUins7OxEcHCwmDRpktDpdAbXIIQQM2fOFADEDz/8YLD8P//5j8Hjvi5duiTGjx8vWrRoISwtLYVSqRSenp4iKipKdoSYEIaP27KwsBCBgYEiOTlZOjasUlFRkRg7dqxwcXERZmZm4rnnnhMzZszQq3fnzh0xadIk0axZM2FmZibc3NxEQkKCuHXrllTnv//9r+jfv79o2rSpUCqVwtHRUbz00kuyY9mEEGLPnj0iODhYmJub13j0VlV/r4bMmTNH+Pr6CjMzM+Hk5CRGjBghrl+/rldv7ty5olmzZkKpVIqQkBDx22+/VXlslSF3794VCxYsEC+88IKwtbUVZmZmwt3dXURHR+sdxVWbv/8Hj9uqtGrVKhEUFCSUSqVo2LChGDhwoMHj9ZYtWyY8PT2Fubm5CAwMFFu2bKnyuK0ZM2bUao1E9HRSCPEYnohCRERE/xMKhQIjR47Uu4SYiIiInhzeY01ERERERERkBAbWREREREREREZgYE1ERERERERkBD4VnIiI6CnCR6MQERH99XDHmoiIiIiIiMgIDKyJiIiIiIiIjMBLwYkeUFFRgYsXL8La2hoKheJJT4eIiIiIiJ4QIQSKiorg4uICE5Oq96UZWBM94OLFi3Bzc3vS0yAiIiIior+Ic+fOwdXVtcpyBtZED7C2tgZw7x8eGxubJzwbIiIiIiJ6UgoLC+Hm5ibFCFVhYE30gMrLv21sbBhYExERERFRjbeI8uFlREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREYwfdITIPrLsrV90jMgIiIiIvp7EeJJz+ChcMeaiIiIiIiIyAgMrImIiIiIiIiMwMCaiIiIiIiIyAgMrImIiIiIiIiMwMCaiIiIiIiIyAgMrImIiIiIiIiMwMCaiIiIiIiIyAgMrImIiIiIiIiMwMCanojt27fDz88P5eXlj3Scf/zjH/jxxx8f6RhERERERPT39swF1pcvX8bo0aPh7e0NCwsLODk5oUOHDkhOTkZJSYnR/aenp6NNmzZQKpXw9vbGkiVLZOXJyckICAiAjY0NbGxsEBoaik2bNtVpDI1Gg65du8LKygo2Njbo1KkTSktL9eqVlZUhMDAQCoUCWq1WNsfevXujcePGsLKyQmBgIJYvXy5r+9NPPyEkJAQNGjSQ6ixdulRWRwiBTz75BI0bN4alpSW6d++OvLw8WZ1r165h4MCBsLGxQYMGDTB06FDcvHmzxjXGxcUhMTER9erVQ+fOnaFQKKpMnTt3lrVt1qwZXF1dq23j4eEBAEhMTMSHH36IioqKGudERERERET0UMQz5MSJE8LZ2Vn4+vqKVatWiSNHjogTJ06In3/+Wbz44oti3bp1RvV/8uRJUb9+ffH++++LI0eOiNmzZ4t69eqJzZs3S3VSU1PFhg0bxPHjx0Vubq746KOPhJmZmTh8+HCtxtizZ4+wsbERSUlJ4vDhw+LYsWNi1apV4tatW3p133vvPREeHi4AiOzsbCl/ypQpIjExUezevVvk5+eLL7/8UpiYmIhffvlFqrNz507x008/iSNHjkh1HlzLtGnThK2trfj555/FgQMHRGRkpGjWrJkoLS2V6vTq1Uu0bt1a/P777yIjI0N4e3uL/v37V7vGjIwMYWtrK/Vz9epVcenSJXHp0iWRmZkpAIht27ZJeVevXpXaHjhwQNja2oobN25I5ZcuXRIAxOLFi6X3V65cEUIIcffuXeHk5CTWr19fq89fCCF0Op0AIHSAEExMTExMTExMTExMjy/9xUixgU5Xbb2/3syNoFarhaurq7h586bB8oqKCuk1ADFv3jwREREhLC0tha+vr9izZ4/Iy8sTYWFhon79+iI0NFTk5+dLbeLi4oS/v7+sz379+gm1Wl3tvOzs7MSCBQtqtYZ27dqJxMTEGutt3LhR+Pr6ipycHPFgYG3Iiy++KKKjo6utExQUJI1dUVEhnJ2dxYwZM6TyGzduCKVSKb7//nshhBBHjhwRAERWVpZUZ9OmTUKhUIgLFy5UOc7IkSPF66+/brDs1KlT1a7n008/Ff369dPLByDWrl1rsE10dLR48803q5zPgxhYMzExMTExMTExMT2h9BdT28D6mbkU/OrVq0hLS8PIkSNhZWVlsI5CoZC9nzx5MqKioqDVauHr64sBAwYgJiYGCQkJ2LdvH4QQGDVqlFRfo9Gge/fusj7UajU0Go3B8crLy7Fy5UoUFxcjNDS0xjVcuXIFe/fuhaOjI9q3bw8nJyeEhYVh165dsnoFBQUYPnw4li5divr169fYLwDodDo0bNjQYJkQAtu3b0dubi46deoEADh16hQuX74sW6+trS3atWsnrVej0aBBgwYICQmR6nTv3h0mJibYu3dvlXPJyMiQtamL1NRU9O7du05t2rZti4yMjCrLy8rKUFhYKEtERERERES19cwE1vn5+RBCwMfHR5Zvb28PlUoFlUqF+Ph4WVl0dDT69u2L5s2bIz4+HqdPn8bAgQOhVqvh5+eH0aNHIz09Xap/+fJlODk5yfpwcnJCYWGh7B7oQ4cOQaVSQalUIjY2FmvXrkWLFi1qXMPJkycBABMnTsTw4cOxefNmtGnTBt26dZPubRZCYPDgwYiNja11cLp69WpkZWUhOjpalq/T6aBSqWBubo6IiAjMnj0bPXr0kNZaub4H11tZdvnyZTg6OsrKTU1N0bBhQ6mOIWfOnIGLi0ut5n6/Cxcu4ODBgwgPD69TOxcXF5w7d67K+6yTkpJga2srJTc3tzrPjYiIiIiI/r6emcC6KpmZmdBqtfD390dZWZmsLCAgQHpdGUC2atVKlnfr1q0672D6+PhAq9Vi7969GDFiBAYNGoQjR47U2K4y8IuJiUF0dDSCgoIwc+ZM+Pj4YNGiRQCA2bNno6ioCAkJCbWay86dOxEdHY1vv/0W/v7+sjJra2totVpkZWVhypQpeP/992U/JDwqpaWlsLCwqHO71NRUdOzYEQ0aNKhTO0tLS1RUVOh9/5USEhKg0+mkdO7cuTrPjYiIiIiI/r5Mn/QE/le8vb2hUCiQm5sry/f09ARwL7h6kJmZmfS68jJxQ3mVAa+zszMKCgpkfRQUFMDGxkbWv7m5Oby9vQEAwcHByMrKwqxZszB//vxq19C4cWMA0Nvd9vPzw9mzZwEAO3bsgEajgVKplNUJCQnBwIEDkZKSIuX9+uuvePnllzFz5kxERUXpjWdiYiLNMzAwEEePHkVSUhI6d+4MZ2dnaX2V86p8HxgYKH0eV65ckfV59+5dXLt2TWpviL29Pa5fv17tZ2FIamoqIiMj69zu2rVrsLKyMvg3AABKpVLv8yQiIiIiIqqtZ2bHulGjRujRowfmzJmD4uLiRzJGaGgotm/fLsvbunVrjfdPV7dbej8PDw+4uLjo/Thw/PhxuLu7AwC++uorHDhwAFqtFlqtFhs3bgQArFq1ClOmTJHapKenIyIiAtOnT8fbb79dq/XdP89mzZrB2dlZtt7CwkLs3btXWm9oaChu3LiB/fv3S3V27NiBiooKtGvXrspxgoKCarWDf7+bN29i586ddb6/GgAOHz6MoKCgOrcjIiIiIiKqjWdmxxoA5s6diw4dOiAkJAQTJ05EQEAATExMkJWVhWPHjiE4ONio/mNjYzFnzhzExcVhyJAh2LFjB1avXo0NGzZIdRISEhAeHo6mTZuiqKgIK1asQHp6OrZs2VJj/wqFAuPHj8eECRPQunVrBAYGIiUlBceOHcMPP/wAAGjatKmsjUqlAgB4eXnB1dUVwL3Lv1966SWMHj0affr0ke53Njc3lx5glpSUhJCQEHh5eaGsrAwbN27E0qVLkZycLM1lzJgx+Oyzz/Dcc8+hWbNm+Ne//gUXFxe88sorAO7tpPfq1QvDhw/HvHnzcOfOHYwaNQpvvPFGtfdQq9Vq2c56bWzevBnNmzeXzqeui4yMDPTs2bPO7YiIiIiIiGrlMTyh/LG6ePGiGDVqlGjWrJkwMzMTKpVKtG3bVsyYMUMUFxdL9QD58UyGjnnauXOnACCuX78uywsMDBTm5ubC09NTLF68WDb+kCFDhLu7uzA3NxcODg6iW7duIi0trU5rSEpKEq6urtKRXxkZGVXWNTTvQYMGCQB6KSwsTKrz8ccfC29vb2FhYSHs7OxEaGioWLlypazviooK8a9//Us4OTkJpVIpunXrJnJzc2V1rl69Kvr37y9UKpWwsbER0dHRoqioqNr1Xb16VVhYWIhjx47Vaj1CCPHmm2+Kjz/+uMo+H/w+K50/f16YmZmJc+fOVTun+/G4LSYmJiYmJiYmJqYnlP5ianvclkIIIZ5IRE9/a+PHj0dhYWGN950D9+7bdnJywqZNm9C2bds6jRMfH4/r16/jm2++qXWbwsJC2NraQgfApk6jERERERGRUf5i4akUG+h0sLGpOjp4Zu6xpqfLxx9/DHd39yqPwLrftWvXMHbsWDz//PN1HsfR0RGTJ09+mCkSERERERHVCnesH6Ply5cjJibGYJm7uztycnIe84zIEO5YExERERE9IX+x8LS2O9bP1MPL/uoiIyOrfFr2/cd8ERERERER0dODgfVjZG1tDWtr6yc9DSIiIiIiIvof4j3WREREREREREZgYE1ERERERERkBAbWREREREREREbgPdZEVdHpgGqe/EdERERERARwx5qIiIiIiIjIKAysiYiIiIiIiIzAwJqIiIiIiIjICAysiYiIiIiIiIzAwJqIiIiIiIjICAysiYiIiIiIiIzA47aIqmJr+6RnQERExhLiSc+AiIj+BrhjTURERERERGQEBtZERERERERERmBgTURERERERGQEBtZERERERERERmBgTURERERERGQEBtZERERERERERmBgTURERERERGQEBtZERERERERERmBgTY9Vbm4unJ2dUVRU9FjGe+ONN/D5558/lrGIiIiIiOjv6ZkJrC9fvozRo0fD29sbFhYWcHJyQocOHZCcnIySkhKj+09PT0ebNm2gVCrh7e2NJUuWyMqTk5MREBAAGxsb2NjYIDQ0FJs2barTGBqNBl27doWVlRVsbGzQqVMnlJaW6tUrKytDYGAgFAoFtFqtlD9x4kQoFAq9ZGVlZXC8lStXQqFQ4JVXXpHlCyHwySefoHHjxrC0tET37t2Rl5cnqzNlyhS0b98e9evXR4MGDWq9xoSEBLz77ruwtrbWK/P19YVSqcTly5erbN+lSxcsWLBAlqdWq1GvXj1kZWXp1U9MTMSUKVOg0+lqPUciIiIiIqK6eCYC65MnTyIoKAhpaWmYOnUqsrOzodFoEBcXh/Xr12Pbtm1G9X/q1ClERESgS5cu0Gq1GDNmDIYNG4YtW7ZIdVxdXTFt2jTs378f+/btQ9euXdG7d2/k5OTUagyNRoNevXqhZ8+eyMzMRFZWFkaNGgUTE/2vKC4uDi4uLnr548aNw6VLl2SpRYsW+Oc//6lX9/Tp0xg3bhxeeOEFvbJ///vf+OqrrzBv3jzs3bsXVlZWUKvVuHXrllTn9u3b+Oc//4kRI0bUan0AcPbsWaxfvx6DBw/WK9u1axdKS0vx+uuvIyUlxWD7a9euYffu3Xj55Zdlfe7ZswejRo3CokWL9Nq0bNkSXl5eWLZsWa3nSUREREREVCfiGaBWq4Wrq6u4efOmwfKKigrpNQAxb948ERERISwtLYWvr6/Ys2ePyMvLE2FhYaJ+/foiNDRU5OfnS23i4uKEv7+/rM9+/foJtVpd7bzs7OzEggULarWGdu3aicTExBrrbdy4Ufj6+oqcnBwBQGRnZ1dZV6vVCgDit99+k+XfvXtXtG/fXixYsEAMGjRI9O7dWyqrqKgQzs7OYsaMGVLejRs3hFKpFN9//73eGIsXLxa2trY1zlsIIWbMmCFCQkIMlg0ePFh8+OGHYtOmTaJ58+YG63z33XeiXbt2sryJEyeKN954Qxw9elTY2tqKkpISvXaTJk0SHTt2rHJet27dEjqdTkrnzp0TAIQOEIKJiYmJ6elORERERtDpdAKA0Ol01dZ76nesr169irS0NIwcObLKS54VCoXs/eTJkxEVFQWtVgtfX18MGDAAMTExSEhIwL59+yCEwKhRo6T6Go0G3bt3l/WhVquh0WgMjldeXo6VK1eiuLgYoaGhNa7hypUr2Lt3LxwdHdG+fXs4OTkhLCwMu3btktUrKCjA8OHDsXTpUtSvX7/GfhcsWIDmzZvr7Up/+umncHR0xNChQ/XanDp1CpcvX5at19bWFu3atatyvbWVkZGBkJAQvfyioiKsWbMGb775Jnr06AGdToeMjAy9eqmpqejdu7f0XgiBxYsX480334Svry+8vb3xww8/6LVr27YtMjMzUVZWZnBeSUlJsLW1lZKbm5sRqyQiIiIior+bpz6wzs/PhxACPj4+snx7e3uoVCqoVCrEx8fLyqKjo9G3b180b94c8fHxOH36NAYOHAi1Wg0/Pz+MHj0a6enpUv3Lly/DyclJ1oeTkxMKCwtl90AfOnQIKpUKSqUSsbGxWLt2LVq0aFHjGk6ePAng3j3Sw4cPx+bNm9GmTRt069ZNurdZCIHBgwcjNjbWYHD6oFu3bmH58uV6wfOuXbuwcOFCfPvttwbbVd7fbGi91d37XBtnzpwxeAn7ypUr8dxzz8Hf3x/16tXDG2+8gYULF8rqlJWVYfPmzYiMjJTytm3bhpKSEqjVagDAm2++qdcOAFxcXHD79u0q55+QkACdTielc+fOGbNMIiIiIiL6m3nqA+uqZGZmQqvVwt/fX2+nMiAgQHpdGUC2atVKlnfr1i0UFhbWaUwfHx9otVrs3bsXI0aMwKBBg3DkyJEa21VUVAAAYmJiEB0djaCgIMycORM+Pj7SfcOzZ89GUVEREhISajWXtWvXoqioCIMGDZLyioqK8NZbb+Hbb7+Fvb19ndb2v1BaWgoLCwu9/EWLFuHNN9+U3r/55ptYs2aN7MnhO3bsgKOjI/z9/WXt+vXrB1NTUwBA//79sXv3bpw4cULWv6WlJQBU+RA7pVIpPXSuMhEREREREdXWUx9Ye3t7Q6FQIDc3V5bv6ekJb29vKai6n5mZmfS68jJxQ3mVAa+zszMKCgpkfRQUFMDGxkbWv7m5Oby9vREcHIykpCS0bt0as2bNqnENjRs3BgC93W0/Pz+cPXsWwL3AUqPRQKlUwtTUFN7e3gCAkJAQWfBcacGCBXjppZdkO88nTpzA6dOn8fLLL8PU1BSmpqb47rvvkJqaClNTU5w4cQLOzs7S+h5cb2XZw7K3t8f169dleUeOHMHvv/+OuLg4aU7/+Mc/UFJSgpUrV0r1UlNTZbvV165dw9q1azF37lypXZMmTXD37l29h5hdu3YNAODg4GDU/ImIiIiIiAx56gPrRo0aoUePHpgzZw6Ki4sfyRihoaHYvn27LG/r1q013j9dUVFR5X299/Pw8ICLi4vejwPHjx+Hu7s7AOCrr77CgQMHoNVqodVqsXHjRgDAqlWrMGXKFFm7U6dOYefOnXqXgfv6+uLQoUNSH1qtFpGRkdLTzt3c3NCsWTM4OzvL1ltYWIi9e/fW6n7x6gQFBent4C9cuBCdOnWSrU2r1eL999+XLusWQuCXX36R3V+9fPlyuLq66rX7/PPPsWTJEpSXl0t1Dx8+DFdX1yeyS09ERERERM8+0yc9gf+FuXPnokOHDggJCcHEiRMREBAAExMTZGVl4dixYwgODjaq/9jYWMyZMwdxcXEYMmQIduzYgdWrV2PDhg1SnYSEBISHh6Np06YoKirCihUrkJ6eLjuSqyoKhQLjx4/HhAkT0Lp1awQGBiIlJQXHjh2THsbVtGlTWRuVSgUA8PLygqurq6xs0aJFaNy4McLDw2X5FhYWaNmypSyv8gzq+/PHjBmDzz77DM899xyaNWuGf/3rX3BxcZGdd3327Flcu3YNZ8+eRXl5uXSetre3tzS3B6nVagwbNgzl5eWoV68e7ty5g6VLl+LTTz/Vm9ewYcPwxRdfICcnB6WlpSgpKUHHjh2l8oULF+L111/Xa+fm5oaEhARs3rwZERERAO49NK1nz54G50RERERERGSsZyKw9vLyQnZ2NqZOnYqEhAScP38eSqUSLVq0wLhx4/DOO+8Y1X+zZs2wYcMGjB07FrNmzYKrqysWLFggPTQLuPdk76ioKFy6dAm2trYICAjAli1b0KNHj1qNMWbMGNy6dQtjx47FtWvX0Lp1a2zduhVeXl51mmtFRQWWLFmCwYMHo169enVqWykuLg7FxcV4++23cePGDXTs2BGbN2+W3R/9ySefyM6bDgoKAgDs3LkTnTt3NthveHg4TE1NsW3bNqjVaqSmpuLq1at49dVX9er6+fnBz88PCxcuhJWVFV588UXpXur9+/fjwIEDBh/AZmtri27dumHhwoWIiIjArVu38PPPP2Pz5s0P9VkQERERERHVRCGEEE96EvT38fXXXyM1NbVWO/mVAgICkJiYiL59+9Z5vOTkZKxduxZpaWm1blNYWAhbW1voAPAxZkRETzn+Zw4RERlBig10umofcvxM7FjT0yMmJgY3btxAUVERrK2ta6x/+/Zt9OnTR++y9toyMzPD7NmzH6otERERERFRbXDH+jFYvnw5YmJiDJa5u7sjJyfnMc+IqsMdayKiZwj/M4eIiIzAHeu/kMjISLRr185g2f3HfBEREREREdHTh4H1Y2BtbV2ry56JiIiIiIjo6fPUn2NNRERERERE9CQxsCYiIiIiIiIyAi8FJ6qKTgdU84ACIiIiIiIigDvWREREREREREZhYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBAbWREREREREREZgYE1ERERERERkBB63RVQVW9snPQMior8uIZ70DIiIiP4yuGNNREREREREZAQG1kRERERERERGYGBNREREREREZAQG1kRERERERERGYGBNREREREREZAQG1kRERERERERGYGBNREREREREZAQG1kRERERERERGYGBNT0Rubi6cnZ1RVFT0SMd544038Pnnnz/SMYiIiIiI6O/tmQusL1++jNGjR8Pb2xsWFhZwcnJChw4dkJycjJKSEqP7T09PR5s2baBUKuHt7Y0lS5bIypOTkxEQEAAbGxvY2NggNDQUmzZtqtMYGo0GXbt2hZWVFWxsbNCpUyeUlpbq1SsrK0NgYCAUCgW0Wq2Un5ubiy5dusDJyQkWFhbw9PREYmIi7ty5I2u/Zs0a+Pr6wsLCAq1atcLGjRtl5QqFwmCaMWOGrN6GDRvQrl07WFpaws7ODq+88kqNa0xISMC7774La2trDB48uMqxFAoFPDw8ZG27dOkCV1fXatsoFAoAQGJiIqZMmQKdTlfjnIiIiIiIiB7GMxVYnzx5EkFBQUhLS8PUqVORnZ0NjUaDuLg4rF+/Htu2bTOq/1OnTiEiIgJdunSBVqvFmDFjMGzYMGzZskWq4+rqimnTpmH//v3Yt28funbtit69eyMnJ6dWY2g0GvTq1Qs9e/ZEZmYmsrKyMGrUKJiY6H9VcXFxcHFx0cs3MzNDVFQU0tLSkJubiy+//BLffvstJkyYINXZs2cP+vfvj6FDhyI7OxuvvPIKXnnlFRw+fFiqc+nSJVlatGgRFAoF+vTpI9X58ccf8dZbbyE6OhoHDhzA7t27MWDAgGrXePbsWaxfvx6DBw8GAMyaNUs2DgAsXrxYep+VlSW1vXbtGnbv3o3du3fL2ri6uuLTTz/V66dly5bw8vLCsmXLavHpExERERERPQTxDFGr1cLV1VXcvHnTYHlFRYX0GoCYN2+eiIiIEJaWlsLX11fs2bNH5OXlibCwMFG/fn0RGhoq8vPzpTZxcXHC399f1me/fv2EWq2udl52dnZiwYIFtVpDu3btRGJiYo31Nm7cKHx9fUVOTo4AILKzs6utP3bsWNGxY0fpfd++fUVERITe2DExMVX20bt3b9G1a1fp/Z07d0STJk1qvbZKM2bMECEhIVWWAxBr1641WPbdd9+Jdu3a6eW7u7uLmTNnGmwzadIk2dprotPpBAChA4RgYmJiYjKciIiI/gak2ECnq7beM7NjffXqVaSlpWHkyJGwsrIyWKfy8uBKkydPRlRUFLRaLXx9fTFgwADExMQgISEB+/btgxACo0aNkuprNBp0795d1odarYZGozE4Xnl5OVauXIni4mKEhobWuIYrV65g7969cHR0RPv27eHk5ISwsDDs2rVLVq+goADDhw/H0qVLUb9+/Rr7zc/Px+bNmxEWFvbQaykoKMCGDRswdOhQKe+///0vLly4ABMTEwQFBaFx48YIDw+X7XobkpGRgZCQkBrnbUhqaip69+5dpzZt27ZFZmYmysrKDJaXlZWhsLBQloiIiIiIiGrrmQms8/PzIYSAj4+PLN/e3h4qlQoqlQrx8fGysujoaPTt2xfNmzdHfHw8Tp8+jYEDB0KtVsPPzw+jR49Genq6VP/y5ctwcnKS9eHk5ITCwkLZPdCHDh2CSqWCUqlEbGws1q5dixYtWtS4hpMnTwIAJk6ciOHDh2Pz5s1o06YNunXrhry8PACAEAKDBw9GbGxsjcFp+/btYWFhgeeeew4vvPACPv300xrXcvnyZYN9paSkwNraGq+99prB+SYmJmL9+vWws7ND586dce3atSrndebMGYOXsNekrKwMmzdvRmRkZJ3aubi44Pbt21WuLSkpCba2tlJyc3Or89yIiIiIiOjv65kJrKuSmZkJrVYLf39/vR3LgIAA6XVlkNmqVStZ3q1bt+q8g+nj4wOtVou9e/dixIgRGDRoEI4cOVJju4qKCgBATEwMoqOjERQUhJkzZ8LHxweLFi0CAMyePRtFRUVISEiosb9Vq1bhv//9L1asWIENGzbgP//5T53Wcb9FixZh4MCBsLCw0Jvvxx9/jD59+iA4OBiLFy+GQqHAmjVrquyrtLRU1k9t7dixA46OjvD3969TO0tLSwCo8uF1CQkJ0Ol0Ujp37lyd50ZERERERH9fpk96Av8r3t7eUCgUyM3NleV7enoC+L/g6n5mZmbS68rLxA3lVQaQzs7OKCgokPVRUFAAGxsbWf/m5ubw9vYGAAQHByMrKwuzZs3C/Pnzq11D48aNAUBvd9vPzw9nz54FcC+41Gg0UCqVsjohISEYOHAgUlJSpLzKndcWLVqgvLwcb7/9Nj744APUq1evyrU4OzvrzSsjIwO5ublYtWpVjfNVKpXw9PSU5muIvb09rl+/XmV5VVJTU+u8Ww1A2j13cHAwWK5UKvU+TyIiIiIiotp6ZnasGzVqhB49emDOnDkoLi5+JGOEhoZi+/btsrytW7fWeP90RUVFlff33s/DwwMuLi56Pw4cP34c7u7uAICvvvoKBw4cgFarhVarlY7IWrVqFaZMmVLtHO7cuSP9SFCXtSxcuBDBwcFo3bq1LD84OBhKpVI23zt37uD06dPSfA0JCgqq1Q7+/YQQ+OWXX+p8fzUAHD58GK6urrC3t69zWyIiIiIiopo8MzvWADB37lx06NABISEhmDhxIgICAmBiYoKsrCwcO3YMwcHBRvUfGxuLOXPmIC4uDkOGDMGOHTuwevVqbNiwQaqTkJCA8PBwNG3aFEVFRVixYgXS09NlR3JVRaFQYPz48ZgwYQJat26NwMBApKSk4NixY/jhhx8AAE2bNpW1UalUAAAvLy+4uroCAJYvXw4zMzO0atUKSqUS+/btQ0JCAvr16yftyI8ePRphYWH4/PPPERERgZUrV2Lfvn345ptvZP0XFhZizZo1+Pzzz/Xma2Njg9jYWEyYMAFubm5wd3eXzrj+5z//WeU61Wo1hg0bhvLyctSrV6/GzwUA9u/fj5KSEnTs2LFW9e+XkZGBnj171rkdERERERFRbTxTgbWXlxeys7MxdepUJCQk4Pz581AqlWjRogXGjRuHd955x6j+mzVrhg0bNmDs2LGYNWsWXF1dsWDBAqjVaqnOlStXEBUVhUuXLsHW1hYBAQHYsmULevToUasxxowZg1u3bmHs2LG4du0aWrduja1bt8LLy6vW8zQ1NcX06dNx/PhxCCHg7u6OUaNGYezYsVKd9u3bY8WKFUhMTMRHH32E5557Dj///DNatmwp62vlypUQQqB///4Gx5oxYwZMTU3x1ltvobS0FO3atcOOHTtgZ2dX5fzCw8NhamqKbdu2yT676qxbtw4vvvgiTE3r9id769Yt/Pzzz9i8eXOd2hEREREREdWWQgghnvQk6O/n66+/Rmpqaq128oF7D5pLTExE37596zROcnIy1q5di7S0tFq3KSwshK2tLXQAbOo0GhHR3wj/84GIiP4GpNhAp4ONTdXRwTO1Y01Pj5iYGNy4cQNFRUWwtrautu7t27fRp08fhIeH13kcMzMzzJ49+2GnSUREREREVCPuWD9Gy5cvR0xMjMEyd3d35OTkPOYZkSHcsSYiqgX+5wMREf0NcMf6LygyMhLt2rUzWHb/MV9ERERERET09GBg/RhZW1vXeNkzERERERERPV2emXOsiYiIiIiIiJ4EBtZERERERERERmBgTURERERERGQE3mNNVBWdDqjmyX9EREREREQAd6yJiIiIiIiIjMLAmoiIiIiIiMgIDKyJiIiIiIiIjMDAmoiIiIiIiMgIDKyJiIiIiIiIjMDAmoiIiIiIiMgIPG6LqCq2tk96BkTPDiGe9AyIiIiIHhnuWBMREREREREZgYE1ERERERERkREYWBMREREREREZgYE1ERERERERkREYWBMREREREREZgYE1ERERERERkREYWBMREREREREZgYE1ERERERERkREYWNMTcfXqVTg6OuL06dOPdJwPP/wQ77777iMdg4iIiIiI/t6eucD68uXLGD16NLy9vWFhYQEnJyd06NABycnJKCkpMbr/9PR0tGnTBkqlEt7e3liyZImsPDk5GQEBAbCxsYGNjQ1CQ0OxadOmOo2h0WjQtWtXWFlZwcbGBp06dUJpaalevbKyMgQGBkKhUECr1crm2Lt3bzRu3BhWVlYIDAzE8uXLZW2//fZbvPDCC7Czs4OdnR26d++OzMxMWZ2ffvoJPXv2RKNGjfTGqHT58mW89dZbcHZ2hpWVFdq0aYMff/yxxjVOmTIFvXv3hoeHByZOnAiFQlFtul90dDRcXV1rbHP69GmMGzcOKSkpOHnyZI1zIiIiIiIiehjPVGB98uRJBAUFIS0tDVOnTkV2djY0Gg3i4uKwfv16bNu2zaj+T506hYiICHTp0gVarRZjxozBsGHDsGXLFqmOq6srpk2bhv3792Pfvn3o2rUrevfujZycnFqNodFo0KtXL/Ts2ROZmZnIysrCqFGjYGKi/1XFxcXBxcVFL3/Pnj0ICAjAjz/+iIMHDyI6OhpRUVFYv369VCc9PR39+/fHzp07odFo4Obmhp49e+LChQtSneLiYnTs2BHTp0+vcr5RUVHIzc1FamoqDh06hNdeew19+/ZFdnZ2lW1KSkqwcOFCDB06FAAwbtw4XLp0SUqurq749NNPZXmVysvLsX79eixfvlxWHhoaiuHDh8vy3NzcYG9vD7VajeTk5Oo/eCIiIiIiooclniFqtVq4urqKmzdvGiyvqKiQXgMQ8+bNExEREcLS0lL4+vqKPXv2iLy8PBEWFibq168vQkNDRX5+vtQmLi5O+Pv7y/rs16+fUKvV1c7Lzs5OLFiwoFZraNeunUhMTKyx3saNG4Wvr6/IyckRAER2dna19V988UURHR1dZfndu3eFtbW1SElJ0Ss7depUlWNYWVmJ7777TpbXsGFD8e2331Y51po1a4SDg0OV5e7u7mLmzJkGy3777TfRuHFj2XcphBBhYWFi9OjRBtukpKQIV1fXKsd7kE6nEwCEDhCCiYnpf5OIiIiInkJSbKDTVVvvmdmxvnr1KtLS0jBy5EhYWVkZrPPgJcWTJ09GVFQUtFotfH19MWDAAMTExCAhIQH79u2DEAKjRo2S6ms0GnTv3l3Wh1qthkajMTheeXk5Vq5cieLiYoSGhta4hitXrmDv3r1wdHRE+/bt4eTkhLCwMOzatUtWr6CgAMOHD8fSpUtRv379GvsFAJ1Oh4YNG1ZZXlJSgjt37lRbx5D27dtj1apVuHbtGioqKrBy5UrcunULnTt3rrJNRkYGgoOD6zROpdTUVLz88st632V12rZti/Pnz1d5P3dZWRkKCwtliYiIiIiIqLaemcA6Pz8fQgj4+PjI8u3t7aFSqaBSqRAfHy8ri46ORt++fdG8eXPEx8fj9OnTGDhwINRqNfz8/DB69Gikp6dL9S9fvgwnJydZH05OTigsLJTdA33o0CGoVCoolUrExsZi7dq1aNGiRY1rqLwPeOLEiRg+fDg2b96MNm3aoFu3bsjLywMACCEwePBgxMbGIiQkpFafzerVq5GVlYXo6Ogq68THx8PFxUXvh4Pa9H3nzh00atQISqUSMTExWLt2Lby9vatsc+bMGYOXsNfGunXrEBkZWac2lWOdOXPGYHlSUhJsbW2l5Obm9lBzIyIiIiKiv6dnJrCuSmZmJrRaLfz9/VFWViYrCwgIkF5XBsytWrWS5d26davOO5g+Pj7QarXYu3cvRowYgUGDBuHIkSM1tquoqAAAxMTEIDo6GkFBQZg5cyZ8fHywaNEiAMDs2bNRVFSEhISEWs1l586diI6Oxrfffgt/f3+DdaZNm4aVK1di7dq1sLCwqOUq7/nXv/6FGzduYNu2bdi3bx/ef/999O3bF4cOHaqyTWlpaZ3HAYCjR4/i4sWL6NatW53aWVpaAkCVD69LSEiATqeT0rlz5+o8NyIiIiIi+vsyfdIT+F/x9vaGQqFAbm6uLN/T0xPA/wVX9zMzM5NeV15abCivMuB1dnZGQUGBrI+CggLY2NjI+jc3N5d2bIODg5GVlYVZs2Zh/vz51a6hcePGAKC3u+3n54ezZ88CAHbs2AGNRgOlUimrExISgoEDByIlJUXK+/XXX/Hyyy9j5syZiIqKMjjmf/7zH0ybNg3btm2T/dBQGydOnMCcOXNw+PBhKWhv3bo1MjIy8PXXX2PevHkG29nb2+P69et1Ggu4dxl4jx496hyUX7t2DQDg4OBgsFypVOp9nkRERERERLX1zOxYN2rUCD169MCcOXNQXFz8SMYIDQ3F9u3bZXlbt26t8f7piooKvd1yQzw8PODi4qL348Dx48fh7u4OAPjqq69w4MABaLVaaLVabNy4EQCwatUqTJkyRWqTnp6OiIgITJ8+HW+//bbB8f79739j8uTJ2Lx5c60vK79f5Q7wg08sr1evnvRjhCFBQUG12sF/0Lp169C7d+86tzt8+DDMzMyq3LEnIiIiIiIyxjOzYw0Ac+fORYcOHRASEoKJEyciICAAJiYmyMrKwrFjxx76gVmVYmNjMWfOHMTFxWHIkCHYsWMHVq9ejQ0bNkh1EhISEB4ejqZNm6KoqAgrVqxAenq67EiuqigUCowfPx4TJkxA69atERgYiJSUFBw7dgw//PADAKBp06ayNiqVCgDg5eUFV1dXAPcu/37ppZcwevRo9OnTB5cvXwZwbye98uFk06dPxyeffIIVK1bAw8NDqlN5Pzpwb6f37NmzuHjxIgBIAb+zszOcnZ3h6+sLb29vxMTE4D//+Q8aNWqEn3/+GVu3bpUd7fUgtVqNhIQEXL9+HXZ2djV+LsC9B7vt27cPqamptap/v4yMDLzwwgsGr1ogIiIiIiIy2uN4RPnjdPHiRTFq1CjRrFkzYWZmJlQqlWjbtq2YMWOGKC4uluoBEGvXrpXeGzpSaufOnQKAuH79uiwvMDBQmJubC09PT7F48WLZ+EOGDBHu7u7C3NxcODg4iG7duom0tLQ6rSEpKUm4urpKR35lZGRUWdfQvAcNGiQA6KWwsDCpjru7u8E6EyZMkOosXry4xjrHjx8Xr732mnB0dBT169cXAQEBesdvGdK2bVsxb948g2WGjttasGCB6NChQ5X9VXfclo+Pj/j+++9rnFMlHrfFxPQIEhEREdFTqLbHbSmEEOKxRvJEADZs2IDx48fj8OHDepeSGxIZGYmOHTsiLi6uTuNs2rQJH3zwAQ4ePAhT09pdoFFYWAhbW1voANjUaTQiqhL/r4aIiIieQlJsoNPBxqbq6OCZuhScnh4RERHIy8vDhQsXanW8VceOHdG/f/86j1NcXIzFixfXOqgmIiIiIiKqK+5YP0bLly9HTEyMwTJ3d3fk5OQ85hmRIdyxJnoE+H81RERE9BTijvVfUGRkJNq1a2ew7P5jvoiIiIiIiOjpwcD6MbK2toa1tfWTngYRERERERH9Dz0z51gTERERERERPQkMrImIiIiIiIiMwMCaiIiIiIiIyAi8x5qoKjodUM2T/4iIiIiIiADuWBMREREREREZhYE1ERERERERkREYWBMREREREREZgYE1ERERERERkREYWBMREREREREZgYE1ERERERERkRF43BZRVWxtn/QM6O9GiCc9AyIiIiJ6CNyxJiIiIiIiIjICA2siIiIiIiIiIzCwJiIiIiIiIjICA2siIiIiIiIiIzCwJiIiIiIiIjICA2siIiIiIiIiIzCwJiIiIiIiIjICA2siIiIiIiIiIzCwpiciNzcXzs7OKCoqeqTjvPHGG/j8888f6RhERERERPT39swF1pcvX8bo0aPh7e0NCwsLODk5oUOHDkhOTkZJSYnR/aenp6NNmzZQKpXw9vbGkiVLZOXJyckICAiAjY0NbGxsEBoaik2bNtVpDI1Gg65du8LKygo2Njbo1KkTSktL9eqVlZUhMDAQCoUCWq1WVnbw4EG88MILsLCwgJubG/7973/Lyn/66SeEhISgQYMGsLKyQmBgIJYuXao3xtGjRxEZGQlbW1tYWVnh+eefx9mzZ6Xyzp07Q6FQyFJsbGyNa0xISMC7774La2trDB48WK+P+5OHh4esbZcuXeDq6lptG4VCAQBITEzElClToNPpapwTERERERHRw3imAuuTJ08iKCgIaWlpmDp1KrKzs6HRaBAXF4f169dj27ZtRvV/6tQpREREoEuXLtBqtRgzZgyGDRuGLVu2SHVcXV0xbdo07N+/H/v27UPXrl3Ru3dv5OTk1GoMjUaDXr16oWfPnsjMzERWVhZGjRoFExP9ryouLg4uLi56+YWFhejZsyfc3d2xf/9+zJgxAxMnTsQ333wj1WnYsCE+/vhjaDQaHDx4ENHR0YiOjpat5cSJE+jYsSN8fX2Rnp6OgwcP4l//+hcsLCxk4w0fPhyXLl2S0oNB/IPOnj2L9evXY/DgwQCAWbNmydoDwOLFi6X3WVlZUttr165h9+7d2L17t6yNq6srPv30U71+WrZsCS8vLyxbtqyGT56IiIiIiOghiWeIWq0Wrq6u4ubNmwbLKyoqpNcAxLx580RERISwtLQUvr6+Ys+ePSIvL0+EhYWJ+vXri9DQUJGfny+1iYuLE/7+/rI++/XrJ9RqdbXzsrOzEwsWLKjVGtq1aycSExNrrLdx40bh6+srcnJyBACRnZ0tlc2dO1fY2dmJsrIyKS8+Pl74+PhU22dQUJBs7H79+ok333yz2jZhYWFi9OjRNc73fjNmzBAhISFVlgMQa9euNVj23XffiXbt2unlu7u7i5kzZxpsM2nSJNGxY8cqx7t165bQ6XRSOnfunAAgdIAQTEyPMxERERHRX4pOpxMAhE6nq7beM7NjffXqVaSlpWHkyJGwsrIyWKfy8uBKkydPRlRUFLRaLXx9fTFgwADExMQgISEB+/btgxACo0aNkuprNBp0795d1odarYZGozE4Xnl5OVauXIni4mKEhobWuIYrV65g7969cHR0RPv27eHk5ISwsDDs2rVLVq+goADDhw/H0qVLUb9+fb1+NBoNOnXqBHNzc9k8c3Nzcf36db36Qghs374dubm56NSpEwCgoqICGzZsQPPmzaFWq+Ho6Ih27drh559/1mu/fPly2Nvbo2XLlkhISKjxkvuMjAyEhITU+HkYkpqait69e9epTdu2bZGZmYmysjKD5UlJSbC1tZWSm5vbQ82NiIiIiIj+np6ZwDo/Px9CCPj4+Mjy7e3toVKpoFKpEB8fLyuLjo5G37590bx5c8THx+P06dMYOHAg1Go1/Pz8MHr0aKSnp0v1L1++DCcnJ1kfTk5OKCwslN0DfejQIahUKiiVSsTGxmLt2rVo0aJFjWs4efIkAGDixIkYPnw4Nm/ejDZt2qBbt27Iy8sDcC8IHjx4MGJjY6sMTquaZ2VZJZ1OB5VKBXNzc0RERGD27Nno0aMHgHtB/s2bNzFt2jT06tULaWlpePXVV/Haa6/h119/lfoYMGAAli1bhp07dyIhIQFLly7Fm2++We06z5w5Y/AS9pqUlZVh8+bNiIyMrFM7FxcX3L59W7b2+yUkJECn00np3LlzdZ4bERERERH9fZk+6Qk8apmZmaioqMDAgQP1diwDAgKk15WBZ6tWrWR5t27dQmFhIWxsbGo9po+PD7RaLXQ6HX744QcMGjQIv/76a43BdUVFBQAgJiYG0dHRAICgoCBs374dixYtQlJSEmbPno2ioiIkJCTUej5Vsba2hlarxc2bN7F9+3a8//778PT0ROfOnaW59O7dG2PHjgUABAYGYs+ePZg3bx7CwsIAAG+//bbUX6tWrdC4cWN069YNJ06cgJeXl8FxS0tL9e7Tro0dO3bA0dER/v7+dWpnaWkJAFXupCuVSiiVyjrPh4iIiIiICHiGAmtvb28oFArk5ubK8j09PQH8X3B1PzMzM+l15WXihvIqg0xnZ2cUFBTI+igoKICNjY2sf3Nzc3h7ewMAgoODkZWVhVmzZmH+/PnVrqFx48YAoBeA+/n5SU/i3rFjBzQajV4gGBISgoEDByIlJaXKeVauoZKJiYk0z8DAQBw9ehRJSUno3Lkz7O3tYWpqanAuD16afr927doBuHcFQVWBtb29vcFL0muSmppa591q4N4DzwDAwcGhzm2JiIiIiIhq8sxcCt6oUSP06NEDc+bMQXFx8SMZIzQ0FNu3b5flbd26tcb7pysqKqq8v/d+Hh4ecHFx0ftx4Pjx43B3dwcAfPXVVzhw4AC0Wi20Wi02btwIAFi1ahWmTJkizfO3337DnTt3ZPP08fGBnZ1dreZpbm6O559/vtq5GFJ57FfljwSGBAUF4ciRI1WWGyKEwC+//FLn+6sB4PDhw3B1dYW9vX2d2xIREREREdXkmdmxBoC5c+eiQ4cOCAkJwcSJExEQEAATExNkZWXh2LFjCA4ONqr/2NhYzJkzB3FxcRgyZAh27NiB1atXY8OGDVKdhIQEhIeHo2nTpigqKsKKFSuQnp4uO8aqKgqFAuPHj8eECRPQunVrBAYGIiUlBceOHcMPP/wAAGjatKmsjUqlAgB4eXnB1dUVwL37nidNmoShQ4ciPj4ehw8fxqxZszBz5kypXVJSEkJCQuDl5YWysjJs3LgRS5cuRXJyslRn/Pjx6NevHzp16oQuXbpg8+bN+OWXX6T7zk+cOIEVK1bgxRdfRKNGjXDw4EGMHTsWnTp1kl1m/yC1Wo1hw4ahvLwc9erVq/FzAYD9+/ejpKQEHTt2rFX9+2VkZKBnz551bkdERERERFQbz1Rg7eXlhezsbEydOhUJCQk4f/48lEolWrRogXHjxuGdd94xqv9mzZphw4YNGDt2LGbNmgVXV1csWLAAarVaqnPlyhVERUXh0qVLsLW1RUBAALZs2SI9FKwmY8aMwa1btzB27Fhcu3YNrVu3xtatW6u8rNoQW1tb6QnpwcHBsLe3xyeffCK7H7q4uBjvvPMOzp8/D0tLS/j6+mLZsmXo16+fVOfVV1/FvHnzkJSUhPfeew8+Pj748ccfpeDW3Nwc27Ztw5dffoni4mK4ubmhT58+SExMrHZ+4eHhMDU1xbZt22SfXXXWrVuHF198EaamdfuTvXXrFn7++Wds3ry5Tu2IiIiIiIhqSyGEEE96EvT38/XXXyM1NbVWO/nAvQfNJSYmom/fvnUaJzk5GWvXrkVaWlqt2xQWFsLW1hY6ALV/ZB3R/wD/dUxERET0lyLFBjpdtQ+0fqZ2rOnpERMTgxs3bqCoqAjW1tbV1r19+zb69OmD8PDwOo9jZmaG2bNnP+w0iYiIiIiIasQd68do+fLliImJMVjm7u6OnJycxzwjMoQ71vTE8F/HRERERH8p3LH+C4qMjJSOo3rQ/cd8ERERERER0dODgfVjZG1tXeNlz0RERERERPR0eWbOsSYiIiIiIiJ6EhhYExERERERERmBl4ITVUWnA6p5QAERERERERHAHWsiIiIiIiIiozCwJiIiIiIiIjICA2siIiIiIiIiIzCwJiIiIiIiIjICA2siIiIiIiIiIzCwJiIiIiIiIjICj9siqoqt7ZOeAT1uQjzpGRARERHRU4g71kRERERERERGYGBNREREREREZAQG1kRERERERERGYGBNREREREREZAQG1kRERERERERGYGBNREREREREZAQG1kRERERERERGYGBNREREREREZAQG1vTIXb16FY6Ojjh9+vRjHffPP/+Eo6Mjzp8//1jHJSIiIiKiv5enOrC+fPkyRo8eDW9vb1hYWMDJyQkdOnRAcnIySkpKjO4/PT0dbdq0gVKphLe3N5YsWSIrT05ORkBAAGxsbGBjY4PQ0FBs2rSpTmNoNBp07doVVlZWsLGxQadOnVBaWqpXr6ysDIGBgVAoFNBqtVL+xIkToVAo9JKVlZVUJycnB3369IGHhwcUCgW+/PJLvf6LioowZswYuLu7w9LSEu3bt0dWVpaszsSJE+Hr6wsrKyvY2dmhe/fu2Lt3b41rnDJlCnr37g0PDw9Z/o8//oiuXbvCzs4OlpaW8PHxwZAhQ5Cdna3XR0pKCjp27Ci9z8/Px5AhQ9C0aVMolUo0adIE3bp1w/Lly3H37l0AgL29PaKiojBhwoQa50hERERERPSwntrA+uTJkwgKCkJaWhqmTp2K7OxsaDQaxMXFYf369di2bZtR/Z86dQoRERHo0qULtFotxowZg2HDhmHLli1SHVdXV0ybNg379+/Hvn370LVrV/Tu3Rs5OTm1GkOj0aBXr17o2bMnMjMzkZWVhVGjRsHERP9riYuLg4uLi17+uHHjcOnSJVlq0aIF/vnPf0p1SkpK4OnpiWnTpsHZ2dngXIYNG4atW7di6dKlOHToEHr27Inu3bvjwoULUp3mzZtjzpw5OHToEHbt2gUPDw/07NkTf/zxR5VrLCkpwcKFCzF06FBZfnx8PPr164fAwECkpqYiNzcXK1asgKenJxISEvT6WbduHSIjIwEAmZmZaNOmDY4ePYqvv/4ahw8fRnp6OoYNG4bk5GTZ5x8dHY3ly5fj2rVrVc6RiIiIiIjIKOIppVarhaurq7h586bB8oqKCuk1ADFv3jwREREhLC0tha+vr9izZ4/Iy8sTYWFhon79+iI0NFTk5+dLbeLi4oS/v7+sz379+gm1Wl3tvOzs7MSCBQtqtYZ27dqJxMTEGutt3LhR+Pr6ipycHAFAZGdnV1lXq9UKAOK3334zWO7u7i5mzpwpyyspKRH16tUT69evl+W3adNGfPzxx1WOpdPpBACxbdu2KuusWbNGODg4yPI0Go0AIGbNmmWwzf3fnRBClJaWCisrK3H06FFRUVEh/Pz8RHBwsCgvL69V+2bNmtX6OxHi/9alA4Rg+nslIiIiIqL7SLGBTldtvadyx/rq1atIS0vDyJEjZZc830+hUMjeT548GVFRUdBqtfD19cWAAQMQExODhIQE7Nu3D0IIjBo1Sqqv0WjQvXt3WR9qtRoajcbgeOXl5Vi5ciWKi4sRGhpa4xquXLmCvXv3wtHREe3bt4eTkxPCwsKwa9cuWb2CggIMHz4cS5cuRf369Wvsd8GCBWjevDleeOGFGutWunv3LsrLy2FhYSHLt7S01JtPpdu3b+Obb76Bra0tWrduXWXfGRkZCA4OluV9//33UKlUeOeddwy2efC72759O5o0aQJfX19otVocPXoU48aNM7izb6h927ZtkZGRUeUcy8rKUFhYKEtERERERES19VQG1vn5+RBCwMfHR5Zvb28PlUoFlUqF+Ph4WVl0dDT69u2L5s2bIz4+HqdPn8bAgQOhVqvh5+eH0aNHIz09Xap/+fJlODk5yfpwcnJCYWGh7B7oQ4cOQaVSQalUIjY2FmvXrkWLFi1qXMPJkycB3Ltvefjw4di8eTPatGmDbt26IS8vDwAghMDgwYMRGxuLkJCQGvu8desWli9frnfZdU2sra0RGhqKyZMn4+LFiygvL8eyZcug0Whw6dIlWd3169dDpVLBwsICM2fOxNatW2Fvb19l32fOnNG7hP348ePw9PSEqamplPfFF19I351KpYJOp5PK7r8M/Pjx4wAg++6vXLkiazt37lzZeC4uLjhz5kyVc0xKSoKtra2U3NzcqqxLRERERET0oKcysK5KZmYmtFot/P39UVZWJisLCAiQXlcGzK1atZLl3bp1q867lT4+PtBqtdi7dy9GjBiBQYMG4ciRIzW2q6ioAADExMQgOjoaQUFBmDlzJnx8fLBo0SIAwOzZs1FUVGTwnmND1q5di6KiIgwaNKhOawCApUuXQgiBJk2aQKlU4quvvkL//v31doUr7znfs2cPevXqhb59++LKlStV9ltaWqq3E27IkCFDoNVqMX/+fBQXF0MIAeDejwu//PKLFFgb0qhRI2i1Wmi1WjRo0AC3b9+WlVtaWlb7MLuEhATodDopnTt3rsb5EhERERERVXoqA2tvb28oFArk5ubK8j09PeHt7Q1LS0u9NmZmZtLrykuFDeVVBrzOzs4oKCiQ9VFQUAAbGxtZ/+bm5vD29kZwcDCSkpLQunVrzJo1q8Y1NG7cGAD0drf9/Pxw9uxZAMCOHTug0WigVCphamoKb29vAEBISIjB4HnBggV46aWX9Hbaa8PLywu//vorbt68iXPnziEzMxN37tyBp6enrJ6VlRW8vb3xj3/8AwsXLoSpqSkWLlxYZb/29va4fv26LO+5557DyZMncefOHSmvQYMG8Pb2RpMmTWR1MzMzcffuXbRv315qC0D23derVw/e3t7w9vaW7YJXunbtGhwcHKqco1KplJ7sXpmIiIiIiIhq66kMrBs1aoQePXpgzpw5KC4ufiRjhIaGYvv27bK8rVu31nj/dEVFhd5uuSEeHh5wcXHR+3Hg+PHjcHd3BwB89dVXOHDggLQbu3HjRgDAqlWrMGXKFFm7U6dOYefOnXW+DPxBVlZWaNy4Ma5fv44tW7agd+/e1davab1BQUF6O/j9+/fHzZs39S7ZNmTdunWIiIhAvXr1pP58fX3xn//8R/oRpCaHDx9GUFBQreoSERERERHVlf723lNi7ty56NChA0JCQjBx4kQEBATAxMQEWVlZOHbsmN4Ds+oqNjYWc+bMQVxcHIYMGYIdO3Zg9erV2LBhg1QnISEB4eHhaNq0KYqKirBixQqkp6fLjuSqikKhwPjx4zFhwgS0bt0agYGBSElJwbFjx/DDDz8AAJo2bSpro1KpANzbXXZ1dZWVLVq0CI0bN0Z4eLjeWLdv35aC29u3b+PChQvQarVQqVTSLviWLVuk+9bz8/Mxfvx4+Pr6Ijo6GgBQXFyMKVOmIDIyEo0bN8aff/6Jr7/+GhcuXJAd7fUgtVqNhIQEXL9+HXZ2dgDu/WjxwQcf4IMPPsCZM2fw2muvwc3NDZcuXcLChQuhUCikS9BTU1Px6aefyj63xYsXo0ePHujQoQMSEhLg5+eHO3fu4LfffsMff/whBeHAveO+9u/fj6lTp9b4nRARERERET2UR/148kfp4sWLYtSoUaJZs2bCzMxMqFQq0bZtWzFjxgxRXFws1QMg1q5dK70/deqUAOTHVu3cuVMAENevX5flBQYGCnNzc+Hp6SkWL14sG3/IkCHC3d1dmJubCwcHB9GtWzeRlpZWpzUkJSUJV1dX6civjIyMKusamrcQQpSXlwtXV1fx0UcfVdvuwRQWFibVWbVqlfD09BTm5ubC2dlZjBw5Uty4cUMqLy0tFa+++qpwcXER5ubmonHjxiIyMlJkZmbWuMa2bduKefPm6eWvWrVKdO7cWdja2gozMzPh6uoqBgwYIH7//XchhBD5+flCqVQaPFItNzdXDBo0SLi6ugpTU1Nha2srOnXqJObPny/u3Lkj1VuxYoXw8fGpcY7343Fbf+NERERERHSf2h63pRBCiCcU09PfxIYNGzB+/HgcPny4yiOyDPniiy+wbds26RL4h/GPf/wD7733HgYMGFDrNoWFhbC1tYUOAO+2/pvhvw6JiIiI6D5SbKDTVfsspqf2UnB6ekRERCAvLw8XLlyo01FWrq6utX4iuiF//vknXnvtNfTv3/+h+yAiIiIiIqoJd6wfkeXLlyMmJsZgmbu7O3Jych7zjKi2uGP9N8Z/HRIRERHRfbhj/YRFRkaiXbt2BsvuP+aLiIiIiIiInm4MrB8Ra2trWFtbP+lpEBERERER0SP2VJ5jTURERERERPRXwcCaiIiIiIiIyAgMrImIiIiIiIiMwHusiaqi0wHVPPmPiIiIiIgI4I41ERERERERkVEYWBMREREREREZgYE1ERERERERkREYWBMREREREREZgYE1ERERERERkREYWBMREREREREZgcdtEVXF1vZJz4AeByGe9AyIiIiI6CnHHWsiIiIiIiIiIzCwJiIiIiIiIjICA2siIiIiIiIiIzCwJiIiIiIiIjICA2siIiIiIiIiIzCwJiIiIiIiIjICA2siIiIiIiIiIzCwJiIiIiIiIjICA2t6IrZv3w4/Pz+Ul5c/0nH+8Y9/4Mcff3ykYxARERER0d/bMxdYX758GaNHj4a3tzcsLCzg5OSEDh06IDk5GSUlJUb3n56ejjZt2kCpVMLb2xtLliyRlScnJyMgIAA2NjawsbFBaGgoNm3aVKcxNBoNunbtCisrK9jY2KBTp04oLS3Vq1dWVobAwEAoFApotVpZ2cGDB/HCCy/AwsICbm5u+Pe//y0r/+mnnxASEoIGDRrAysoKgYGBWLp0qazO4MGDoVAoZKlXr16yz+LB8sqUlZVV7Rrj4uKQmJiIevXqoXPnzlX2o1Ao0LlzZ1nbZs2awdXVtdo2Hh4eAIDExER8+OGHqKioqOFTJyIiIiIiejimT3oC/0snT55Ehw4d0KBBA0ydOhWtWrWCUqnEoUOH8M0336BJkyaIjIx86P5PnTqFiIgIxMbGYvny5di+fTuGDRuGxo0bQ61WAwBcXV0xbdo0PPfccxBCICUlBb1790Z2djb8/f1rHEOj0aBXr15ISEjA7NmzYWpqigMHDsDERP83kLi4OLi4uODAgQOy/MLCQvTs2RPdu3fHvHnzcOjQIQwZMgQNGjTA22+/DQBo2LAhPv74Y/j6+sLc3Bzr169HdHQ0HB0dpbUAQK9evbB48WLpvVKplF63b98ely5dko39r3/9C9u3b0dISEiVa9y1axdOnDiBPn36ALgX5N++fRsAcO7cObRt2xbbtm2TPi9zc3Op7cGDB3H9+nWcOXNG9mND48aNsXjxYinwr1evHgAgPDwcw4YNw6ZNmxAREVHlnIiIiIiIiB6aeIao1Wrh6uoqbt68abC8oqJCeg1AzJs3T0RERAhLS0vh6+sr9uzZI/Ly8kRYWJioX7++CA0NFfn5+VKbuLg44e/vL+uzX79+Qq1WVzsvOzs7sWDBglqtoV27diIxMbHGehs3bhS+vr4iJydHABDZ2dlS2dy5c4WdnZ0oKyuT8uLj44WPj0+1fQYFBcnGHjRokOjdu3et5i2EELdv3xYODg7i008/rbbeyJEjxeuvv26w7NSpU3rrud+nn34q+vXrp5cPQKxdu9Zgm+joaPHmm29WO6f76XQ6AUDoACGYnv1ERERERFQFKTbQ6aqt98xcCn716lWkpaVh5MiRsLKyMlhHoVDI3k+ePBlRUVHQarXw9fXFgAEDEBMTg4SEBOzbtw9CCIwaNUqqr9Fo0L17d1kfarUaGo3G4Hjl5eVYuXIliouLERoaWuMarly5gr1798LR0RHt27eHk5MTwsLCsGvXLlm9goICDB8+HEuXLkX9+vX1+tFoNOjUqZNsp1etViM3NxfXr1/Xqy+EwPbt25Gbm4tOnTrJytLT0+Ho6AgfHx+MGDECV69erXL+qampuHr1KqKjo6tdZ0ZGRrU72tVJTU1F796969Smbdu2yMjIqLK8rKwMhYWFskRERERERFRbz0xgnZ+fDyEEfHx8ZPn29vZQqVRQqVSIj4+XlUVHR6Nv375o3rw54uPjcfr0aQwcOBBqtRp+fn4YPXo00tPTpfqXL1+Gk5OTrA8nJycUFhbKLks+dOgQVCoVlEolYmNjsXbtWrRo0aLGNZw8eRIAMHHiRAwfPhybN29GmzZt0K1bN+Tl5QG4FwQPHjwYsbGxVQanVc2zsqySTqeDSqWCubk5IiIiMHv2bPTo0UMq79WrF7777jts374d06dPx6+//orw8PAqHzi2cOFCqNVquLq6VrvOM2fOwMXFpYZPQ9+FCxdw8OBBhIeH16mdi4sLzp07V+V91klJSbC1tZWSm5tbnedGRERERER/X8/UPdaGZGZmoqKiAgMHDkRZWZmsLCAgQHpdGXi2atVKlnfr1i0UFhbCxsam1mP6+PhAq9VCp9Phhx9+wKBBg/Drr7/WGFxXBn4xMTHSrm9QUBC2b9+ORYsWISkpCbNnz0ZRURESEhJqPZ+qWFtbQ6vV4ubNm9i+fTvef/99eHp6Sg8Le+ONN6S6rVq1QkBAALy8vJCeno5u3brJ+jp//jy2bNmC1atX1zhuaWkpLCws6jzf1NRUdOzYEQ0aNKhTO0tLS1RUVKCsrAyWlpZ65QkJCXj//fel94WFhQyuiYiIiIio1p6ZwNrb2xsKhQK5ubmyfE9PTwAwGFCZmZlJrysvEzeUVxnwOjs7o6CgQNZHQUEBbGxsZP2bm5vD29sbABAcHIysrCzMmjUL8+fPr3YNjRs3BgC9ANzPzw9nz54FAOzYsQMajUb2EDEACAkJwcCBA5GSklLlPCvXUMnExESaZ2BgII4ePYqkpCS9p3BX8vT0hL29PfLz8/UC68WLF6NRo0a1ejicvb29wUvSa5KamvpQD5+7du0arKysDP4NAPceyPbg50lERERERFRbz8yl4I0aNUKPHj0wZ84cFBcXP5IxQkNDsX37dlne1q1ba7x/unK3tCYeHh5wcXHR+3Hg+PHjcHd3BwB89dVXOHDgALRaLbRaLTZu3AgAWLVqFaZMmSLN87fffsOdO3dk8/Tx8YGdnd1Dz/P8+fO4evWq9ANAJSEEFi9ejKioKNkPE1UJCgrCkSNHaqx3v5s3b2Lnzp11vr8aAA4fPoygoKA6tyMiIiIiIqqNZ2bHGgDmzp2LDh06ICQkBBMnTkRAQABMTEyQlZWFY8eOITg42Kj+Y2NjMWfOHMTFxWHIkCHYsWMHVq9ejQ0bNkh1EhISEB4ejqZNm6KoqAgrVqxAeno6tmzZUmP/CoUC48ePx4QJE9C6dWsEBgYiJSUFx44dww8//AAAaNq0qayNSqUCAHh5eUn3Ng8YMACTJk3C0KFDER8fj8OHD2PWrFmYOXOm1C4pKQkhISHw8vJCWVkZNm7ciKVLlyI5ORnAvUB20qRJ6NOnD5ydnXHixAnExcXB29tbdhwXcG8X/dSpUxg2bFitPke1Wo2UlJRa1a20efNmNG/eXDqfui4yMjLQs2fPOrcjIiIiIiKqjWcqsPby8kJ2djamTp2KhIQEnD9/HkqlEi1atMC4cePwzjvvGNV/s2bNsGHDBowdOxazZs2Cq6srFixYIAs0r1y5gqioKFy6dAm2trYICAjAli1bZA8Fq86YMWNw69YtjB07FteuXUPr1q2xdetWeHl51Xqetra20hPSg4ODYW9vj08++UQ6wxoAiouL8c477+D8+fOwtLSEr68vli1bhn79+gG4dw70wYMHkZKSghs3bsDFxQU9e/bE5MmT9S6bXrhwIdq3bw9fX99azW/gwIGIi4tDbm6u3sPmqrJu3bqHugz8woUL2LNnD5YtW1bntkRERERERLWhEEKIJz0J+vsZP348CgsLa7zvHADu3r0LJycnbNq0CW3btq3TOPHx8bh+/Tq++eabWrcpLCyEra0tdABq/8g6emrxX4FEREREVAUpNtDpqn2g9TNzjzU9XT7++GO4u7tXeQTW/a5du4axY8fi+eefr/M4jo6OmDx58sNMkYiIiIiIqFa4Y/0YLV++HDExMQbL3N3dkZOT85hnRIZwx/pvhv8KJCIiIqIq1HbH+pm6x/qvLjIyEu3atTNYVpunaRMREREREdFfDwPrx8ja2hrW1tZPehpERERERET0P8R7rImIiIiIiIiMwMCaiIiIiIiIyAgMrImIiIiIiIiMwHusiaqi0wHVPPmPiIiIiIgI4I41ERERERERkVEYWBMREREREREZgYE1ERERERERkREYWBMREREREREZgYE1ERERERERkREYWBMREREREREZgcdtEVXF1vZJz4AeJSGe9AyIiIiI6BnBHWsiIiIiIiIiIzCwJiIiIiIiIjICA2siIiIiIiIiIzCwJiIiIiIiIjICA2siIiIiIiIiIzCwJiIiIiIiIjICA2siIiIiIiIiIzCwJiIiIiIiIjICA2t6bG7fvg1vb2/s2bPnsY3n4eGBffv2PZbxiIiIiIjo7+mZCKwvX76M0aNHw9vbGxYWFnByckKHDh2QnJyMkpISo/tPT09HmzZtoFQq4e3tjSVLlsjKk5OTERAQABsbG9jY2CA0NBSbNm2qVd+nT5+GQqEwmNasWSPVy8rKQrdu3dCgQQPY2dlBrVbjwIEDsr62bNmCf/zjH7C2toaDgwP69OmD06dPGxx39+7dMDU1RWBgoF7ZhQsX8Oabb6JRo0awtLREq1atZMHpTz/9hJ49e6JRo0ZQKBTQarW1Wuu8efPQrFkztG/fXpa/c+dOvPjii2jUqBHq16+PFi1a4IMPPsCFCxdk9X799Ve4ublJ7y9fvox3330Xnp6eUCqVcHNzw8svv4zt27cDAMzNzTFu3DjEx8fXan5EREREREQP46kPrE+ePImgoCCkpaVh6tSpyM7OhkajQVxcHNavX49t27YZ1f+pU6cQERGBLl26QKvVYsyYMRg2bBi2bNki1XF1dcW0adOwf/9+7Nu3D127dkXv3r2Rk5NTY/9ubm64dOmSLE2aNAkqlQrh4eEAgJs3b6JXr15o2rQp9u7di127dsHa2hpqtRp37tyR5tm7d2907doVWq0WW7ZswZ9//onXXntNb8wbN24gKioK3bp10yu7fv06OnToADMzM2zatAlHjhzB559/Djs7O6lOcXExOnbsiOnTp9f6cxRCYM6cORg6dKgsf/78+ejevTucnZ3x448/4siRI5g3bx50Oh0+//xzWd1169bh5ZdfBnDvB4ng4GDs2LEDM2bMwKFDh7B582Z06dIFI0eOlNoMHDgQu3btqtV3QURERERE9FDEU06tVgtXV1dx8+ZNg+UVFRXSawBi3rx5IiIiQlhaWgpfX1+xZ88ekZeXJ8LCwkT9+vVFaGioyM/Pl9rExcUJf39/WZ/9+vUTarW62nnZ2dmJBQsWPNSaAgMDxZAhQ6T3WVlZAoA4e/aslHfw4EEBQOTl5QkhhFizZo0wNTUV5eXlUp3U1FShUCjE7du39eafmJgoJkyYIFq3bi0ri4+PFx07dqzVPE+dOiUAiOzs7BrrZmVlCRMTE1FYWCjlnTt3Tpibm4sxY8YYbHP9+nXZey8vL7Fp0yYhhBDh4eGiSZMmBr/3B9t16dJFJCYmVjm3W7duCZ1OJ6Vz584JAEIHCMH07CYiIiIiohrodLp7sYFOV229p3rH+urVq0hLS8PIkSNhZWVlsI5CoZC9nzx5MqKioqDVauHr64sBAwYgJiYGCQkJ2LdvH4QQGDVqlFRfo9Gge/fusj7UajU0Go3B8crLy7Fy5UoUFxcjNDS0zmvav38/tFqtbGfXx8cHjRo1wsKFC3H79m2UlpZi4cKF8PPzg4eHBwAgODgYJiYmWLx4McrLy6HT6bB06VJ0794dZmZmUl+LFy/GyZMnMWHCBIPjp6amIiQkBP/85z/h6OiIoKAgfPvtt3Vex4MyMjLQvHlzWFtbS3lr1qzB7du3ERcXZ7BNgwYNpNc5OTm4cuUKunbtimvXrmHz5s1Vfu/3twOAtm3bIiMjo8q5JSUlwdbWVkr3X25ORERERERUk6c6sM7Pz4cQAj4+PrJ8e3t7qFQqqFQqvftro6Oj0bdvXzRv3hzx8fE4ffo0Bg4cCLVaDT8/P4wePRrp6elS/cuXL8PJyUnWh5OTEwoLC1FaWirlHTp0CCqVCkqlErGxsVi7di1atGhR5zVVBsz334dsbW2N9PR0LFu2DJaWllCpVNi8eTM2bdoEU1NTAECzZs2QlpaGjz76CEqlEg0aNMD58+exevVqqZ+8vDx8+OGHWLZsmdTuQSdPnkRycjKee+45bNmyBSNGjMB7772HlJSUOq/lfmfOnIGLi4ssLy8vDzY2NmjcuHGN7detWwe1Wg1zc3Ppe/f19a3V2C4uLjhz5kyV5QkJCdDpdFI6d+5crfolIiIiIiICnvLAuiqZmZnQarXw9/dHWVmZrCwgIEB6XRkwt2rVSpZ369YtFBYW1mlMHx8faLVa7N27FyNGjMCgQYNw5MiROvVRWlqKFStW6N2HXFpaiqFDh6JDhw74/fffsXv3brRs2RIRERFScH/58mUMHz4cgwYNQlZWFn799VeYm5vj9ddfhxAC5eXlGDBgACZNmoTmzZtXOYeKigq0adMGU6dORVBQEN5++20MHz4c8+bNq9NaDK3NwsJClieE0LuioCrr1q1DZGSk1K4uLC0tq32InVKplB48V5mIiIiIiIhqy/C25VPC29sbCoUCubm5snxPT08A9wKqB91/WXRlUGcor6KiAgDg7OyMgoICWR8FBQWwsbGR9W9ubg5vb28A9y7LzsrKwqxZszB//vxar+eHH35ASUkJoqKiZPkrVqzA6dOnodFoYGJiIuXZ2dlh3bp1eOONN/D111/D1tYW//73v6V2y5Ytg5ubG/bu3QtfX1/s27cP2dnZ0qXuFRUVEELA1NQUaWlp6Nq1Kxo3bqy30+7n54cff/yx1uswxN7eHocOHZLlNW/eHDqdDpcuXap21/rSpUvIzs5GREQEAOC5556DQqHAsWPHajX2tWvX4ODg8PCTJyIiIiIiqsZTvWPdqFEj9OjRA3PmzEFxcfEjGSM0NFQ6vqnS1q1ba7x/uqKiQm+3vCYLFy5EZGSkXhBYUlICExMT2e5u5fvKHwAq69yvXr160lxsbGxw6NAhaLVaKcXGxko77e3atQMAdOjQQe+HiuPHj8Pd3b1Oa3lQUFAQjh07Jtttfv3112Fubi77MeB+N27cAAD88ssvaN++PRo2bAgAaNiwIdRqNb7++muD33tlu0qHDx9GUFCQUfMnIiIiIiKqylMdWAPA3LlzcffuXYSEhGDVqlU4evQocnNzsWzZMhw7dkwKLh9WbGwsTp48ibi4OBw7dgxz587F6tWrMXbsWKlOQkICfvvtN5w+fRqHDh1CQkIC0tPTMXDgwFqPk5+fj99++w3Dhg3TK+vRoweuX7+OkSNH4ujRo8jJyUF0dDRMTU3RpUsXAEBERASysrLw6aefIi8vD//9738RHR0Nd3d3BAUFwcTEBC1btpQlR0dHWFhYoGXLltJDwMaOHYvff/8dU6dORX5+PlasWIFvvvlGdoTVtWvXoNVqpUvdc3NzodVqcfny5SrX16VLF9y8eVN27JWbmxtmzpyJWbNmYejQofj1119x5swZ7N69GzExMZg8eTKAew9Uq7wMvNLXX3+N8vJytG3bFj/++CPy8vJw9OhRfPXVV3o/emRkZKBnz561/i6IiIiIiIjq5BE/nfyxuHjxohg1apRo1qyZMDMzEyqVSrRt21bMmDFDFBcXS/UAiLVr10rvDR0XtXPnTgFAdmTTzp07RWBgoDA3Nxeenp5i8eLFsvGHDBki3N3dhbm5uXBwcBDdunUTaWlpdVpDQkKCcHNzkx2Xdb+0tDTRoUMHYWtrK+zs7ETXrl2FRqOR1fn+++9FUFCQsLKyEg4ODiIyMlIcPXq0yjENHbclhBC//PKLaNmypVAqlcLX11d88803svLFixcLAHppwoQJ1a6xb9++4sMPP9TL37p1q1Cr1cLOzk5YWFgIX19fMW7cOHHx4kVx8+ZNYWFhIR0rdr+LFy+KkSNHSp99kyZNRGRkpNi5c6dUZ8+ePaJBgwaipKSk2rndT3qk/pM+Dorp0SYiIiIiohrU9rgthRB1fBIU0UM6ePAgevTogRMnTkClUtWqzU8//YTExMQ6PwiuUr9+/dC6dWt89NFHtW5TWFgIW1tb6ADwMWbPMP6rj4iIiIhqIMUGOl21Dzl+6i8Fp6dHQEAApk+fjlOnTtW6jUqlwvTp0x9qvNu3b6NVq1ayy/aJiIiIiIj+17hj/YgtX74cMTExBsvc3d1l9xzTXwN3rP8m+K8+IiIiIqpBbXesn+rjtp4GkZGR0hO3H3T/MV9ERERERET0dGJg/YhZW1vD2tr6SU+DiIiIiIiIHhHeY01ERERERERkBAbWREREREREREbgpeBEVdHpgGoeUEBERERERARwx5qIiIiIiIjIKAysiYiIiIiIiIzAwJqIiIiIiIjICAysiYiIiIiIiIzAwJqIiIiIiIjICAysiYiIiIiIiIzA47aIqmJr+6RnQA9LiCc9AyIiIiL6G+GONREREREREZERGFgTERERERERGYGBNREREREREZERGFgTERERERERGYGBNREREREREZERGFgTERERERERGYGBNREREREREZERGFgTERERERERGYGBNT1yV69ehaOjI06fPv1Yx/3zzz/h6OiI8+fPP9ZxiYiIiIjo7+WpDqwvX76M0aNHw9vbGxYWFnByckKHDh2QnJyMkpISo/tPT09HmzZtoFQq4e3tjSVLlsjKk5OTERAQABsbG9jY2CA0NBSbNm2q0xgajQZdu3aFlZUVbGxs0KlTJ5SWlurVKysrQ2BgIBQKBbRarWyOvXv3RuPGjWFlZYXAwEAsX75c1nbJkiVQKBSyZGFhIatTUFCAwYMHw8XFBfXr10evXr2Ql5f30PO935QpU9C7d294eHjI8n/88Ud07doVdnZ2sLS0hI+PD4YMGYLs7Gy9PlJSUtCxY0fpfX5+PoYMGYKmTZtCqVSiSZMm6NatG5YvX467d+8CAOzt7REVFYUJEyZUOz8iIiIiIiJjPLWB9cmTJxEUFIS0tDRMnToV2dnZ0Gg0iIuLw/r167Ft2zaj+j916hQiIiLQpUsXaLVajBkzBsOGDcOWLVukOq6urpg2bRr279+Pffv2oWvXrujduzdycnJqNYZGo0GvXr3Qs2dPZGZmIisrC6NGjYKJif7XEhcXBxcXF738PXv2ICAgAD/++CMOHjyI6OhoREVFYf369bJ6NjY2uHTpkpTOnDkjlQkh8Morr+DkyZNYt24dsrOz4e7uju7du6O4uPih5luppKQECxcuxNChQ2X58fHx6NevHwIDA5Gamorc3FysWLECnp6eSEhI0Otn3bp1iIyMBABkZmaiTZs2OHr0KL7++mscPnwY6enpGDZsGJKTk2Wff3R0NJYvX45r165VOUciIiIiIiKjiKeUWq0Wrq6u4ubNmwbLKyoqpNcAxLx580RERISwtLQUvr6+Ys+ePSIvL0+EhYWJ+vXri9DQUJGfny+1iYuLE/7+/rI++/XrJ9RqdbXzsrOzEwsWLKjVGtq1aycSExNrrLdx40bh6+srcnJyBACRnZ1dbf0XX3xRREdHS+8XL14sbG1tq6yfm5srAIjDhw9LeeXl5cLBwUF8++23dZ7v/dasWSMcHBxkeRqNRgAQs2bNMtjm/u9OCCFKS0uFlZWVOHr0qKioqBB+fn4iODhYlJeX16p9s2bNav2dCCGETqcTAIQOEILp6UxERERERP8DUmyg01Vb76ncsb569SrS0tIwcuRIWFlZGayjUChk7ydPnoyoqChotVr4+vpiwIABiImJQUJCAvbt2wchBEaNGiXV12g06N69u6wPtVoNjUZjcLzy8nKsXLkSxcXFCA0NrXENV65cwd69e+Ho6Ij27dvDyckJYWFh2LVrl6xeQUEBhg8fjqVLl6J+/fo19gsAOp0ODRs2lOXdvHkT7u7ucHNz09tVLysrAwDZ5eEmJiZQKpXSfGo73wdlZGQgODhYlvf9999DpVLhnXfeMdjmwe9u+/btaNKkCXx9faHVanH06FGMGzeuyp3yB9u3bdsWGRkZVc6xrKwMhYWFskRERERERFRbT2VgnZ+fDyEEfHx8ZPn29vZQqVRQqVSIj4+XlUVHR6Nv375o3rw54uPjcfr0aQwcOBBqtRp+fn4YPXo00tPTpfqXL1+Gk5OTrA8nJycUFhbK7ik+dOgQVCoVlEolYmNjsXbtWrRo0aLGNZw8eRIAMHHiRAwfPhybN29GmzZt0K1bN+neZiEEBg8ejNjYWISEhNTqs1m9ejWysrIQHR0t5fn4+GDRokVYt24dli1bhoqKCrRv3156qJevry+aNm2KhIQEXL9+Hbdv38b06dNx/vx5XLp0qdbzNeTMmTN6l7AfP34cnp6eMDU1lfK++OIL6btTqVTQ6XRS2f2XgR8/flxaU6UrV67I2s6dO1c2nouLi+zS9wclJSXB1tZWSm5ublXWJSIiIiIietBTGVhXJTMzE1qtFv7+/tIubKWAgADpdWXA3KpVK1nerVu36rxb6ePjA61Wi71792LEiBEYNGgQjhw5UmO7iooKAEBMTAyio6MRFBSEmTNnSkEwAMyePRtFRUUG7zk2ZOfOnYiOjsa3334Lf39/KT80NBRRUVEIDAxEWFgYfvrpJzg4OGD+/PkAADMzM/z00084fvw4GjZsiPr162Pnzp0IDw+XdoVrM19DSktL9R6UZsiQIUOg1Woxf/58FBcXQwgB4N6PC7/88osUWBvSqFEjaLVaaLVaNGjQALdv35aVW1paVvswu4SEBOh0OimdO3euxvkSERERERFVMq25yl+Pt7c3FAoFcnNzZfmenp4A7gVSDzIzM5NeV14qbCivMoB0dnZGQUGBrI+CggLY2NjI+jc3N4e3tzcAIDg4GFlZWZg1a5YUtFalcePGAKC3u+3n54ezZ88CAHbs2AGNRgOlUimrExISgoEDByIlJUXK+/XXX/Hyyy9j5syZiIqKqnZsMzMzBAUFIT8/X8oLDg6GVquFTqfD7du34eDggHbt2kk75bWZryH29va4fv26LO+5557Drl27cOfOHek7aNCgARo0aKB3NFZmZibu3r2L9u3bS20BIDc3F0FBQQCAevXqSd/B/bvgla5du4b/x969x/V8///jv706vTrpJZ1IRKJySFlvSXMYEWvY2JxyHmNzPtUaxpjYGmYOmanMac3mEGaUQw4fOfdyyEQkc8im9KpQqR6/P/x6fj29KuWFbLtdL5fH5fJ6PR73x+H55O29++vxPNjY2JS5RqVSqXWOiYiIiIiIKuofuWNtZWWFTp06YenSpbKnVr9IPj4+2Lt3r6wuLi7umfdPFxcXa+2Wl6ZevXqwt7fX+nHg0qVLcHR0BAB89913OHPmjLQbu3PnTgDAzz//jLlz50p94uPjERAQgK+++gofffTRM+cuKirCuXPnpGT5SSqVCjY2Nrh8+TJOnjyJHj16VHi9pfH09NTawe/Xrx9yc3O1LtkuTUxMDAICAqCvry+N5+rqim+++Ub6EeRZzp8/LyXhREREREREL9o/cscaAJYvXw5fX194eXlh1qxZcHd3h56eHk6cOIGLFy9qPTCrskaNGoWlS5ciKCgIw4YNw759+7Bx40b89ttvUkxISAi6du2KunXrIicnBxs2bEB8fLzslVxlUSgUmDp1KmbOnInmzZvDw8MDP/74Iy5evIhff/0VAFC3bl1ZH3NzcwBAgwYN4ODgAODx5d/vvPMOxo8fj169eiE9PR3A4530kgeYzZ49G61atYKzszOysrIQFhaGtLQ0DB8+XBr7l19+gY2NDerWrYtz585h/PjxePfdd9G5c+cKr7c0/v7+0r3blpaWAB7/aDF58mRMnjwZaWlp6NmzJ+rUqYPbt28jIiICCoVCugR927ZtmD17tuy8RUVFoVOnTvD19UVISAjc3Nzw6NEjHDx4EH///beUhAOPX/d16tQphIaGPvPPhIiIiIiI6Lm8/AeUvzy3bt0SY8aMEfXr1xeGhobC3NxctGzZUoSFhYn79+9LcQDEli1bpO+pqalar63av3+/ACDu3bsnq/Pw8BBGRkbCyclJREVFyeYfNmyYcHR0FEZGRsLGxkZ07NhRxMbGVuoY5s2bJxwcHKRXfh06dKjM2NLWPXjwYAFAq7Rr106KmTBhgqhbt64wMjISdnZ24u233xanT5+Wjb148WLh4OAgDA0NRd26dcX06dNFfn6+Tust0bJlS7FixQqt+p9//lm0b99eqFQqYWhoKBwcHET//v3F0aNHhRBCpKSkCKVSWeor1ZKTk8XgwYOFg4ODMDAwECqVSrRt21Z8//334tGjR1Lchg0bhIuLyzPX+CS+butfUIiIiIiIXoCKvm5LIcT//5Qoopfkt99+w9SpU3H+/PkyX5FVmoULF2LPnj3SJfDPo1WrVhg3bhz69+9f4T7Z2dlQqVTQALB47pmpSvGfNSIiIiJ6AaTcQKOBhUXZ2cE/9lJw+ucICAjA5cuXcfPmzUq9ysrBwaHCT0Qvzd27d9GzZ0/069fvuccgIiIiIiJ6Fu5YvyTr16/HyJEjS21zdHREUlLSK14RVRR3rP8F+M8aEREREb0A3LGuYt27d4e3t3epbU++5ouIiIiIiIj+2ZhYvyTVqlVDtWrVqnoZRERERERE9JL9I99jTURERERERPS6YGJNREREREREpAMm1kREREREREQ64D3WRGXRaIBynvxHREREREQEcMeaiIiIiIiISCdMrImIiIiIiIh0wMSaiIiIiIiISAdMrImIiIiIiIh0wMSaiIiIiIiISAcVeir42bNnKzygu7v7cy+GiIiIiIiI6J+mQom1h4cHFAoFhBCltpe0KRQKFBUVvdAFElUZlaqqV0DPq4x/q4iIiIiIXoYKJdapqakvex1ERERERERE/0gVSqwdHR1f9jqIiIiIiIiI/pGe6+Fla9euha+vL+zt7ZGWlgYA+PbbbxETE/NCF0dERERERET0uqt0Yh0eHo5Jkybh7bffRlZWlnRPdfXq1fHtt9++6PURERERERERvdYqnVgvWbIEP/zwA6ZNmwZ9fX2p3svLC+fOnXuhiyMiIiIiIiJ63VU6sU5NTYWnp6dWvVKpxP3791/IooiIiIiIiIj+KSqdWNevXx9qtVqrfteuXXBzc3sRayIiIiIiIiL6x6jQU8GfNGnSJIwePRp5eXkQQuD48eP46aefMG/ePKxateplrJGIiIiIiIjotVXpHevhw4fjq6++wvTp0/HgwQP0798f4eHhWLx4Mfr27fsy1kj/Inv37oWbm5v00LuXrVWrVti0adMrmYuIiIiIiP6bnut1W4GBgbh8+TJyc3ORnp6OGzdu4MMPP3zRa6uU9PR0jB8/Hs7OzjA2NoadnR18fX0RHh6OBw8e6Dx+fHw8WrRoAaVSCWdnZ6xevVrWHh4eDnd3d1hYWMDCwgI+Pj74/fffKzVHQkICOnToADMzM1hYWKBt27Z4+PChVlx+fj48PDygUChkl+XPmjULCoVCq5iZmUkxmzdvhpeXF6pXrw4zMzN4eHhg7dq1svGFEPj8889Rq1YtmJiYwM/PD5cvX5bF1KtXT2ue+fPnP/MYg4KCMH36dNmD7wDg4cOHqFGjBqytrZGfn19m//r162PPnj2yOldXVyiVSqSnp2vFT58+HZ9++imKi4ufuTYiIiIiIqLn8VyJNQD89ddfOHXqFJKTk/H333+/yDVV2tWrV+Hp6YnY2FiEhoYiMTERCQkJCAoKwo4dO7QSscpKTU1FQEAA3nrrLajVakyYMAHDhw/H7t27pRgHBwfMnz8fp06dwsmTJ9GhQwf06NEDSUlJFZojISEBXbp0QefOnXH8+HGcOHECY8aMgZ6e9h9RUFAQ7O3tteqnTJmC27dvy0rjxo3xwQcfSDE1atTAtGnTkJCQgLNnz2Lo0KEYOnSo7Fi+/vprfPfdd1ixYgWOHTsGMzMz+Pv7Iy8vTzbf7NmzZXONHTu23GM8fPgwrly5gl69emm1bdq0CU2aNIGrqyu2bt1aav+zZ8/i3r17aNeunWzMhw8f4v3338ePP/6o1adr167Iycmp9I8cREREREREFSYqKTs7WwwYMEDo6+sLhUIhFAqFMDAwEIGBgSIrK6uyw70Q/v7+wsHBQeTm5pbaXlxcLH0GIFasWCECAgKEiYmJcHV1FUeOHBGXL18W7dq1E6ampsLHx0ekpKRIfYKCgkSTJk1kY/bp00f4+/uXuy5LS0uxatWqCh2Dt7e3mD59+jPjdu7cKVxdXUVSUpIAIBITE8uMVavVAoA4ePBguWN6enpKcxcXF4uaNWuKsLAwqT0rK0solUrx008/SXWOjo5i0aJFz1zvk0aPHi3ef//9Utvat28vVqxYIcLDw0WnTp1KjZk9e7bo06ePrG7IkCHi008/Fb///rto1KhRqf2GDh0qBgwYUOF1ajQaAUBoACFY/pmFiIiIiOgFkHIDjabcuOe6x/rYsWP47bffkJWVhaysLOzYsQMnT57EyJEjX3Da/2wZGRmIjY3F6NGjZZc8P0mhUMi+z5kzB4MGDYJarYarqyv69++PkSNHIiQkBCdPnoQQAmPGjJHiExIS4OfnJxvD398fCQkJpc5XVFSE6Oho3L9/Hz4+Ps88hr/++gvHjh2Dra0tWrduDTs7O7Rr1w6HDx+Wxd25cwcjRozA2rVrYWpq+sxxV61ahUaNGqFNmzaltgshsHfvXiQnJ6Nt27YAHu/Op6eny45XpVLB29tb63jnz58PKysreHp6IiwsDIWFheWu59ChQ/Dy8tKqv3LlChISEtC7d2/07t0bhw4dQlpamlbctm3b0KNHD+l7Tk4OfvnlFwwYMACdOnWCRqPBoUOHtPq1bNmy1PoS+fn5yM7OlhUiIiIiIqKKqnRivWPHDkRGRsLf31+6n9jf3x8//PADtm/f/jLWWK6UlBQIIeDi4iKrt7a2hrm5OczNzREcHCxrGzp0KHr37o1GjRohODgY165dQ2BgIPz9/eHm5obx48cjPj5eik9PT4ednZ1sDDs7O2RnZ8vugT537hzMzc2hVCoxatQobNmyBY0bN37mMVy9ehXA43ukR4wYgV27dqFFixbo2LGjdG+zEAJDhgzBqFGjSk1On5aXl4f169eXeu+7RqOBubk5jIyMEBAQgCVLlqBTp07SsZYc39PH++Q9zOPGjUN0dDT279+PkSNHIjQ0FEFBQeWuKS0trdRL2CMjI9G1a1dYWlqiRo0a8Pf3R1RUlCzm5s2bOHv2LLp27SrVRUdHo2HDhmjSpAn09fXRt29fREREaI1vb2+PP//8s8z7rOfNmweVSiWVOnXqlHscRERERERET6p0Ym1lZQWVSqVVr1KpYGlp+UIW9SIcP34carUaTZo00XoYlru7u/S5JIFs1qyZrC4vL6/SO5cuLi5Qq9U4duwYPv74YwwePBgXLlx4Zr+ShG/kyJEYOnQoPD09sWjRIri4uCAyMhIAsGTJEuTk5CAkJKRCa9myZQtycnIwePBgrbZq1apBrVbjxIkTmDt3LiZNmiT7IaEiJk2ahPbt28Pd3R2jRo3CggULsGTJknIfPPbw4UMYGxvL6oqKivDjjz9iwIABUt2AAQOwevVqWSK8bds2vPnmm6hevbpUFxkZqdXvl19+QU5OjmwOExMTFBcXl7m2kJAQaDQaqfz5558VOgdERERERETAcyTW06dPx6RJk2S7l+np6Zg6dSpmzJjxQhdXEc7OzlAoFEhOTpbVOzk5wdnZGSYmJlp9DA0Npc8ll4mXVleS2NWsWRN37tyRjXHnzh1YWFjIxjcyMoKzszPeeOMNzJs3D82bN8fixYufeQy1atUCAK3dbTc3N1y/fh0AsG/fPiQkJECpVMLAwADOzs4AAC8vr1KT51WrVuGdd97R2nkGAD09PTg7O8PDwwOTJ0/G+++/j3nz5knHWnJ8Tx9vSVtpvL29UVhYiGvXrpUZY21tjXv37snqdu/ejZs3b6JPnz4wMDCAgYEB+vbti7S0NOzdu1eK27ZtG7p37y59v3DhAo4ePYqgoCCpX6tWrfDgwQNER0fL5sjMzISZmVmpfxcAQKlUSldflBQiIiIiIqKKMqhIkKenp+w+5cuXL6Nu3bqoW7cuAOD69etQKpX4+++/X/l91lZWVujUqROWLl2KsWPHlnmftS58fHywc+dOWV1cXNwz758ub5f0SfXq1YO9vb3WjwOXLl2SLn3+7rvv8OWXX0ptt27dgr+/P37++Wd4e3vL+qWmpmL//v3Ytm3bM+d+ep3169dHzZo1sXfvXnh4eAAAsrOzpV34sqjVaujp6cHW1rbMGE9PT60d/IiICPTt2xfTpk2T1c+dOxcRERHo1KkTcnNzsX//foSHh8v6tW3bFsuWLZP1i4qKQkREBEaMGCHVnT9/Hp6enuWfBCIiIiIioudUocT63XfffcnL0M3y5cvh6+sLLy8vzJo1C+7u7tDT08OJEydw8eJFvPHGGzqNP2rUKCxduhRBQUEYNmwY9u3bh40bN+K3336TYkJCQtC1a1fUrVsXOTk52LBhA+Lj42WvsSqLQqHA1KlTMXPmTDRv3hweHh748ccfcfHiRfz6668AIP2IUcLc3BwA0KBBAzg4OMjaIiMjUatWLdn9yCXmzZsHLy8vNGjQAPn5+di5cyfWrl0rJa0KhQITJkzAl19+iYYNG6J+/fqYMWMG7O3tpb8HCQkJOHbsGN566y1Uq1YNCQkJmDhxIgYMGFDu7QD+/v6yV2L9/fff2L59O7Zt24amTZvKYgcNGoT33nsPmZmZ2LdvHxo1aoR69eoBAB49eoS1a9di9uzZWv2GDx+OhQsXIikpCU2aNAHw+KFpnTt3LnNdREREREREOnkFTyh/JW7duiXGjBkj6tevLwwNDYW5ublo2bKlCAsLE/fv35fiAIgtW7ZI31NTU7VeW7V//34BQNy7d09W5+HhIYyMjISTk5OIioqSzT9s2DDh6OgojIyMhI2NjejYsaOIjY2t1DHMmzdPODg4SK/8OnToUJmxpa1bCCGKioqEg4OD+Oyzz0rtN23aNOHs7CyMjY2FpaWl8PHxEdHR0bKY4uJiMWPGDGFnZyeUSqXo2LGjSE5OltpPnTolvL29hUqlEsbGxsLNzU2EhoaKvLy8co8vIyNDGBsbi4sXLwohhPjmm29E9erVRUFBgVZsfn6+qF69uli8eLEYMGCAmDZtmtT266+/Cj09PZGenl7qPG5ubmLixIlCCCFu3LghDA0NxZ9//lnu2p7E1239CwoRERER0QtQ0ddtKYQQoioTe/pvmTp1KrKzs/H9999XKL6wsBB2dnb4/fff0bJly0rPFxwcjHv37mHlypUV7pOdnQ2VSgUNAN5t/Q/Ff9aIiIiI6AWQcgONptxnMVX64WVFRUX45ptv0LJlS9SsWRM1atSQFaLyTJs2DY6OjmW++uppmZmZmDhxIv73v/8913y2traYM2fOc/UlIiIiIiKqiEon1l988QUWLlyIPn36QKPRYNKkSejZsyf09PQwa9asl7DEf77169dL79R+upTcB/xfUb16dXz22WfQ06vYXz1bW1tMnz5d9vC8ypg8eXKpT0YnIiIiIiJ6USp9KXiDBg3w3XffISAgQHofcknd0aNHsWHDhpe11n+snJwcrddXlTA0NISjo+MrXhGVh5eC/wvwUnAiIiIiegEqeil4hZ4K/qT09HQ0a9YMwOMnU2s0GgDAO++8UyXvsf4nqFatGqpVq1bVyyAiIiIiIqKXoNKXgjs4OOD27dsAHu9ex8bGAgBOnDgBpVL5YldHRERERERE9JqrdGL93nvvYe/evQCAsWPHYsaMGWjYsCEGDRqEYcOGvfAFEhEREREREb3OdH7d1tGjR3HkyBE0bNgQ3bp1e1HrIqoyvMf6X4D3WBMRERHRC1DRe6xf2Hus//rrL6xatQqfffbZixiOqMpU9H88RERERET07/bS3mNdltu3b/PhZURERERERPSf88ISayIiIiIiIqL/IibWRERERERERDpgYk1ERERERESkA4OKBk6aNKnc9r///lvnxRARERERERH901Q4sU5MTHxmTNu2bXVaDBEREREREdE/TYUT6/3797/MdRC9dlTzVIBxVa+C6NURM/n+byIiIqLnwXusiYiIiIiIiHTAxJqIiIiIiIhIB0ysiYiIiIiIiHTAxJqIiIiIiIhIB0ysiYiIiIiIiHRQ4cT6/v37+Pjjj1G7dm3Y2Nigb9++fHc1ERERERER/edVOLGeMWMG1q5di3feeQeBgYHYt28fPvroo5e5NiIiIiIiIqLXXoXfY71lyxZERUXhgw8+AAAMHDgQrVq1QmFhIQwMKjwMERERERER0b9KhXesb9y4AV9fX+n7G2+8AUNDQ9y6deulLIz+nSIiItC5c+dXMldBQQHq1auHkydPvpL5iIiIiIjov6nCiXVxcTEMDQ1ldQYGBigqKtJpAenp6Rg/fjycnZ1hbGwMOzs7+Pr6Ijw8HA8ePNBpbACIj49HixYtoFQq4ezsjNWrV8vaZ82aBYVCISuurq4VHn/lypVo3749LCwsoFAokJWVpRUzd+5ctG7dGqampqhevXqp4zy9BoVCgejoaNlxlBaTnp4uxeTk5GDChAlwdHSEiYkJWrdujRMnTkjtjx49QnBwMJo1awYzMzPY29tj0KBBsh9HyppHoVDIxtq9ezdatWqFatWqwcbGBr169cK1a9fKPVd5eXmYMWMGZs6cqdV248YNGBkZoWnTpmX2f/jwIczMzJCSkiKrq1GjBqytrZGfny+LNzIywpQpUxAcHFzuuoiIiIiIiHRR4cRaCIGOHTuiRYsWUnnw4AG6desmq6uMq1evwtPTE7GxsQgNDUViYiISEhIQFBSEHTt2YM+ePZU+oCelpqYiICAAb731FtRqNSZMmIDhw4dj9+7dsrgmTZrg9u3bUjl8+HCF53jw4AG6dOmCzz77rMyYgoICfPDBB/j444/LHSsqKkq2jnfffVcrJjk5WRZja2srtQ0fPhxxcXFYu3Ytzp07h86dO8PPzw83b96U1nr69GnMmDEDp0+fxubNm5GcnIzu3btLY7Ru3Vo2/u3btzF8+HDUr18fXl5eAB6f1x49eqBDhw5Qq9XYvXs37t69i549e5Z7fL/++issLCxkVz6UWL16NXr37o3s7GwcO3as1P5xcXFwdHSEs7OzVLdp0yY0adIErq6u2Lp1q1afwMBAHD58GElJSeWujYiIiIiI6HlV+Obo0nYZe/ToodPkn3zyCQwMDHDy5EmYmZlJ9U5OTujRoweEEFKdQqHAihUrsH37duzbtw+Ojo6IjIyEjY0Nhg8fjhMnTqB58+ZYu3YtGjRoAABYsWIF6tevjwULFgAA3NzccPjwYSxatAj+/v7S2AYGBqhZs+ZzHcOECRMAPN7pLcsXX3wBAFq75U+rXr36M9dha2tb6q73w4cPsWnTJsTExKBt27YAHu/Gb9++HeHh4fjyyy+hUqkQFxcn67d06VK0bNkS169fR926dWFkZCRbw6NHjxATE4OxY8dCoVAAAE6dOoWioiJ8+eWX0NN7/NvMlClT0KNHDzx69EjryoYS0dHR6Natm1a9EAJRUVFYvnw5HBwcEBERAW9vb624mJgY2Y8AwONLywcMGAAhBCIiItCnTx9Zu6WlJXx9fREdHY05c+aUuq78/HzZbnd2dnapcURERERERKXRKbHWRUZGhrRT/WRS/aSSRK7EnDlzsHDhQixcuBDBwcHo378/nJycEBISgrp162LYsGEYM2YMfv/9dwBAQkIC/Pz8ZGP4+/tLyXCJy5cvw97eHsbGxvDx8cG8efNQt27dF3ewFTR69GgMHz4cTk5OGDVqFIYOHap1Djw8PJCfn4+mTZti1qxZ0u5vYWEhioqKYGxsLIs3MTEpdwdeo9FAoVCUeYn6tm3bkJGRgaFDh0p1b7zxBvT09BAVFYUhQ4YgNzcXa9euhZ+fX5lJNQAcPnwYAwcO1Krfv38/Hjx4AD8/P9SuXRutW7fGokWLZH8viouLsWPHDtmu9JUrV5CQkIDNmzdDCIGJEyciLS0Njo6OsvFbtmyJQ4cOlbmuefPmST9+EBERERERVVaFLwV/0VJSUiCEgIuLi6ze2toa5ubmMDc317o3dujQoejduzcaNWqE4OBgXLt2DYGBgfD394ebmxvGjx8v2zlOT0+HnZ2dbAw7OztkZ2fj4cOHAABvb2+sXr0au3btQnh4OFJTU9GmTRvk5OS8nAMvw+zZs7Fx40bExcWhV69e+OSTT7BkyRKpvVatWlixYgU2bdqETZs2oU6dOmjfvj1Onz4NAKhWrRp8fHwwZ84c3Lp1C0VFRVi3bh0SEhJw+/btUufMy8tDcHAw+vXrBwsLi1JjIiIi4O/vDwcHB6mufv36iI2NxWeffQalUonq1avjxo0b2LhxY5nHl5WVBY1GA3t7+1Ln6Nu3L/T19dG0aVM4OTnhl19+kcUcPXoUAGQ72ZGRkejatSssLS1Ro0YN+Pv7IyoqSmt8e3t7pKWllbm2kJAQaDQaqfz5559lxhIRERERET2twjvWb731ltbu6dMUCgX27t2r04KOHz+O4uJiBAYGaj2Myt3dXfpckjA3a9ZMVpeXl4fs7OwyE8Wnde3aVTa+t7c3HB0dsXHjRnz44Ye6HEqlzJgxQ/rs6emJ+/fvIywsDOPGjQMAuLi4yH6EaN26Na5cuYJFixZh7dq1AIC1a9di2LBhqF27NvT19dGiRQv069cPp06d0prv0aNH6N27N4QQCA8PL3VNN27cwO7du7US5vT0dIwYMQKDBw9Gv379kJOTg88//xzvv/8+4uLiSv17UvJDxtM76llZWdi8ebNsV33AgAGIiIjAkCFDpLqYmBi888470qXnRUVF+PHHH7F48WJZvylTpuDzzz+X4oDHu/blPQhPqVRCqVSW2U5ERERERFSeCifWHh4eZbbl5ORgw4YNWolweZydnaFQKJCcnCyrd3JyAvA4GXrak5cZlyRvpdUVFxcDAGrWrIk7d+7Ixrhz5w4sLCxKHR94fJ9zo0aNZE+ergre3t6YM2cO8vPzy0z6WrZsKUtIGzRogAMHDuD+/fvIzs5GrVq10KdPH+mclihJqtPS0rBv374yf4SIioqClZWV1n3Ny5Ytg0qlwtdffy3VrVu3DnXq1MGxY8fQqlUrrbGsrKygUChw7949Wf2GDRuQl5cn24kWQqC4uBiXLl1Co0aNADy+JH3+/PlSzO7du3Hz5k2te6qLioqwd+9edOrUSarLzMyEjY1NqcdIRERERESkqwon1osWLdKqKywsxLJlyzB37lzUrl27zIdDlcbKygqdOnXC0qVLMXbs2DLvs9aFj48Pdu7cKauLi4uDj49PmX1yc3Nx5cqVUu8FfpXUajUsLS3L3UlVq9WoVauWVr2ZmRnMzMxw79497N69W5YAlyTVly9fxv79+2FlZVXq2CUPFBs0aJDWfdMPHjyQ7QgDgL6+PoD/96PG04yMjNC4cWNcuHBB9h7riIgITJ48WbY7DTx+sF1kZCTmz5+Py5cvIy0tTZYsl1w+Pm3aNFm/uXPnIiIiQhZ7/vx5eHp6lrouIiIiIiIiXVU4sX7a+vXr8fnnn+Phw4eYNWsWPvroIxgYVG645cuXw9fXF15eXpg1axbc3d2hp6eHEydO4OLFi3jjjTeed3kAgFGjRmHp0qUICgrCsGHDsG/fPmzcuBG//fabFDNlyhR069YNjo6OuHXrFmbOnAl9fX3069evQnOkp6cjPT1d2uE+d+4cqlWrhrp166JGjRoAgOvXryMzMxPXr19HUVER1Go1gMe79ubm5ti+fTvu3LmDVq1awdjYGHFxcQgNDcWUKVOkeb799lvUr18fTZo0QV5eHlatWoV9+/YhNjZWitm9e7d033pKSgqmTp0KV1dX6cFjjx49wvvvv4/Tp09jx44dKCoqkt6DXaNGDRgZGUlj7du3D6mpqRg+fLjWMQcEBGDRokWYPXu2dCn4Z599BkdHx3ITWH9/fxw+fFh6eJxarcbp06exfv16rXeH9+vXD7Nnz8aXX36JmJgY+Pn5wdTUFADw999/Y/v27di2bZvWe68HDRqE9957D5mZmdL5P3ToUKV+9CEiIiIiIqqMSifWu3btwqefforU1FRMmTIFkyZNeu7d5gYNGiAxMRGhoaEICQnBjRs3oFQq0bhxY0yZMgWffPLJc41bon79+vjtt98wceJELF68GA4ODli1apXsVVs3btxAv379kJGRARsbG7z55ps4evRohS8dXrFiheyJ0iWvuip5YjYAfP755/jxxx+lmJLkc//+/Wjfvj0MDQ2xbNkyTJw4EUIIODs7Y+HChRgxYoTUp6CgAJMnT8bNmzdhamoKd3d37NmzB2+99ZYUo9FopPNYo0YN9OrVC3PnzpV2nG/evIlt27YB0L60v2QtJSIiItC6dWuthBcAOnTogA0bNuDrr7/G119/DVNTU/j4+GDXrl1lXmIPAB9++CG8vLyg0WigUqkQERGBxo0blzrHe++9hzFjxmDnzp2IiYnB4MGDpbY1a9bAzMwMHTt21OrXsWNHmJiYYN26dRg3bhwSEhKg0Wjw/vvvl7kuIiIiIiIiXSjEky+LLsfx48cRHByMo0ePYtSoUZg2bRqsra1f9vroX+aDDz5AixYtEBISUqH4u3fvolatWrhx44bWE94rok+fPmjevDk+++yzCvfJzs6GSqUCPgVg/Mxwon8NMbNC/3dARERE9J9RkhtoNJpyH5Bd4R3rVq1awcTEBKNGjUL9+vWxYcOGUuNKnmJNVJqwsDBs3769wvGZmZlYuHDhcyXVBQUFaNasGSZOnFjpvkRERERERBVV4R3revXqVeh1W1evXn0hC3sdrF+/HiNHjiy1zdHREUlJSa94RfQqcMea/qu4Y01EREQk98J3rK9du/Yi1vWP0r17d9lroJ709JOyiYiIiIiI6L/puZ8K/l9QrVo1VKtWraqXQURERERERK8xvWeHPJaQkIAdO3bI6tasWYP69evD1tYWH330EfLz81/4AomIiIiIiIheZxVOrGfPni27p/jcuXP48MMP4efnh08//RTbt2/HvHnzXsoiiYiIiIiIiF5XFb4UXK1WY86cOdL36OhoeHt744cffgAA1KlTBzNnzsSsWbNe+CKJqoImpPwHFBAREREREQGV2LG+d++e7JVHBw4cQNeuXaXv//vf//Dnn3++2NURERERERERveYqnFjb2dkhNTUVwOP3A58+fRqtWrWS2nNycvikbCIiIiIiIvrPqXBi/fbbb+PTTz/FoUOHEBISAlNTU7Rp00ZqP3v2LBo0aPBSFklERERERET0uqrwPdZz5sxBz5490a5dO5ibm+PHH3+EkZGR1B4ZGYnOnTu/lEUSERERERERva4UQghRmQ4ajQbm5ubQ19eX1WdmZsLc3FyWbBP9E2VnZ0OlUkGj4cPLiIiIiIj+yyqaG1R4x7qESqUqtb5GjRqVHYqIiIiIiIjoH6/SiTXRf4VqngowrupVEL04YmalLlAiIiIiogqq8MPLiIiIiIiIiEgbE2siIiIiIiIiHTCxJiIiIiIiItIBE2siIiIiIiIiHTCxJiIiIiIiItIBE2siIiIiIiIiHTCxJiIiIiIiItIBE2siIiIiIiIiHTCxplcqIiICnTt3fiVzFRQUoF69ejh58uQrmY+IiIiIiP6b/hGJdXp6OsaPHw9nZ2cYGxvDzs4Ovr6+CA8Px4MHD3QePz4+Hi1atIBSqYSzszNWr14ta581axYUCoWsuLq6VmqOhIQEdOjQAWZmZrCwsEDbtm3x8OFDqb179+6oW7cujI2NUatWLQwcOBC3bt2SjXH27Fm0adMGxsbGqFOnDr7++mtZ++bNm+Hl5YXq1avDzMwMHh4eWLt2rSwmNzcXY8aMgYODA0xMTNC4cWOsWLFCFpOXl4fRo0fDysoK5ubm6NWrF+7cuSOLefp8KBQKREdHl3sO8vLyMGPGDMycOVOr7caNGzAyMkLTpk3L7P/w4UOYmZkhJSVFVlejRg1YW1sjPz9fFm9kZIQpU6YgODi43HURERERERHp4rVPrK9evQpPT0/ExsYiNDQUiYmJSEhIQFBQEHbs2IE9e/boNH5qaioCAgLw1ltvQa1WY8KECRg+fDh2794ti2vSpAlu374tlcOHD1d4joSEBHTp0gWdO3fG8ePHceLECYwZMwZ6ev/v9L/11lvYuHEjkpOTsWnTJly5cgXvv/++1J6dnY3OnTvD0dERp06dQlhYGGbNmoWVK1dKMTVq1MC0adOQkJCAs2fPYujQoRg6dKjsWCZNmoRdu3Zh3bp1+OOPPzBhwgSMGTMG27Ztk2ImTpyI7du345dffsGBAwdw69Yt9OzZU+u4oqKiZOfk3XffLfc8/Prrr7CwsICvr69W2+rVq9G7d29kZ2fj2LFjpfaPi4uDo6MjnJ2dpbpNmzahSZMmcHV1xdatW7X6BAYG4vDhw0hKSip3bURERERERM9LIYQQVb2I8nTp0gVJSUm4ePEizMzMtNqFEFAoFAAe76KuWLEC27dvx759++Do6IjIyEjY2Nhg+PDhOHHiBJo3b461a9eiQYMGAIDg4GD89ttvOH/+vDRm3759kZWVhV27dgF4vGO9detWqNXq5zqGVq1aoVOnTpgzZ06F+2zbtg3vvvsu8vPzYWhoiPDwcEybNg3p6ekwMjICAHz66afYunUrLl68WOY4LVq0QEBAgDR306ZN0adPH8yYMUOKeeONN9C1a1d8+eWX0Gg0sLGxwYYNG6TE/uLFi3Bzc0NCQgJatWoF4PG53rJlyzOT6Se98847cHNzQ1hYmKxeCAFnZ2csX74c+/fvR2ZmpuwHgxIffvghbGxsMH/+fKnurbfeQt++fSGEwObNmxEbG6vVr0OHDvD19a3w+c/OzoZKpQI+BWBc4cMjeu2Jma/1P/dEREREr52S3ECj0cDCwqLMuNd6xzojIwOxsbEYPXp0qUk1ACmpLjFnzhwMGjQIarUarq6u6N+/P0aOHImQkBCcPHkSQgiMGTNGik9ISICfn59sDH9/fyQkJMjqLl++DHt7ezg5OSEwMBDXr1+v0DH89ddfOHbsGGxtbdG6dWvY2dmhXbt25e54Z2ZmYv369WjdujUMDQ2ldbZt21ZKqkvWmZycjHv37mmNIYTA3r17kZycjLZt20r1rVu3xrZt23Dz5k0IIbB//35cunRJuu/51KlTePTokeycuLq6om7dulrnZPTo0bC2tkbLli0RGRmJZ/1Gc/jwYXh5eWnV79+/Hw8ePICfnx8GDBiA6Oho3L9/XxZTXFyMHTt2oEePHlLdlStXkJCQgN69e6N37944dOgQ0tLStMZv2bIlDh06VOa68vPzkZ2dLStEREREREQV9Von1ikpKRBCwMXFRVZvbW0Nc3NzmJuba90/O3ToUPTu3RuNGjVCcHAwrl27hsDAQPj7+8PNzQ3jx49HfHy8FJ+eng47OzvZGHZ2dsjOzpbugfb29sbq1auxa9cuhIeHIzU1FW3atEFOTs4zj+Hq1asAHu96jxgxArt27UKLFi3QsWNHXL58WRYbHBwMMzMzWFlZ4fr164iJiXnmOkvaSmg0Gpibm8PIyAgBAQFYsmQJOnXqJLUvWbIEjRs3hoODA4yMjNClSxcsW7ZMSr5LdsSrV6+uNdeT88yePRsbN25EXFwcevXqhU8++QRLliwp8zxkZWVBo9HA3t5eqy0iIgJ9+/aFvr4+mjZtCicnJ/zyyy+ymKNHjwJ4/GdRIjIyEl27doWlpSVq1KgBf39/REVFaY1vb29fasJdYt68eVCpVFKpU6dOmbFERERERERPe60T67IcP34carUaTZo00Xpglbu7u/S5JPFs1qyZrC4vL69Su5Jdu3bFBx98AHd3d/j7+2Pnzp3IysrCxo0bn9m3uLgYADBy5EgMHToUnp6eWLRoEVxcXBAZGSmLnTp1KhITExEbGwt9fX0MGjTombvAT6tWrRrUajVOnDiBuXPnYtKkSbIfEpYsWYKjR49i27ZtOHXqFBYsWIDRo0dX+l71GTNmwNfXF56enggODkZQUJDWJd5PKvmRwthYfm11VlYWNm/ejAEDBkh1AwYMQEREhCwuJiYG77zzjnRfelFREX788UetfqtXr5bOeQkTE5NyH3IXEhICjUYjlT///PMZR09ERERERPT/GFT1Asrj7OwMhUKB5ORkWb2TkxOAxwnT00ounQb+32XipdWVJF81a9bUeuL1nTt3YGFhUer4AFC9enU0atRI9nTqstSqVQsA0LhxY1m9m5ub1uXk1tbWsLa2RqNGjeDm5oY6derg6NGj8PHxKXOdJcdQQk9PT3q4l4eHB/744w/MmzcP7du3x8OHD/HZZ59hy5YtCAgIAPD4hwi1Wo1vvvkGfn5+qFmzJgoKCpCVlSXbtb5z545snqd5e3tjzpw5yM/Ph1Kp1Gq3srKCQqHQumx9w4YNyMvLk+1ECyFQXFyMS5cuoVGjRgAe33P+5L3Vu3fvxs2bN9GnTx/ZeEVFRdi7d69slz4zMxM2NjZlrl2pVJa6ZiIiIiIioop4rXesrays0KlTJyxdulTrntsXxcfHB3v37pXVxcXFwcfHp8w+ubm5uHLlipQ0l6devXqwt7fX+nHg0qVLcHR0LLNfSeJfsiPv4+ODgwcP4tGjR7J1uri4wNLSstxxSsZ49OgRHj16JHsaOQDo6+tL873xxhswNDSUnZPk5GRcv3693HOiVqthaWlZZoJqZGSExo0b48KFC7L6iIgITJ48GWq1WipnzpxBmzZtpB39y5cvIy0tTZYsl1w+/mQ/tVqNvn37au12nz9/Hp6enmWunYiIiIiISBev9Y41ACxfvhy+vr7w8vLCrFmz4O7uDj09PZw4cQIXL17EG2+8odP4o0aNwtKlSxEUFIRhw4Zh37592LhxI3777TcpZsqUKejWrRscHR1x69YtzJw5E/r6+ujXr98zx1coFJg6dSpmzpyJ5s2bw8PDAz/++CMuXryIX3/9FQBw7NgxnDhxAm+++SYsLS1x5coVzJgxAw0aNJCS2f79++OLL77Ahx9+iODgYJw/fx6LFy/GokWLpLnmzZsHLy8vNGjQAPn5+di5cyfWrl2L8PBwAICFhQXatWuHqVOnwsTEBI6Ojjhw4ADWrFmDhQsXAgBUKhU+/PBDTJo0CTVq1ICFhQXGjh0LHx8f6Yng27dvx507d9CqVSsYGxsjLi4OoaGhmDJlSrnnwt/fH4cPH8aECRMAPE7GT58+jfXr12u9F7xfv36YPXs2vvzyS8TExMDPzw+mpqYAgL///hvbt2/Htm3btN57PWjQILz33nvIzMxEjRo1AACHDh2q1BPZiYiIiIiIKuO1T6wbNGiAxMREhIaGIiQkBDdu3IBSqUTjxo0xZcoUfPLJJzqNX79+ffz222+YOHEiFi9eDAcHB6xatQr+/v5SzI0bN9CvXz9kZGTAxsYGb775Jo4ePVru5cVPmjBhAvLy8jBx4kRkZmaiefPmiIuLk175ZWpqis2bN2PmzJm4f/8+atWqhS5dumD69OnSDrBKpZKekP7GG2/A2toan3/+OT766CNpnvv37+OTTz7BjRs3YGJiAldXV6xbt052uXR0dDRCQkIQGBiIzMxMODo6Yu7cuRg1apQUs2jRIujp6aFXr17Iz8+Hv78/li9fLrUbGhpi2bJlmDhxovSqrIULF2LEiBHlnocPP/wQXl5e0Gg0UKlUiIiIQOPGjbWSagB47733MGbMGOzcuRMxMTEYPHiw1LZmzRqYmZmhY8eOWv06duwIExMTrFu3DuPGjUNCQgI0Go3sneBEREREREQv0mv/Hmv6d/nggw/QokULhISEVCj+7t27qFWrFm7cuKH1VPSK6NOnD5o3b47PPvuswn34Hmv6t+J7rImIiIgq51/xHmv69wkLC4O5uXmF4zMzM7Fw4cLnSqoLCgrQrFkzTJw4sdJ9iYiIiIiIKoo71jpav349Ro4cWWqbo6MjkpKSXvGKSFfcsaZ/K+5YExEREVVORXesX/t7rF933bt3l70q6klPvuaLiIiIiIiI/p2YWOuoWrVqqFatWlUvg4iIiIiIiKoI77EmIiIiIiIi0gETayIiIiIiIiIdMLEmIiIiIiIi0gHvsSYqgyak/Cf/ERERERERAdyxJiIiIiIiItIJE2siIiIiIiIiHTCxJiIiIiIiItIBE2siIiIiIiIiHTCxJiIiIiIiItIBE2siIiIiIiIiHfB1W0Rl2agCTKt6EUREL1l/UdUrICIi+sfjjjURERERERGRDphYExEREREREemAiTURERERERGRDphYExEREREREemAiTURERERERGRDphYExEREREREemAiTURERERERGRDphYExEREREREemAiTW9UgMHDkRoaOgrm69v375YsGDBK5uPiIiIiIj+e6o8sU5PT8f48ePh7OwMY2Nj2NnZwdfXF+Hh4Xjw4IHO48fHx6NFixZQKpVwdnbG6tWrZe316tWDQqHQKqNHj67UPEIIdO3aFQqFAlu3bpW1lTZ+dHS0LCY/Px/Tpk2Do6MjlEol6tWrh8jISKn9hx9+QJs2bWBpaQlLS0v4+fnh+PHjZa5n1KhRUCgU+Pbbb595vPPnz9c6lm+++QaNGjWCUqlE7dq1MXfuXKl9yJAhpR5TkyZNyj1HZ86cwc6dOzFu3DhZfUpKCoYOHQoHBwcolUrUr18f/fr1w8mTJ2VxDx8+hJmZGVJSUgAABQUF+Prrr9G8eXOYmprC2toavr6+iIqKwqNHjwAA06dPx9y5c6HRaMpdGxERERER0fMyqMrJr169Cl9fX1SvXh2hoaFo1qwZlEolzp07h5UrV6J27dro3r37c4+fmpqKgIAAjBo1CuvXr8fevXsxfPhw1KpVC/7+/gCAEydOoKioSOpz/vx5dOrUCR988EGl5vr222+hUCjKbI+KikKXLl2k79WrV5e19+7dG3fu3EFERAScnZ1x+/ZtFBcXS+3x8fHo168fWrduDWNjY3z11Vfo3LkzkpKSULt2bdlYW7ZswdGjR2Fvb1/qWmbPno0RI0ZI36tVqyZrHz9+PGJjY/HNN9+gWbNmyMzMRGZmptS+ePFiWTJeWFiI5s2bP/OcLVmyBB988AHMzc2lupMnT6Jjx45o2rQpvv/+e7i6uiInJwcxMTGYPHkyDhw4IMXGxcXB0dERzs7OKCgogL+/P86cOYM5c+bA19cXFhYWOHr0KL755ht4enrCw8MDTZs2RYMGDbBu3bpK/1hCRERERERUEVWaWH/yyScwMDDAyZMnYWZmJtU7OTmhR48eEEJIdQqFAitWrMD27duxb98+ODo6IjIyEjY2Nhg+fDhOnDiB5s2bY+3atWjQoAEAYMWKFahfv750KbCbmxsOHz6MRYsWSYm1jY2NbE3z589HgwYN0K5duwofh1qtxoIFC3Dy5EnUqlWr1Jjq1aujZs2apbbt2rULBw4cwNWrV1GjRg0Aj3eWn7R+/XrZ91WrVmHTpk3Yu3cvBg0aJNXfvHkTY8eOxe7duxEQEFDqfNWqVStzLX/88QfCw8Nx/vx5uLi4AADq168vi1GpVFCpVNL3rVu34t69exg6dGipYwJAUVERfv31V9lxCCEwZMgQNGzYEIcOHYKe3v+7gMLDwwPjx4+XjRETEyP90PLtt9/i4MGDOHnyJDw9PaUYJycnfPDBBygoKJDqunXrhujoaCbWRERERET0UlTZpeAZGRmIjY3F6NGjZUn1k57eAZ4zZw4GDRoEtVoNV1dX9O/fHyNHjkRISAhOnjwJIQTGjBkjxSckJMDPz082hr+/PxISEkqdr6CgAOvWrcOwYcPK3X1+0oMHD9C/f38sW7aszGQVAEaPHg1ra2u0bNkSkZGRsh8Ntm3bBi8vL3z99deoXbs2GjVqhClTpuDhw4flzvvo0SMpEQeA4uJiDBw4EFOnTi33suz58+fDysoKnp6eCAsLQ2FhodS2fft2ODk5YceOHahfvz7q1auH4cOHy3asnxYREQE/Pz84OjqWGXP27FloNBp4eXlJdWq1GklJSZg8ebIsqS7x5K5+cXExduzYgR49egB4/EODn5+fLKkuYWhoKPs71bJlSxw/fhz5+fmlri0/Px/Z2dmyQkREREREVFFVtmOdkpICIYS0K1rC2toaeXl5AB4no1999ZXUNnToUPTu3RsAEBwcDB8fH8yYMUPafR4/frxs1zQ9PR12dnay8e3s7JCdnY2HDx/CxMRE1rZ161ZkZWVhyJAhFT6OiRMnonXr1lLCV5rZs2ejQ4cOMDU1RWxsLD755BPk5uZK9xpfvXoVhw8fhrGxMbZs2YK7d+/ik08+QUZGBqKiokodMzg4GPb29rIfDr766isYGBho3cP8pHHjxqFFixaoUaMGjhw5gpCQENy+fRsLFy6U1pKWloZffvkFa9asQVFRESZOnIj3338f+/bt0xrv1q1b+P3337Fhw4Zyz1NaWhr09fVha2sr1V2+fBkA4OrqWm5fADh69CgAwNvbW+rbvn37Z/YDAHt7exQUFCA9Pb3U5H/evHn44osvKjQWERERERHR06r0UvDSHD9+HMXFxQgMDNTaYXR3d5c+lyTMzZo1k9Xl5eUhOzsbFhYWlZ47IiICXbt2LfPe5Kdt27YN+/btQ2JiYrlxM2bMkD57enri/v37CAsLkxLg4uJiKBQKrF+/XrrEeuHChXj//fexfPlyrR8A5s+fj+joaMTHx8PY2BgAcOrUKSxevBinT58ud7d90qRJ0md3d3cYGRlh5MiRmDdvHpRKJYqLi5Gfn481a9agUaNG0nl54403kJycrPVDyI8//ojq1avj3XffLfccPHz4EEqlUra2J3ftnyUmJgbvvPOOtLNdmb4l56+sh+GFhITIzkt2djbq1KlT4fGJiIiIiOi/rcouBXd2doZCoUBycrKs3snJCc7OzlrJJPD4Et8SJQlaaXUlD/2qWbMm7ty5Ixvjzp07sLCw0Bo/LS0Ne/bswfDhwyt8DPv27cOVK1dQvXp1GBgYwMDg8e8UvXr1Knc31dvbGzdu3JB+OKhVqxZq164tu2/Zzc0NQgjcuHFD1vebb77B/PnzERsbK/uh4dChQ/jrr79Qt25daS1paWmYPHmy1v3aT6+lsLAQ165dk9ZiYGAgJdUlawGA69evy/oKIRAZGYmBAwfCyMio7BOFx1ciPHjwQHbvc8kcFy9eLLcv8PhHjCcfZNeoUaMK9QMgXcb+9P30JZRKJSwsLGSFiIiIiIiooqossbayskKnTp2wdOlS3L9//6XM4ePjg71798rq4uLi4OPjoxUbFRUFW1vbMh/4VZpPP/0UZ8+ehVqtlgoALFq0qMxLuIHH9xZbWlpCqVQCAHx9fXHr1i3k5uZKMZcuXYKenh4cHBykuq+//hpz5szBrl27ZPcqA4/fD/30Wuzt7TF16lTs3r273LXo6elJl2j7+vqisLAQV65cka0FgNZl1AcOHEBKSgo+/PDD8k4TgMcPIwOACxcuyOoaN26MBQsWyJ6AXiIrKwvA48u+09LS0KlTJ6mtf//+2LNnT6lXCzx69Ej2d+r8+fNwcHCAtbX1M9dJRERERERUWVX6Huvly5ejsLAQXl5e+Pnnn/HHH38gOTkZ69atw8WLF6Gvr6/T+KNGjcLVq1cRFBSEixcvYvny5di4cSMmTpwoiysuLkZUVBQGDx4s7TpXRM2aNdG0aVNZAYC6detKT9Levn07Vq1ahfPnzyMlJQXh4eEIDQ3F2LFjpXH69+8PKysrDB06FBcuXMDBgwcxdepUDBs2TNpZ/+qrrzBjxgxERkaiXr16SE9PR3p6upSMW1lZaa3F0NAQNWvWlC7fTkhIwLfffoszZ87g6tWrWL9+PSZOnIgBAwbA0tISAODn54cWLVpg2LBhSExMxKlTpzBy5Eh06tRJtosNPL5E3NvbWzru8tjY2KBFixY4fPiwVKdQKBAVFYVLly6hTZs22LlzJ65evYqzZ89i7ty50n3rMTEx8PPzg6mpqdR3woQJ8PX1RceOHbFs2TLpmDZu3IhWrVpJ928Dj3fzO3fuXME/VSIiIiIiosqp0sS6QYMGSExMhJ+fH0JCQtC8eXN4eXlhyZIlmDJlCubMmaPT+PXr18dvv/2GuLg4NG/eHAsWLMCqVaukh52V2LNnD65fv45hw4bpNF9pDA0NsWzZMvj4+MDDwwPff/89Fi5ciJkzZ0ox5ubmiIuLQ1ZWFry8vBAYGIhu3brhu+++k2LCw8NRUFCA999/H7Vq1ZLKN998U+G1KJVKREdHo127dmjSpAnmzp2LiRMnYuXKlVKMnp4etm/fDmtra7Rt2xYBAQFwc3NDdHS0bCyNRoNNmzZVaLe6xPDhw7VeG9ayZUucPHkSzs7OGDFiBNzc3NC9e3ckJSXh22+/BSB/zdaTxxIXF4egoCB8//33aNWqFf73v//hu+++w7hx46RkPy8vD1u3bpW9t5uIiIiIiOhFUojKPAWKSAcPHz6Ei4sLfv7551Ivxy/N3bt3UatWLdy4cUPrCe8VER4eji1btiA2NrbCfbKzs6FSqaD5AbAwfXY8EdE/Wn/+ZwAREVFZpNxAoyn3WUxVumNN/y0mJiZYs2YN7t69W+E+mZmZWLhw4XMl1cDjKwaWLFnyXH2JiIiIiIgqgjvW5Vi/fj1GjhxZapujoyOSkpJe8YroVeCONRH9p3DHmoiIqEwV3bF+7d5j/Trp3r07vL29S2178jVfRERERERE9N/FxLoc1apVQ7Vq1ap6GURERERERPQa4z3WRERERERERDpgYk1ERERERESkAybWRERERERERDrgPdZEZemtAcp58h8RERERERHAHWsiIiIiIiIinTCxJiIiIiIiItIBE2siIiIiIiIiHTCxJiIiIiIiItIBE2siIiIiIiIiHTCxJiIiIiIiItIBX7dFVAaVSlXVSyAiIiKiV0QIUdVLoH8w7lgTERERERER6YCJNREREREREZEOmFgTERERERER6YCJNREREREREZEOmFgTERERERER6YCJNREREREREZEOmFgTERERERER6YCJNREREREREZEOmFjTKzVw4ECEhoa+krkuXLgABwcH3L9//5XMR0RERERE/01Vnlinp6dj/PjxcHZ2hrGxMezs7ODr64vw8HA8ePBA5/Hj4+PRokULKJVKODs7Y/Xq1bL2gwcPolu3brC3t4dCocDWrVsrNf6QIUOgUChkpUuXLrKYevXqacXMnz9fFnP27Fm0adMGxsbGqFOnDr7++mtZ++rVq7XGMDY2lsXMmjULrq6uMDMzg6WlJfz8/HDs2DFZTGZmJgIDA2FhYYHq1avjww8/RG5urtSel5eHIUOGoFmzZjAwMMC7775b7vH/3//9HwwMDODh4fHMc3XmzBns3LkT48aN02r76aefoK+vj9GjR5fZ/8CBA6hTp46sLiEhAfr6+ggICNCKb9y4MVq1aoWFCxc+c21ERERERETPq0oT66tXr8LT0xOxsbEIDQ1FYmIiEhISEBQUhB07dmDPnj06jZ+amoqAgAC89dZbUKvVmDBhAoYPH47du3dLMffv30fz5s2xbNmy556nS5cuuH37tlR++uknrZjZs2fLYsaOHSu1ZWdno3PnznB0dMSpU6cQFhaGWbNmYeXKlbIxLCwsZGOkpaXJ2hs1aoSlS5fi3LlzOHz4MOrVq4fOnTvj77//lmICAwORlJSEuLg47NixAwcPHsRHH30ktRcVFcHExATjxo2Dn59fucedlZWFQYMGoWPHjhU6T0uWLMEHH3wAc3NzrbaIiAgEBQXhp59+Ql5eXqn9Y2Ji0K1bN61+Y8eOxcGDB3Hr1i2tPkOHDkV4eDgKCwsrtEYiIiIiIqJKE1XI399fODg4iNzc3FLbi4uLpc8AxIoVK0RAQIAwMTERrq6u4siRI+Ly5cuiXbt2wtTUVPj4+IiUlBSpT1BQkGjSpIlszD59+gh/f/9S5wMgtmzZUqljGDx4sOjRo0e5MY6OjmLRokVlti9fvlxYWlqK/Px8qS44OFi4uLhI36OiooRKparU2jQajQAg9uzZI4QQ4sKFCwKAOHHihBTz+++/C4VCIW7evKnV/1nH1qdPHzF9+nQxc+ZM0bx583LXUlhYKFQqldixY4dW29WrV4WJiYnIysoS3t7eYv369aWO0aBBA/H7779L33NycoS5ubm4ePGi6NOnj5g7d65Wn/z8fKFUKqVzUJq8vDyh0Wik8ueffwoALCwsLCwsLCws/6FCVJqSnEqj0ZQbV2U71hkZGYiNjcXo0aNhZmZWaoxCoZB9nzNnDgYNGgS1Wg1XV1f0798fI0eOREhICE6ePAkhBMaMGSPFJyQkaO26+vv7IyEh4YUeS3x8PGxtbeHi4oKPP/4YGRkZWjHz58+HlZUVPD09ERYWJttBTUhIQNu2bWFkZCRbZ3JyMu7duyfV5ebmwtHREXXq1EGPHj2QlJRU5poKCgqwcuVKqFQqNG/eXJqnevXq8PLykuL8/Pygp6endcn4s0RFReHq1auYOXNmheLPnj0LjUYjm/vJsQICAqBSqTBgwABERERoxSQlJeGvv/5Chw4dpLqNGzfC1dUVLi4uGDBgACIjIyGEkPUzMjKCh4cHDh06VOba5s2bB5VKJZWnLzcnIiIiIiIqT5Ul1ikpKRBCwMXFRVZvbW0Nc3NzmJubIzg4WNY2dOhQ9O7dG40aNUJwcDCuXbuGwMBA+Pv7w83NDePHj0d8fLwUn56eDjs7O9kYdnZ2yM7OxsOHD1/IcXTp0gVr1qzB3r178dVXX+HAgQPo2rUrioqKpJhx48YhOjoa+/fvx8iRIxEaGoqgoKBnrrOkDQBcXFwQGRmJmJgYrFu3DsXFxWjdujVu3Lgh67djxw6Ym5vD2NgYixYtQlxcHKytraWxbG1tZfEGBgaoUaOGNE9FXL58GZ9++inWrVsHAwODCvVJS0uDvr6+1vzFxcVYvXo1BgwYAADo27cvDh8+jNTUVFlcTEwM/P39ZT8+RERESP26dOkCjUaDAwcOaM1tb2+vddn8k0JCQqDRaKTy559/VuiYiIiIiIiIAKBiWdErdPz4cRQXFyMwMBD5+fmyNnd3d+lzSeLZrFkzWV1eXh6ys7NhYWHxStbbt29f6XOzZs3g7u6OBg0aID4+Xrr3eNKkSVKMu7s7jIyMMHLkSMybNw9KpbJC8/j4+MDHx0f63rp1a7i5ueH777/HnDlzpPqS+8nv3r2LH374Ab1798axY8e0EtrnVVRUhP79++OLL75Ao0aNKtzv4cOHUCqVWlchxMXF4f79+3j77bcBPP5hpVOnToiMjJQdV0xMjOxqhOTkZBw/fhxbtmwB8PgHgj59+iAiIgLt27eXzWFiYlLug/CUSmWF/xyIiIiIiIieVmU71s7OzlAoFEhOTpbVOzk5wdnZGSYmJlp9DA0Npc8lCVppdcXFxQCAmjVr4s6dO7Ix7ty5AwsLi1LHfxGcnJxgbW2NlJSUMmO8vb1RWFiIa9eulbvOkrbSGBoawtPTU2seMzMzODs7o1WrVoiIiICBgYF0aXXNmjXx119/yeILCwuRmZlZ5jxPy8nJwcmTJzFmzBgYGBjAwMAAs2fPxpkzZ2BgYIB9+/aV2s/a2hoPHjxAQUGBrD4iIgKZmZkwMTGRxtu5cyd+/PFH6c/x9u3bSExMlD35OyIiAoWFhbC3t5f6hYeHY9OmTdBoNLI5MjMzYWNjU6HjIyIiIiIiqqwqS6ytrKzQqVMnLF269KW9Z9jHxwd79+6V1cXFxcl2fl+0GzduICMjA7Vq1SozRq1WQ09PT9pF9vHxwcGDB/Ho0SPZOl1cXGBpaVnqGEVFRTh37ly58wCPf2Qo2fn38fFBVlYWTp06JbXv27cPxcXF8Pb2rtDxWVhY4Ny5c1Cr1VIZNWoUXFxcoFaryxyn5HVcFy5ckOoyMjIQExOD6Oho2XiJiYm4d+8eYmNjAQDbt29H69atUaNGDQCPfwxYs2YNFixYIOt35swZ2Nvbaz2V/fz58/D09KzQ8REREREREVXaq3iSWllSUlKEnZ2dcHV1FdHR0eLChQvi4sWLYu3atcLOzk5MmjRJigXkT+xOTU0VAERiYqJUt3//fgFA3Lt3Twjx+GnTpqamYurUqeKPP/4Qy5YtE/r6+mLXrl1Sn5ycHJGYmCgSExMFALFw4UKRmJgo0tLSnrn+nJwcMWXKFJGQkCBSU1PFnj17RIsWLUTDhg1FXl6eEEKII0eOiEWLFgm1Wi2uXLki1q1bJ2xsbMSgQYOkcbKysoSdnZ0YOHCgOH/+vIiOjhampqbi+++/l2K++OILsXv3bnHlyhVx6tQp0bdvX2FsbCySkpKEEELk5uaKkJAQkZCQIK5duyZOnjwphg4dKpRKpTh//rw0TpcuXYSnp6c4duyYOHz4sGjYsKHo16+f7LiSkpJEYmKi6Natm2jfvr10fspSkaeCCyFEixYtxJIlS6TvixYtErVq1ZI9/b1E7969xfvvvy+EECIgIEAsWLBAatuyZYswMjISWVlZWv2CgoKEl5eX9D01NVUoFApx7dq1Z66vRMmT/1hYWFhYWFhYWP47hag0FX0qeJX/Dbp165YYM2aMqF+/vjA0NBTm5uaiZcuWIiwsTNy/f1+KAyqfWJfUeXh4CCMjI+Hk5CSioqJk85f0eboMHjz4mWt/8OCB6Ny5s7CxsRGGhobC0dFRjBgxQqSnp0sxp06dEt7e3kKlUgljY2Ph5uYmQkNDpcS7xJkzZ8Sbb74plEqlqF27tpg/f76sfcKECaJu3brCyMhI2NnZibffflucPn1aan/48KF47733hL29vTAyMhK1atUS3bt3F8ePH5eNk5GRIfr16yfMzc2FhYWFGDp0qMjJyZHFODo6Vuofm4om1suXLxetWrWSvjdr1kx88sknpcb+/PPPwsjISFy7dk0YGxuLy5cvS23vvPOOePvtt0vtd+zYMQFAnDlzRgghRGhoaJmvVysLE2sWFhYWFhYWlv9eISpNRRNrhRBPvZ+I6CV5+PAhXFxc8PPPP1f4cvzNmzdj+vTpskvIK6qgoAANGzbEhg0b4OvrW+F+2dnZUKlUlZ6PiIiIiP65mBZRaUpyA41GU+4DsqvsHmv67zExMcGaNWtw9+7dCvcxNzfHV1999VzzXb9+HZ999lmlkmoiIiIiIqLK4o51OQ4dOoSuXbuW2Z6bm/sKV0OvCnesiYiIiP57mBZRaSq6Y/3avcf6deLl5QW1Wl3VyyAiIiIiIqLXGBPrcpiYmMDZ2bmql0FERERERESvMd5jTURERERERKQDJtZEREREREREOuCl4ERleNYDCoiIiIiIiADuWBMRERERERHphIk1ERERERERkQ6YWBMRERERERHpgIk1ERERERERkQ6YWBMRERERERHpgIk1ERERERERkQ74ui2iMqhUqqpeAhERERFVkBCiqpdA/2HcsSYiIiIiIiLSARNrIiIiIiIiIh0wsSYiIiIiIiLSARNrIiIiIiIiIh0wsSYiIiIiIiLSARNrIiIiIiIiIh0wsSYiIiIiIiLSARNrIiIiIiIiIh0wsaZXasaMGfjoo49eyVx3796Fra0tbty48UrmIyIiIiKi/6YqT6zT09Mxfvx4ODs7w9jYGHZ2dvD19UV4eDgePHig8/jx8fFo0aIFlEolnJ2dsXr1aln7wYMH0a1bN9jb20OhUGDr1q2VGn/IkCFQKBSy0qVLF1lMvXr1tGLmz58vizl79izatGkDY2Nj1KlTB19//bWsffXq1VpjGBsby2JmzZoFV1dXmJmZwdLSEn5+fjh27Jgs5vTp0+jUqROqV68OKysrfPTRR8jNzdU6rtWrV8Pd3R3GxsawtbXF6NGjK7Xe0qSnp2Px4sWYNm2aVltCQgL09fUREBBQZv+0tDSYmJjI1nvjxg0YGRmhadOmWvHW1tYYNGgQZs6c+cy1ERERERERPa8qTayvXr0KT09PxMbGIjQ0FImJiUhISEBQUBB27NiBPXv26DR+amoqAgIC8NZbb0GtVmPChAkYPnw4du/eLcXcv38fzZs3x7Jly557ni5duuD27dtS+emnn7RiZs+eLYsZO3as1JadnY3OnTvD0dERp06dQlhYGGbNmoWVK1fKxrCwsJCNkZaWJmtv1KgRli5dinPnzuHw4cOoV68eOnfujL///hsAcOvWLfj5+cHZ2RnHjh3Drl27kJSUhCFDhsjGWbhwIaZNm4ZPP/0USUlJ2LNnD/z9/Su93qetWrUKrVu3hqOjo1ZbREQExo4di4MHD+LWrVul9o+JicFbb70Fc3NzqW716tXo3bs3srOztX5EAIChQ4di/fr1yMzMLHdtREREREREz01UIX9/f+Hg4CByc3NLbS8uLpY+AxArVqwQAQEBwsTERLi6uoojR46Iy5cvi3bt2glTU1Ph4+MjUlJSpD5BQUGiSZMmsjH79Okj/P39S50PgNiyZUuljmHw4MGiR48e5cY4OjqKRYsWldm+fPlyYWlpKfLz86W64OBg4eLiIn2PiooSKpWqUmvTaDQCgNizZ48QQojvv/9e2NraiqKiIinm7NmzAoC4fPmyEEKIzMxMYWJiIvV53vWWpkmTJmLp0qVa9Tk5OcLc3FxcvHhR9OnTR8ydO7fU/h06dBDh4eHS9+LiYuHk5CR27dolgoODxYgRI0rtV79+fbFq1apy1/akkvPGwsLCwsLCwsLyzylEL0NJbqDRaMqNq7Id64yMDMTGxmL06NEwMzMrNUahUMi+z5kzB4MGDYJarYarqyv69++PkSNHIiQkBCdPnoQQAmPGjJHiExIS4OfnJxvD398fCQkJL/RY4uPjYWtrCxcXF3z88cfIyMjQipk/fz6srKzg6emJsLAwFBYWytbZtm1bGBkZydaZnJyMe/fuSXW5ublwdHREnTp10KNHDyQlJZW5poKCAqxcuRIqlQrNmzcHAOTn58PIyAh6ev/vj93ExAQAcPjwYQBAXFwciouLcfPmTbi5ucHBwQG9e/fGn3/+Wen1PikzMxMXLlyAl5eXVtvGjRvh6uoKFxcXDBgwAJGRkRBCyGKysrJw+PBhdO/eXarbv38/Hjx4AD8/PwwYMADR0dG4f/++1vgtW7bEoUOHyjxX+fn5yM7OlhUiIiIiIqKKqrLEOiUlBUIIuLi4yOqtra1hbm4Oc3NzBAcHy9qGDh2K3r17o1GjRggODsa1a9cQGBgIf39/uLm5Yfz48YiPj5fi09PTYWdnJxvDzs4O2dnZePjw4Qs5ji5dumDNmjXYu3cvvvrqKxw4cABdu3ZFUVGRFDNu3DhER0dj//79GDlyJEJDQxEUFPTMdZa0AYCLiwsiIyMRExODdevWobi4GK1bt9Z6MNeOHTtgbm4OY2NjLFq0CHFxcbC2tgYAdOjQAenp6QgLC0NBQQHu3buHTz/9FABw+/ZtAI8vzy8uLkZoaCi+/fZb/Prrr8jMzESnTp1QUFBQ4fU+7fr16xBCwN7eXqstIiICAwYMkM6nRqPBgQMHZDE7d+6Eu7u7rH9ERAT69u0LfX19NG3aFE5OTvjll1+0xre3t9e6bP5J8+bNg0qlkkqdOnXKjCUiIiIiInpalT+87GnHjx+HWq1GkyZNkJ+fL2tzd3eXPpckcs2aNZPV5eXlvdIdx759+6J79+5o1qwZ3n33XezYsQMnTpyQJfiTJk1C+/bt4e7ujlGjRmHBggVYsmSJ1vGVx8fHB4MGDYKHhwfatWuHzZs3w8bGBt9//70sruR+8iNHjqBLly7o3bs3/vrrLwBAkyZN8OOPP2LBggUwNTVFzZo1Ub9+fdjZ2Um72MXFxXj06BG+++47+Pv7o1WrVvjpp59w+fJl7N+//7nPU8kPGU8/cC05ORnHjx9Hv379AAAGBgbo06cPIiIiZHExMTGy3eqsrCxs3rxZSsgBYMCAAVr9gMe78uU9CC8kJAQajUYqT+7OExERERERPYtBVU3s7OwMhUKB5ORkWb2TkxOA/3eJ8pMMDQ2lzyWXiZdWV1xcDACoWbMm7ty5Ixvjzp07sLCwKHX8F8HJyQnW1tZISUlBx44dS43x9vZGYWEhrl27BhcXlzLXWXIMpTE0NISnpydSUlJk9WZmZnB2doazszNatWqFhg0bIiIiAiEhIQCA/v37o3///rhz5w7MzMygUCiwcOFC6bzXqlULANC4cWNpTBsbG1hbW+P69evSmiq73pJd83v37sHGxkaqj4iIQGFhoWwnWggBpVKJpUuXQqVSoaCgALt27cJnn30mxWzYsAF5eXnw9vaW9SsuLsalS5fQqFEjqT4zM1M259OUSiWUSmWZ7UREREREROWpsh1rKysrdOrUCUuXLi31vtgXwcfHB3v37pXVxcXFwcfH56XMBzx+/VNGRoaUoJZGrVZDT08Ptra20joPHjyIR48eydbp4uICS0vLUscoKirCuXPnyp0HePwjQ2k743Z2djA3N8fPP/8MY2NjdOrUCQDg6+sLALIfPDIzM3H37l3pad7Ps94GDRrAwsICFy5ckOoKCwuxZs0aLFiwAGq1WipnzpyBvb299HT1+Ph4WFpaSveKA48T8smTJ2v1a9OmDSIjI2Vznz9/Hp6enuWeJyIiIiIiouf20h+jVo6UlBRhZ2cnXF1dRXR0tLhw4YK4ePGiWLt2rbCzsxOTJk2SYgH5E7tTU1MFAJGYmCjV7d+/XwAQ9+7dE0IIcfXqVWFqaiqmTp0q/vjjD7Fs2TKhr68vdu3aJfXJyckRiYmJIjExUQAQCxcuFImJiSItLe2Z68/JyRFTpkwRCQkJIjU1VezZs0e0aNFCNGzYUOTl5QkhhDhy5IhYtGiRUKvV4sqVK2LdunXCxsZGDBo0SBonKytL2NnZiYEDB4rz58+L6OhoYWpqKr7//nsp5osvvhC7d+8WV65cEadOnRJ9+/YVxsbGIikpSQghRG5urggJCREJCQni2rVr4uTJk2Lo0KFCqVSK8+fPS+MsWbJEnDp1SiQnJ4ulS5cKExMTsXjxYtlx9ejRQzRp0kT83//9nzh37px45513ROPGjUVBQUGF11uanj17ismTJ0vft2zZIoyMjERWVpZWbFBQkPDy8hJCCDF69GgxduxYqa3kz+qPP/7Q6rd8+XJRs2ZN8ejRIyGEEPfv3xcmJibi4MGD5a7tSXwqOAsLCwsLCwvLP68QvQwVfSp4lf8NvHXrlhgzZoyoX7++MDQ0FObm5qJly5YiLCxM3L9/X4oDKp9Yl9R5eHgIIyMj4eTkJKKiomTzl/R5ugwePPiZa3/w4IHo3LmzsLGxEYaGhsLR0VGMGDFCpKenSzGnTp0S3t7eQqVSCWNjY+Hm5iZCQ0OlxLvEmTNnxJtvvimUSqWoXbu2mD9/vqx9woQJom7dusLIyEjY2dmJt99+W5w+fVpqf/jwoXjvvfeEvb29MDIyErVq1RLdu3cXx48fl40zcOBAUaNGDWFkZCTc3d3FmjVrtI5Lo9GIYcOGierVq4saNWqI9957T1y/fr1S6y3Nzp07Re3ataXXfb3zzjvi7bffLjX22LFjAoA4c+aMqFOnjoiLi5PaxowZIxo3blxqv9u3bws9PT0RExMjhBBiw4YNz3wN2NOYWLOwsLCwsLCw/PMK0ctQ0cRaIcRT7zUiekmEEPD29sbEiROlh5U9y+nTp9GhQwf8/fffsvvpK6pVq1YYN24c+vfvX+E+2dnZUKlUlZ6LiIiIiKoO0xp6GUpyA41GAwsLizLjXrungtO/l0KhwMqVK2Xv8H6WwsJCLFmy5LmS6rt376Jnz54VTuKJiIiIiIieB3esy3Ho0CF07dq1zPbc3NxXuBp6VbhjTURERPTPw7SGXoaK7lhX2eu2/gm8vLygVqurehlERERERET0GmNiXQ4TExM4OztX9TKIiIiIiIjoNcZ7rImIiIiIiIh0wMSaiIiIiIiISAdMrImIiIiIiIh0wHusicrwrCf/ERERERERAdyxJiIiIiIiItIJE2siIiIiIiIiHTCxJiIiIiIiItIBE2siIiIiIiIiHTCxJiIiIiIiItIBE2siIiIiIiIiHfB1W0RlUKlUVb0EIiIioucmhKjqJRD9Z3DHmoiIiIiIiEgHTKyJiIiIiIiIdMDEmoiIiIiIiEgHTKyJiIiIiIiIdMDEmoiIiIiIiEgHTKyJiIiIiIiIdMDEmoiIiIiIiEgHTKyJiIiIiIiIdMDEml6pGTNm4KOPPnolc929exe2tra4cePGK5mPiIiIiIj+m6o8sU5PT8f48ePh7OwMY2Nj2NnZwdfXF+Hh4Xjw4IHO48fHx6NFixZQKpVwdnbG6tWrZe0HDx5Et27dYG9vD4VCga1bt1Zq/CFDhkChUMhKly5dZDH16tXTipk/f74s5uzZs2jTpg2MjY1Rp04dfP3111pzZWVlYfTo0ahVqxaUSiUaNWqEnTt3ymKWLVuGevXqwdjYGN7e3jh+/LjUlpmZibFjx8LFxQUmJiaoW7cuxo0bB41GIxvj6bUqFApER0drzePm5gYTExO4uLhgzZo1zzxX6enpWLx4MaZNm6bVlpCQAH19fQQEBJTZPy0tDSYmJsjNzZXqbty4ASMjIzRt2lQr3traGoMGDcLMmTOfuTYiIiIiIqLnJqrQlStXRM2aNYWrq6v4+eefxYULF8SVK1fE1q1bxdtvvy1iYmJ0Gv/q1avC1NRUTJo0SVy4cEEsWbJE6Ovri127dkkxO3fuFNOmTRObN28WAMSWLVsqNcfgwYNFly5dxO3bt6WSmZkpi3F0dBSzZ8+WxeTm5krtGo1G2NnZicDAQHH+/Hnx008/CRMTE/H9999LMfn5+cLLy0u8/fbb4vDhwyI1NVXEx8cLtVotxURHRwsjIyMRGRkpkpKSxIgRI0T16tXFnTt3hBBCnDt3TvTs2VNs27ZNpKSkiL1794qGDRuKXr16ydYLQERFRcnW+/DhQ6l9+fLlolq1aiI6OlpcuXJF/PTTT8Lc3Fxs27at3HM1Z84c4e/vX2rbhx9+KMaPHy/Mzc3FzZs3S41ZvHix6Nq1q9aYgYGBok6dOuLo0aNafc6fPy+USqXIyMgod21P0mg0AgALCwsLCwsLyz+6EJHuSnIDjUZTblyV/i/O399fODg4yJLMJxUXF0ufAYgVK1aIgIAAYWJiIlxdXcWRI0fE5cuXRbt27YSpqanw8fERKSkpUp+goCDRpEkT2Zh9+vQpM7kDni+x7tGjR7kxjo6OYtGiRWW2L1++XFhaWor8/HypLjg4WLi4uEjfw8PDhZOTkygoKChznJYtW4rRo0dL34uKioS9vb2YN29emX02btwojIyMxKNHj6S6Z50HHx8fMWXKFFndpEmThK+vb5l9hBCiSZMmYunSpVr1OTk5wtzcXFy8eFH06dNHzJ07t9T+HTp0EOHh4dL34uJi4eTkJHbt2iWCg4PFiBEjSu1Xv359sWrVqnLX9iQm1iwsLCwsLCz/hkJEuqtoYl1ll4JnZGQgNjYWo0ePhpmZWakxCoVC9n3OnDkYNGgQ1Go1XF1d0b9/f4wcORIhISE4efIkhBAYM2aMFJ+QkAA/Pz/ZGP7+/khISHihxxIfHw9bW1u4uLjg448/RkZGhlbM/PnzYWVlBU9PT4SFhaGwsFC2zrZt28LIyEi2zuTkZNy7dw8AsG3bNvj4+GD06NGws7ND06ZNERoaiqKiIgBAQUEBTp06JTtePT09+Pn5lXu8Go0GFhYWMDAwkNWPHj0a1tbWaNmyJSIjIyGEkNry8/NhbGwsizcxMcHx48fx6NGjUufJzMzEhQsX4OXlpdW2ceNGuLq6wsXFBQMGDNCaD3h8Gfzhw4fRvXt3qW7//v148OAB/Pz8MGDAAERHR+P+/fta47ds2RKHDh0q8xzk5+cjOztbVoiIiIiIiCqqyhLrlJQUCCHg4uIiq7e2toa5uTnMzc0RHBwsaxs6dCh69+6NRo0aITg4GNeuXUNgYCD8/f3h5uaG8ePHIz4+XopPT0+HnZ2dbAw7OztkZ2fj4cOHL+Q4unTpgjVr1mDv3r346quvcODAAXTt2lVKeAFg3LhxiI6Oxv79+zFy5EiEhoYiKCjomessaQOAq1ev4tdff0VRURF27tyJGTNmYMGCBfjyyy8BPH5QV1FRUanjlIzxtLt372LOnDlaDxObPXs2Nm7ciLi4OPTq1QuffPIJlixZIrX7+/tj1apVOHXqFIQQOHnyJFatWoVHjx7h7t27pc51/fp1CCFgb2+v1RYREYEBAwZI51Oj0eDAgQOymJ07d8Ld3V3WPyIiAn379oW+vj6aNm0KJycn/PLLL1rj29vbIy0trdR1AcC8efOgUqmkUqdOnTJjiYiIiIiInmbw7JBX6/jx4yguLkZgYCDy8/Nlbe7u7tLnkgSyWbNmsrq8vDxkZ2fDwsLilay3b9++0udmzZrB3d0dDRo0QHx8PDp27AgAmDRpkhTj7u4OIyMjjBw5EvPmzYNSqazQPMXFxbC1tcXKlSuhr6+PN954Azdv3kRYWNhzPZwrOzsbAQEBaNy4MWbNmiVrmzFjhvTZ09MT9+/fR1hYGMaNGye1p6eno1WrVhBCwM7ODoMHD8bXX38NPb3Sf6sp+SHj6Z3u5ORkHD9+HFu2bAEAGBgYoE+fPoiIiED79u2luJiYGNludVZWFjZv3ozDhw9LdQMGDEBERASGDBkim8PExKTcB+GFhITI/oyys7OZXBMRERERUYVV2Y61s7MzFAoFkpOTZfVOTk5wdnaGiYmJVh9DQ0Ppc8ll4qXVFRcXAwBq1qyJO3fuyMa4c+cOLCwsSh3/RXBycoK1tTVSUlLKjPH29kZhYSGuXbtW7jpL2gCgVq1aaNSoEfT19aUYNzc3pKeno6CgANbW1tDX1y91nJIxSuTk5KBLly6oVq0atmzZIjuHZa33xo0b0g8dJiYmiIyMxIMHD3Dt2jVcv34d9erVQ7Vq1WBjY1PqGNbW1gAgXdpeIiIiAoWFhbC3t4eBgQEMDAwQHh6OTZs2SU8rLygowK5du2SJ9YYNG5CXlwdvb2+pX3BwMA4fPoxLly7J5sjMzCxzXQCgVCphYWEhK0RERERERBVVZYm1lZUVOnXqhKVLl5Z6X+yL4OPjg71798rq4uLi4OPj81LmAx6//ikjIwO1atUqM0atVkNPTw+2trbSOg8ePCi7PzkuLg4uLi6wtLQEAPj6+iIlJUX60QAALl26hFq1asHIyAhGRkZ44403ZMdbXFyMvXv3yo43OzsbnTt3hpGREbZt26a1g1zWei0tLbV21w0NDeHg4AB9fX1ER0fjnXfeKXPHukGDBrCwsMCFCxekusLCQqxZswYLFiyAWq2WypkzZ2Bvb4+ffvoJwON72C0tLdG8eXOpb0REBCZPnqzVr02bNoiMjJTNff78eXh6ej7zOImIiIiIiJ7Ly36KWnlSUlKEnZ2dcHV1FdHR0eLChQvi4sWLYu3atcLOzk5MmjRJigXkT6pOTU0VAERiYqJUt3//fgFA3Lt3Twjx/163NXXqVPHHH3+IZcuWab1uKycnRyQmJorExEQBQCxcuFAkJiaKtLS0Z64/JydHTJkyRSQkJIjU1FSxZ88e0aJFC9GwYUORl5cnhBDiyJEjYtGiRUKtVosrV66IdevWCRsbGzFo0CBpnKysLGFnZycGDhwozp8/L6Kjo4WpqansdVvXr18X1apVE2PGjBHJyclix44dwtbWVnz55ZdSTHR0tFAqlWL16tXiwoUL4qOPPhLVq1cX6enpQojHT7Tz9vYWzZo1EykpKbLXaRUWFgohhNi2bZv44YcfxLlz58Tly5fF8uXLhampqfj888+leZKTk8XatWvFpUuXxLFjx0SfPn1EjRo1RGpqarnnq2fPnmLy5MnS9y1btggjIyORlZWlFRsUFCS8vLyEEEKMHj1ajB07Vmor+bP6448/tPotX75c1KxZU3rK+f3794WJiYk4ePBguWt7Ep8KzsLCwsLCwvJvKESku3/E67aEEOLWrVtizJgxon79+sLQ0FCYm5uLli1birCwMHH//n0pDqh8Yl1S5+HhIYyMjISTk5OIioqSzV/S5+kyePDgZ679wYMHonPnzsLGxkYYGhoKR0dHMWLECCmRFUKIU6dOCW9vb6FSqYSxsbFwc3MToaGhUuJd4syZM+LNN98USqVS1K5dW8yfP19rviNHjghvb2+hVCqFk5OTmDt3rpQQl1iyZImoW7euMDIyEi1btpS927msYwUgJcW///678PDwEObm5sLMzEw0b95crFixQhQVFUnjXLhwQXh4eAgTExNhYWEhevToIS5evPjM87Vz505Ru3Ztaax33nlHvP3226XGHjt2TAAQZ86cEXXq1BFxcXFS25gxY0Tjxo1L7Xf79m2hp6cnvQN9w4YNsteWVQQTaxYWFhYWFpZ/QyEi3VU0sVYI8dR7jYheEiEEvL29MXHiRPTr169CfU6fPo0OHTrg77//fua94KVp1aoVxo0bh/79+1e4T3Z2NlQqVaXnIiIiInqd8D/ziXRXkhuUvKa4LFV2jzX99ygUCqxcuVL2Du9nKSwsxJIlS54rqb579y569uxZ4SSeiIiIiIjoeXDHuhyHDh1C165dy2zPzc19hauhV4U71kRERPRvwP/MJ9JdRXesX7v3WL9OvLy8oFarq3oZRERERERE9BpjYl0OExMTODs7V/UyiIiIiIiI6DXGe6yJiIiIiIiIdMDEmoiIiIiIiEgHTKyJiIiIiIiIdMB7rInK8Kwn/xEREREREQHcsSYiIiIiIiLSCRNrIiIiIiIiIh0wsSYiIiIiIiLSARNrIiIiIiIiIh0wsSYiIiIiIiLSARNrIiIiIiIiIh3wdVtEZVCpVFW9BCIiInoFhBBVvQQi+ofjjjURERERERGRDphYExEREREREemAiTURERERERGRDphYExEREREREemAiTURERERERGRDphYExEREREREemAiTURERERERGRDphYExEREREREemAiTW9UhEREejcufMrm2/FihXo1q3bK5uPiIiIiIj+e6o8sU5PT8f48ePh7OwMY2Nj2NnZwdfXF+Hh4Xjw4IHO48fHx6NFixZQKpVwdnbG6tWrZe0HDx5Et27dYG9vD4VCga1bt1Zq/CFDhkChUMhKly5dZDH16tXTipk/f74s5uzZs2jTpg2MjY1Rp04dfP3117L21atXa41hbGystZ4//vgD3bt3h0qlgpmZGf73v//h+vXrUnt6ejoGDhyImjVrwszMDC1atMCmTZu0xvntt9/g7e0NExMTWFpa4t1335W1X79+HQEBATA1NYWtrS2mTp2KwsLCcs9VXl4eZsyYgZkzZ8rqs7OzMW3aNLi6usLY2Bg1a9aEn58fNm/eDCGELPatt97CqlWrpO+bNm1C+/btoVKpYG5uDnd3d8yePRuZmZkAgGHDhuH06dM4dOhQuWsjIiIiIiJ6bqIKXblyRdSsWVO4urqKn3/+WVy4cEFcuXJFbN26Vbz99tsiJiZGp/GvXr0qTE1NxaRJk8SFCxfEkiVLhL6+vti1a5cUs3PnTjFt2jSxefNmAUBs2bKlUnMMHjxYdOnSRdy+fVsqmZmZshhHR0cxe/ZsWUxubq7UrtFohJ2dnQgMDBTnz58XP/30kzAxMRHff/+9FBMVFSUsLCxkY6Snp8vmSUlJETVq1BBTp04Vp0+fFikpKSImJkbcuXNHiunUqZP43//+J44dOyauXLki5syZI/T09MTp06elmF9//VVYWlqK8PBwkZycLJKSksTPP/8stRcWFoqmTZsKPz8/kZiYKHbu3Cmsra1FSEhIuedq7dq1wsXFRVZ379490aRJE+Hg4CBWr14tkpKSRHJysli5cqVo0KCBuHfvnhSbkZEhDA0NpeP+7LPPhL6+vpgyZYr4v//7P5GamipiY2NFz549xbfffiv1mzJlinj//ffLXduTNBqNAMDCwsLCwsLyHylERGUpyQ00Gk25cVX6L4m/v79wcHCQJZlPKi4ulj4DECtWrBABAQHCxMREuLq6iiNHjojLly+Ldu3aCVNTU+Hj4yNSUlKkPkFBQaJJkyayMfv06SP8/f1LnQ94vsS6R48e5cY4OjqKRYsWldm+fPlyYWlpKfLz86W64OBgWRIaFRUlVCpVufP06dNHDBgwoNwYMzMzsWbNGlldjRo1xA8//CCEEOLRo0eidu3aYtWqVWWOsXPnTqGnpydL7MPDw4WFhYXsGJ4WEBAgpkyZIqv7+OOPhZmZmbh586ZWfE5Ojnj06JH0fc2aNcLb21sIIcSxY8cEAFkC/aQnE/IDBw4IIyMj8eDBg1Jj8/LyhEajkcqff/5Z5f8Hz8LCwsLCwvLqChFRWSqaWFfZpeAZGRmIjY3F6NGjYWZmVmqMQqGQfZ8zZw4GDRoEtVoNV1dX9O/fHyNHjkRISAhOnjwJIQTGjBkjxSckJMDPz082hr+/PxISEl7oscTHx8PW1hYuLi74+OOPkZGRoRUzf/58WFlZwdPTE2FhYbLLphMSEtC2bVsYGRnJ1pmcnIx79+5Jdbm5uXB0dESdOnXQo0cPJCUlSW3FxcX47bff0KhRI/j7+8PW1hbe3t5al7a3bt0aP//8MzIzM1FcXIzo6Gjk5eWhffv2AIDTp0/j5s2b0NPTg6enJ2rVqoWuXbvi/PnzsvU2a9YMdnZ2svVmZ2fL1vS0w4cPw8vLS7bm6OhoBAYGwt7eXive3NwcBgYG0vdt27ahR48eAID169fD3Nwcn3zySalzVa9eXfrs5eWFwsJCHDt2rNTYefPmQaVSSaVOnTplHgMREREREdHTqiyxTklJgRACLi4usnpra2uYm5vD3NwcwcHBsrahQ4eid+/eaNSoEYKDg3Ht2jUEBgbC398fbm5uGD9+POLj46X49PR0WfIHAHZ2dsjOzsbDhw9fyHF06dIFa9aswd69e/HVV1/hwIED6Nq1K4qKiqSYcePGITo6Gvv378fIkSMRGhqKoKCgZ66zpA0AXFxcEBkZiZiYGKxbtw7FxcVo3bo1bty4AQD466+/kJubi/nz56NLly6IjY3Fe++9h549e+LAgQPSuBs3bsSjR49gZWUFpVKJkSNHYsuWLXB2dgYAXL16FQAwa9YsTJ8+HTt27IClpSXat28v3bdckfU+LSsrCxqNRpZA3717F/fu3YOrq+szz3N+fj527dqF7t27AwAuX74MJycnGBoaPrOvqakpVCoV0tLSSm0PCQmBRqORyp9//vnMMYmIiIiIiEoYPDvk1Tp+/DiKi4sRGBiI/Px8WZu7u7v0uSSRa9asmawuLy8P2dnZsLCweCXr7du3r/S5WbNmcHd3R4MGDRAfH4+OHTsCACZNmiTFuLu7w8jICCNHjsS8efOgVCorNI+Pjw98fHyk761bt4abmxu+//57zJkzB8XFxQCAHj16YOLEiQAADw8PHDlyBCtWrEC7du0AADNmzEBWVhb27NkDa2trbN26Fb1798ahQ4fQrFkzaZxp06ahV69eAICoqCg4ODjgl19+wciRI5/rPJX8kPHkA9fEUw8mK8++fftga2uLJk2aVLovAJiYmJT5MDylUlnhPwciIiIiIqKnVdmOtbOzMxQKBZKTk2X1Tk5OcHZ2homJiVafJ3cnSy4TL62uJDmsWbMm7ty5Ixvjzp07sLCwKHX8F8HJyQnW1tZISUkpM8bb2xuFhYW4du1auessaSuNoaEhPD09pXmsra1hYGCAxo0by+Lc3Nykp4JfuXIFS5cuRWRkJDp27IjmzZtj5syZ8PLywrJlywAAtWrVAgDZOEqlEk5OTtI4z7NeKysrKBQK2aXtNjY2qF69Oi5evFhqnydt27ZN2q0GgEaNGuHq1at49OjRM/sCQGZmJmxsbCoUS0REREREVBlVllhbWVmhU6dOWLp0Ke7fv/9S5vDx8cHevXtldXFxcbKd3xftxo0byMjIkBLU0qjVaujp6cHW1lZa58GDB2VJYlxcHFxcXGBpaVnqGEVFRTh37pw0j5GREf73v/9p/VBx6dIlODo6AoC0Y6unJ/9j19fXl36MeOONN6BUKmXjPHr0CNeuXZPG8fHxwblz5/DXX3/J1mthYaGV2JcwMjJC48aNceHCBalOT08Pffv2xfr163Hr1i2tPrm5uSgsLIQQAtu3b5furwaA/v37Izc3F8uXLy91vqysLOnzlStXkJeXB09Pz1JjiYiIiIiIdPLyn6NWtpSUFGFnZydcXV1FdHS0uHDhgrh48aJYu3atsLOzE5MmTZJiAfkTu1NTUwUAkZiYKNXt379fAJCeCF3yuq2pU6eKP/74QyxbtkzrdVs5OTkiMTFRJCYmCgBi4cKFIjExUaSlpT1z/Tk5OWLKlCkiISFBpKamij179ogWLVqIhg0biry8PCGEEEeOHBGLFi0SarVaXLlyRaxbt07Y2NiIQYMGSeNkZWUJOzs7MXDgQHH+/HkRHR0tTE1NZa/b+uKLL8Tu3bvFlStXxKlTp0Tfvn2FsbGxSEpKkmI2b94sDA0NxcqVK8Xly5el14sdOnRICCFEQUGBcHZ2Fm3atBHHjh0TKSkp4ptvvhEKhUL89ttv0jjjx48XtWvXFrt37xYXL14UH374obC1tZVeI1byuq3OnTsLtVotdu3aJWxsbJ75uq1JkyaJXr16yeoyMjKEq6urcHBwED/++KNISkoSly5dEhEREcLZ2Vncu3dPnDhxQlhaWsqeEC7E46e+6+vri6lTp4ojR46Ia9euiT179oj3339f9rTwqKgo4eTk9Mw/zxJ83RYLCwsLC8t/qxARleUf8botIYS4deuWGDNmjKhfv74wNDQU5ubmomXLliIsLEzcv39figMqn1iX1Hl4eAgjIyPh5OQkoqKiZPOX9Hm6DB48+Jlrf/DggejcubOwsbERhoaGwtHRUYwYMUL2GqpTp04Jb29voVKphLGxsXBzcxOhoaFS4l3izJkz4s033xRKpVLUrl1bzJ8/X9Y+YcIEUbduXWFkZCTs7OzE22+/LXv3dImShNTY2Fg0b95cbN26VdZ+6dIl0bNnT2FraytMTU2Fu7u71uu3CgoKxOTJk4Wtra2oVq2a8PPzE+fPn5fFXLt2TXTt2lWYmJgIa2trMXnyZK3E92lJSUnCxMREZGVlyeqzsrLEp59+Kho2bCgdn5+fn9iyZYsoLi4W06dPF4GBgaWO+fPPP4u2bduKatWqCTMzM+Hu7i5mz54t+zvQuXNnMW/evHLX9iQm1iwsLCwsLP+tQkRUloom1gohKvkUKCIdfPDBB2jRogVCQkIq3Mfd3R3Tp09H7969Kz1fUlISOnTogEuXLkGlUlWoT3Z2doVjiYiI6J+P/zlMRGUpyQ00Gk25D8iusnus6b8pLCwM5ubmFY4vKChAr1690LVr1+ea7/bt21izZg0TZSIiIiIiemm4Y12OQ4cOlZvQ5ebmvsLV0KvCHWsiIqL/Fv7nMBGVpaI71q/de6xfJ15eXlCr1VW9DCIiIiIiInqNMbEuh4mJCZydnat6GURERERERPQa4z3WRERERERERDpgYk1ERERERESkA14KTlSGZz2ggIiIiIiICOCONREREREREZFOmFgTERERERER6YCJNREREREREZEOmFgTERERERER6YCJNREREREREZEOmFgTERERERER6YCv2yIqg0qlquolEBG9NoQQVb0EIiKi1xZ3rImIiIiIiIh0wMSaiIiIiIiISAdMrImIiIiIiIh0wMSaiIiIiIiISAdMrImIiIiIiIh0wMSaiIiIiIiISAdMrImIiIiIiIh0wMSaiIiIiIiISAdMrOmVGjhwIEJDQ1/JXBcuXICDgwPu37//SuYjIiIiIqL/pipPrNPT0zF+/Hg4OzvD2NgYdnZ28PX1RXh4OB48eKDz+PHx8WjRogWUSiWcnZ2xevVqWfvBgwfRrVs32NvbQ6FQYOvWrZUaf8iQIVAoFLLSpUsXWUy9evW0YubPny+LOXv2LNq0aQNjY2PUqVMHX3/9dZlzRkdHQ6FQ4N1335XVPz1HSQkLC5NiMjMzERgYCAsLC1SvXh0ffvghcnNzK7WWR48eYfbs2WjQoAGMjY3RvHlz7Nq165nn6syZM9i5cyfGjRun1fbTTz9BX18fo0ePLrP/gQMHUKdOHVldQkIC9PX1ERAQoBXfuHFjtGrVCgsXLnzm2oiIiIiIiJ5XlSbWV69ehaenJ2JjYxEaGorExEQkJCQgKCgIO3bswJ49e3QaPzU1FQEBAXjrrbegVqsxYcIEDB8+HLt375Zi7t+/j+bNm2PZsmXPPU+XLl1w+/Ztqfz0009aMbNnz5bFjB07VmrLzs5G586d4ejoiFOnTiEsLAyzZs3CypUrtca5du0apkyZgjZt2mi1PTn+7du3ERkZCYVCgV69ekkxgYGBSEpKQlxcHHbs2IGDBw/io48+qtRapk+fju+//x5LlizBhQsXMGrUKLz33ntITEws9zwtWbIEH3zwAczNzbXaIiIiEBQUhJ9++gl5eXml9o+JiUG3bt20+o0dOxYHDx7ErVu3tPoMHToU4eHhKCwsLHdtREREREREz01UIX9/f+Hg4CByc3NLbS8uLpY+AxArVqwQAQEBwsTERLi6uoojR46Iy5cvi3bt2glTU1Ph4+MjUlJSpD5BQUGiSZMmsjH79Okj/P39S50PgNiyZUuljmHw4MGiR48e5cY4OjqKRYsWldm+fPlyYWlpKfLz86W64OBg4eLiIosrLCwUrVu3FqtWrarQvD169BAdOnSQvl+4cEEAECdOnJDqfv/9d6FQKMTNmzcrvJZatWqJpUuXyubq2bOnCAwMLHMthYWFQqVSiR07dmi1Xb16VZiYmIisrCzh7e0t1q9fX+oYDRo0EL///rv0PScnR5ibm4uLFy+KPn36iLlz52r1yc/PF0qlUuzZs6fMtT1No9EIACwsLCwsTxQiIqL/opLcQKPRlBtXZTvWGRkZiI2NxejRo2FmZlZqjEKhkH2fM2cOBg0aBLVaDVdXV/Tv3x8jR45ESEgITp48CSEExowZI8UnJCTAz89PNoa/vz8SEhJe6LHEx8fD1tYWLi4u+Pjjj5GRkaEVM3/+fFhZWcHT0xNhYWGyHdSEhAS0bdsWRkZGsnUmJyfj3r17Ut3s2bNha2uLDz/88JlrunPnDn777TdZbEJCAqpXrw4vLy+pzs/PD3p6ejh27FiF15Kfnw9jY2PZfCYmJjh8+HCZ6zl79iw0Go1s7hJRUVEICAiASqXCgAEDEBERoRWTlJSEv/76Cx06dJDqNm7cCFdXV7i4uGDAgAGIjIyEEELWz8jICB4eHjh06FCZa8vPz0d2drasEBERERERVVSVJdYpKSkQQsDFxUVWb21tDXNzc5ibmyM4OFjWNnToUPTu3RuNGjVCcHAwrl27hsDAQPj7+8PNzQ3jx49HfHy8FJ+eng47OzvZGHZ2dsjOzsbDhw9fyHF06dIFa9aswd69e/HVV1/hwIED6Nq1K4qKiqSYcePGITo6Gvv378fIkSMRGhqKoKCgZ66zpA0ADh8+jIiICPzwww8VWtePP/6IatWqoWfPnrJ5bG1tZXEGBgaoUaOGNE9F1uLv74+FCxfi8uXLKC4uRlxcHDZv3ozbt2+XuZ60tDTo6+trzV9cXIzVq1djwIABAIC+ffvi8OHDSE1NlcXFxMTA399flvBHRERI/bp06QKNRoMDBw5ozW1vb4+0tLQy1zZv3jyoVCqpPH0fNxERERERUXmq/OFlTzt+/DjUajWaNGmC/Px8WZu7u7v0uSTZa9asmawuLy/vle449u3bF927d0ezZs3w7rvvYseOHThx4oQswZ80aRLat28Pd3d3jBo1CgsWLMCSJUu0jq8sOTk5GDhwIH744QdYW1tXqE9kZCQCAwO1dpZfhMWLF6Nhw4ZwdXWFkZERxowZg6FDh0JPr+y/Tg8fPoRSqdS6CiEuLg7379/H22+/DeDxDyudOnVCZGSkLC4mJgbdu3eXvicnJ+P48ePo168fgMc/EPTp06fU3W4TE5NyH4QXEhICjUYjlT///PPZJ4GIiIiIiOj/Z1BVEzs7O0OhUCA5OVlW7+TkBOBxMvQ0Q0ND6XNJglZaXXFxMQCgZs2auHPnjmyMO3fuwMLCotTxXwQnJydYW1sjJSUFHTt2LDXG29sbhYWFuHbtGlxcXMpcZ8kxXLlyBdeuXZM9uKvkGA0MDJCcnIwGDRpIbYcOHUJycjJ+/vln2Zg1a9bEX3/9JasrLCxEZmYmatasKcWUtxYAsLGxwdatW5GXl4eMjAzY29vj008/lf7sSmNtbY0HDx6goKBAa9c5MzNT9udRXFyMs2fP4osvvoCenh5u376NxMRE2ZO/IyIiUFhYCHt7e6lOCAGlUomlS5dCpVJJ9ZmZmbLz8zSlUgmlUllmOxERERERUXmqbMfaysoKnTp1wtKlS1/ae4Z9fHywd+9eWV1cXBx8fHxeynwAcOPGDWRkZKBWrVplxqjVaujp6UmXRfv4+ODgwYN49OiRbJ0uLi6wtLSEq6srzp07B7VaLZXu3btLTzt/+tLliIgIvPHGG2jevLms3sfHB1lZWTh16pRUt2/fPhQXF8Pb27tCa3mSsbExateujcLCQmzatAk9evQo85g9PDwAPH63dImMjAzExMQgOjpadmyJiYm4d+8eYmNjAQDbt29H69atUaNGDQCPfwxYs2YNFixYIOt35swZ2Nvbaz2V/fz58/D09CxzbURERERERDp5BQ9SK1NKSoqws7MTrq6uIjo6Wly4cEFcvHhRrF27VtjZ2YlJkyZJsYD8id2pqakCgEhMTJTq9u/fLwCIe/fuCSEeP23a1NRUTJ06Vfzxxx9i2bJlQl9fX+zatUvqk5OTIxITE0ViYqIAIBYuXCgSExNFWlraM9efk5MjpkyZIhISEkRqaqrYs2ePaNGihWjYsKHIy8sTQghx5MgRsWjRIqH+/9q7+7ie7v9/4I+36l3vSqFUsqRExaqhacnGJoVYxsdcZDJjV5mLmVyMD19uK+a7YRcutrXMB8MumNnMEimTq3iHWETYhfi4qFQq1fP3x76d347epbwl7HG/2Z0xUgAANGtJREFU3c7t1nm9Xud1nue59+Tpdd7n6PVy+vRpWb16tTRv3lxGjhypzJObmyuOjo7ywgsvyLFjx2TdunViaWkpK1asqPbc1T0VPC8vTywtLWXZsmUGj+vdu7d07NhR9u3bJ7t375a2bdvKsGHD6hTL3r175ZtvvpHTp09LcnKyPPPMM+Lm5qbkvTqdOnWSDz/8UNlftGiRtGjRQvX090rPP/+8/Otf/xIRkbCwMHnvvfeUvo0bN4pWq5Xc3Nwqx0VHR4u/v7+yn52dLRqNRs6ePVtjbH/Hp4Jz48aNW9WNiIjon6i2TwVv8N+Uf/75p4wbN07c3NzEzMxMrK2tpUuXLrJw4UIpLCxUxgF1L6wr2x577DHRarXi7u4u8fHxqvNXHnPrFhkZedvYi4qKJCQkRJo3by5mZmbi6uoqY8eOlZycHGVMWlqaBAQEiK2trVhYWIi3t7fExMQohXel9PR06datm5ibm0vLli1l/vz5NZ67usJ6xYoVyqurDLly5YoMGzZMrK2txcbGRl588UW5fv16nWJJSkoSb29vMTc3Fzs7O3nhhReU13XVZOnSpfLEE08o+z4+PvL6668bHLt+/XrRarVy9uxZsbCwkFOnTil9/fr1k759+xo8bt++fQJA0tPTRUQkJiam2terVYeFNTdu3LhV3YiIiP6JaltYa0RueT8RUT25ceMGPD09sX79+lrfjv/tt99i5syZqlvIa6u0tBRt27bF2rVrERQUVOvj8vPzVd/RJiIiVHmdIRER0T9BZW2Ql5cHGxubasfdd08Fp4eXTqfDqlWrcPny5VofY21tjQULFtzR+c6fP48ZM2bUqagmIiIiIiKqK65Y1yAlJQV9+vSptr+goOAeRkP3ClesiYiq4l8XiIjon6i2K9YN9rqtB4G/vz/0en1Dh0FERERERET3MRbWNdDpdPDw8GjoMIiIiIiIiOg+xu9YExERERERERmBhTURERERERGREVhYExERERERERmB37EmqsbtnvxHREREREQEcMWaiIiIiIiIyCgsrImIiIiIiIiMwMKaiIiIiIiIyAgsrImIiIiIiIiMwMKaiIiIiIiIyAgsrImIiIiIiIiMwNdtEVXD1ta2oUMgIgNEpKFDICIiIlLhijURERERERGREVhYExERERERERmBhTURERERERGREVhYExERERERERmBhTURERERERGREVhYExERERERERmBhTURERERERGREVhYExERERERERmBhTU1iLi4OISEhNTrOUpLS9G6dWscPHiwXs9DRERERET/bPdNYZ2Tk4MJEybAw8MDFhYWcHR0RFBQEJYtW4aioiKj509KSkKnTp1gbm4ODw8PrFy5UtWfnJyM/v37w9nZGRqNBps2barT/KNGjYJGo1FtvXv3Vo1p3bp1lTHz589XjTly5AiefPJJWFhYwMXFBe+++66qf+XKlVXmsLCwUI2ZM2cOvLy8YGVlhaZNmyI4OBj79u1TjTl58iTCw8Nhb28PGxsbdOvWDTt37lSNGT9+PDp37gxzc3M89thjVa65uLgYo0aNgo+PD0xNTTFgwIBa5aq4uBizZs3C7Nmzq83L37dRo0Ypx964cQNWVlZ45JFHajymR48e0Gq1eOuttzB16tRaxUVERERERHQnTBs6AAA4c+YMgoKC0KRJE8TExMDHxwfm5uY4evQoPvnkE7Rs2RLPPvvsHc+fnZ2NsLAwvPrqq1izZg0SExMxZswYtGjRAqGhoQCAwsJC+Pn5YfTo0Rg4cOAdnad3796Ij49X9s3NzauMmTt3LsaOHavsN27cWPk5Pz8fISEhCA4OxvLly3H06FGMHj0aTZo0wcsvv6yMs7GxQWZmprKv0WhU52jXrh0++ugjuLu748aNG1i0aBFCQkKQlZWF5s2bAwD69euHtm3bYseOHdDpdFi8eDH69euH06dPw8nJSZlr9OjR2LdvH44cOVLlWsrLy6HT6TB+/Hh88803tc7T119/DRsbGwQFBQEADhw4gPLycgDAnj17MGjQIGRmZsLGxgYAoNPplGMTEhLg6uqK3bt3o7S0FADw22+/oUuXLti+fTs6dOgAANBqtQCAiIgITJ48GRkZGUofERERERHRXSX3gdDQUHnkkUekoKDAYH9FRYXyMwBZvny5hIWFiU6nEy8vL9mzZ4+cOnVKunfvLpaWlhIYGChZWVnKMdHR0dKhQwfVnEOGDJHQ0FCD5wMgGzdurNM1REZGSnh4eI1jXF1dZdGiRdX2L126VJo2bSolJSVK29SpU8XT01PZj4+PF1tb2zrFlpeXJwBk+/btIiLy3//+VwBIcnKyMiY/P18ASEJCQpXjZ8+eLX5+fjWeozbXXyksLEzeeustg307d+4UAHLt2jWD/aNHj5apU6eq2rKzswWAHD582OAxTz/9tMycObNWsYn8/3xx48bt/tyIiIiI7pXK2iAvL6/GcQ1+K/iVK1fw888/IyoqClZWVgbH3LoiO2/ePIwcORJ6vR5eXl4YPnw4XnnlFUyfPh0HDx6EiGDcuHHK+NTUVAQHB6vmCA0NRWpq6l29lqSkJDg4OMDT0xOvvfYarly5UmXM/PnzYWdnh44dO2LhwoUoKytTxfnUU08pq62VcWZmZuLatWtKW0FBAVxdXeHi4oLw8HBkZGRUG1NpaSk++eQT2Nraws/PDwBgZ2cHT09PrFq1CoWFhSgrK8OKFSvg4OCAzp07341U1Gj37t3w9/ev83EVFRXYsmULwsPD63Rcly5dkJKSUm1/SUkJ8vPzVRsREREREVFtNXhhnZWVBRGBp6enqt3e3h7W1tawtrau8h3ZF198Ec8//zzatWuHqVOn4uzZs4iIiEBoaCi8vb0xYcIEJCUlKeNzcnLg6OiomsPR0RH5+fm4cePGXbmO3r17Y9WqVUhMTMSCBQuwa9cu9OnTR7nFGfjrO8vr1q3Dzp078corryAmJgbR0dG3jbOyDwA8PT3x+eef47vvvsPq1atRUVGBrl274vfff1cdt2XLFlhbW8PCwgKLFi1CQkIC7O3tAfz1DxXbt2/H4cOH0bhxY1hYWOD999/HTz/9hKZNm96VfFQnNzcXeXl5cHZ2rvOxe/fuBQAEBATU6ThnZ2ecO3eu2v7Y2FjY2toqm4uLS51jIyIiIiKif6774jvWhuzfvx8VFRWIiIhASUmJqs/X11f5ubLw9PHxUbUVFxcjPz9f+Z5ufRs6dKjys4+PD3x9fdGmTRskJSWhZ8+eAIA333xTGePr6wutVotXXnkFsbGxBr+PbUhgYCACAwOV/a5du8Lb2xsrVqzAvHnzlPann34aer0ely9fxqeffornn38e+/btg4ODA0QEUVFRcHBwQEpKCnQ6HT777DP0798fBw4cQIsWLYxNR7Uq/yHj1geu1cZ3332Hfv36oVGjuv17kE6nq/EBeNOnT1f9t8nPz2dxTUREREREtdbgK9YeHh7QaDSqh3EBgLu7Ozw8PFQPrqpkZmam/Fx5m7ihtoqKCgCAk5MTLl68qJrj4sWLsLGxMTj/3eDu7g57e3tkZWVVOyYgIABlZWU4e/ZsjXFW9hliZmaGjh07VjmPlZUVPDw88MQTTyAuLg6mpqaIi4sDAOzYsQNbtmzBunXrEBQUhE6dOmHp0qXQ6XT44osv7vSSa8XOzg4ajUZ1a3ttbd68+Y4eYnf16lXloW2GmJubw8bGRrURERERERHVVoMX1nZ2dujVqxc++ugjFBYW1ss5AgMDkZiYqGpLSEhQrfzebb///juuXLlS4+qvXq9Ho0aN4ODgoMSZnJyMmzdvquL09PSs9hbt8vJyHD169LarzBUVFcrKf+Xq7a0rv40aNVL+MaK+aLVatG/fHsePH6/TcadOncK5c+fQq1evOp/z2LFj6NixY52PIyIiIiIiqo0GL6wBYOnSpSgrK4O/vz/Wr1+PEydOIDMzE6tXr8avv/4KExMTo+Z/9dVXcebMGURHR+PXX3/F0qVLsWHDBkyaNEkZU1BQAL1eD71eD+CvV3Tp9XqcP3/+tvMXFBRgypQp2Lt3L86ePYvExESEh4fDw8NDeZ1XamoqFi9ejPT0dJw5cwZr1qzBpEmTMGLECKVoHj58OLRaLV566SVkZGRg/fr1WLJkieo25blz5+Lnn3/GmTNncOjQIYwYMQLnzp3DmDFjAPz12rAZM2Zg7969OHfuHNLS0jB69Gj88ccfGDx4MIC/CvimTZsiMjIS6enpOHnyJKZMmaK8lqxSVlYW9Ho9cnJycOPGDSU/la+5AoDjx49Dr9fj6tWryMvLU+WwOqGhodi9e/dt8/p33333HYKDg2FpaVmn4wAgJSUFISEhdT6OiIiIiIioVu7FI8pr488//5Rx48aJm5ubmJmZibW1tXTp0kUWLlwohYWFyjhA/SosQ69aMvTKpp07d8pjjz0mWq1W3N3dJT4+XnX+ymNu3SIjI28be1FRkYSEhEjz5s3FzMxMXF1dZezYsZKTk6OMSUtLk4CAALG1tRULCwvx9vaWmJgYKS4uVs2Vnp4u3bp1E3Nzc2nZsqXMnz9f1T9x4kRp1aqVaLVacXR0lL59+8qhQ4eU/hs3bshzzz0nzs7OotVqpUWLFvLss8/K/v37VfMcOHBAQkJCpFmzZtK4cWN54okn5Mcff1SN6d69u8GcZGdnK2NcXV3r/DqcjIwM0el0kpubW6WvutdtdevWTT799FOD89X0uq09e/ZIkyZNpKioqMaY/o6v2+LG7f7eiIiIiO6V2r5uSyMiAqJ7bPDgwejUqROmT59+27GXL19GixYt8Pvvv1d5avrtDBkyBH5+fpgxY0atj8nPz4etrW2dzkNE9w5/bREREdG9Ulkb5OXl1fgspvviVnD651m4cCGsra1rNfbq1at4//3361xUl5aWwsfHR3XLPxERERER0d3GFetaSElJQZ8+fartLygouIfRUH3jijXR/Y2/toiIiOheqe2K9X37Huv7ib+//20fyEVERERERET/TCysa0Gn08HDw6OhwyAiIiIiIqL7EL9jTURERERERGQEFtZERERERERERmBhTURERERERGQEfseaqBq3e/IfERERERERwBVrIiIiIiIiIqOwsCYiIiIiIiIyAgtrIiIiIiIiIiOwsCYiIiIiIiIyAgtrIiIiIiIiIiOwsCYiIiIiIiIyAl+3RVQNW1vbhg6B7hERaegQiIiIiOgBxhVrIiIiIiIiIiOwsCYiIiIiIiIyAgtrIiIiIiIiIiOwsCYiIiIiIiIyAgtrIiIiIiIiIiOwsCYiIiIiIiIyAgtrIiIiIiIiIiOwsCYiIiIiIiIyAgtruqfi4uIQEhJyz863fPly9O/f/56dj4iIiIiI/nkavLDOycnBhAkT4OHhAQsLCzg6OiIoKAjLli1DUVGR0fMnJSWhU6dOMDc3h4eHB1auXKnqT05ORv/+/eHs7AyNRoNNmzbVaf5Ro0ZBo9Gott69e6vGtG7dusqY+fPnq8YcOXIETz75JCwsLODi4oJ33323yrkWL14MT09P6HQ6uLi4YNKkSSguLq7xPBqNBlFRUcqYTz75BD169ICNjQ00Gg1yc3OrnOfkyZMIDw+Hvb09bGxs0K1bN+zcuVM15vz58wgLC4OlpSUcHBwwZcoUlJWV1Zir4uJizJo1C7Nnz1a15+fn4+2334aXlxcsLCzg5OSE4OBgfPvttxAR1dinn34an332mbL/zTffoEePHrC1tYW1tTV8fX0xd+5cXL16FQAwevRoHDp0CCkpKTXGRkREREREdKcatLA+c+YMOnbsiJ9//hkxMTE4fPgwUlNTER0djS1btmD79u1GzZ+dnY2wsDA8/fTT0Ov1mDhxIsaMGYNt27YpYwoLC+Hn54ePP/74js/Tu3dvXLhwQdm+/PLLKmPmzp2rGvPGG28offn5+QgJCYGrqyvS0tKwcOFCzJkzB5988okyZu3atZg2bRpmz56NEydOIC4uDuvXr8eMGTOUMQcOHFCdIyEhAQAwePBgZUxRURF69+6tOu5W/fr1Q1lZGXbs2IG0tDT4+fmhX79+yMnJAQCUl5cjLCwMpaWl2LNnD7744gusXLkS//73v2vM09dffw0bGxsEBQUpbbm5uejatStWrVqF6dOn49ChQ0hOTsaQIUMQHR2NvLw8ZezVq1fxyy+/KCvQb7/9NoYMGYLHH38cW7duxbFjx/Dee+8hPT0d//nPfwAAWq0Ww4cPxwcffFBjbERERERERHdMGlBoaKg88sgjUlBQYLC/oqJC+RmALF++XMLCwkSn04mXl5fs2bNHTp06Jd27dxdLS0sJDAyUrKws5Zjo6Gjp0KGDas4hQ4ZIaGiowfMBkI0bN9bpGiIjIyU8PLzGMa6urrJo0aJq+5cuXSpNmzaVkpISpW3q1Kni6emp7EdFRckzzzyjOu7NN9+UoKCgauedMGGCtGnTRpXHSjt37hQAcu3aNVX7f//7XwEgycnJSlt+fr4AkISEBBER+fHHH6VRo0aSk5OjjFm2bJnY2NioruFWYWFh8tZbb6naXnvtNbGyspI//vijyvjr16/LzZs3lf1Vq1ZJQECAiIjs27dPAMjixYsNnuvv17Vr1y7RarVSVFRkcGxxcbHk5eUp22+//SYAuP2DNiIiIiIiQ/Ly8gSA5OXl1TiuwVasr1y5gp9//hlRUVGwsrIyOEaj0aj2582bh5EjR0Kv18PLywvDhw/HK6+8gunTp+PgwYMQEYwbN04Zn5qaiuDgYNUcoaGhSE1NvavXkpSUBAcHB3h6euK1117DlStXqoyZP38+7Ozs0LFjRyxcuFB123RqaiqeeuopaLVaVZyZmZm4du0aAKBr165IS0vD/v37Afy12v/jjz+ib9++BmMqLS3F6tWrMXr06Cp5rImdnR08PT2xatUqFBYWoqysDCtWrICDgwM6d+6sxOvj4wNHR0dVvPn5+cjIyKh27t27d8Pf31/Zr6iowLp16xAREQFnZ+cq462trWFqaqrsb968GeHh4QCANWvWwNraGq+//rrBczVp0kT52d/fH2VlZdi3b5/BsbGxsbC1tVU2FxeXaq+BiIiIiIjoVqa3H1I/srKyICLw9PRUtdvb2yvfG46KisKCBQuUvhdffBHPP/88AGDq1KkIDAzErFmzEBoaCgCYMGECXnzxRWV8Tk6OqvgDAEdHR+Tn5+PGjRvQ6XRGX0fv3r0xcOBAuLm54fTp05gxYwb69OmD1NRUmJiYAADGjx+PTp06oVmzZtizZw+mT5+OCxcu4P3331fidHNzqxJnZV/Tpk0xfPhwXL58Gd26dYOIoKysDK+++mq1t3Rv2rQJubm5GDVqVJ2uR6PRYPv27RgwYAAaN26MRo0awcHBAT/99BOaNm2qxGQor5V9huTm5iIvL09VQF++fBnXrl2Dl5fXbeMqKSnBTz/9hDlz5gAATp06BXd3d5iZmd32WEtLS9ja2uLcuXMG+6dPn44333xT2c/Pz2dxTUREREREtdZghXV19u/fj4qKCkRERKCkpETV5+vrq/xcWcj5+Pio2oqLi5Gfnw8bG5t7Eu/QoUOVn318fODr64s2bdogKSkJPXv2BABV0ebr6wutVotXXnkFsbGxMDc3r9V5kpKSEBMTg6VLlyIgIABZWVmYMGEC5s2bh1mzZlUZHxcXhz59+hhcCa6JiCAqKgoODg5ISUmBTqfDZ599hv79++PAgQNo0aJFneardOPGDQCAhYWF6ly1tWPHDjg4OKBDhw51PhYAdDpdtQ/DMzc3r/V/ByIiIiIiols12K3gHh4e0Gg0yMzMVLW7u7vDw8PD4Gry31cnK29vNtRWUVEBAHBycsLFixdVc1y8eBE2NjZ3ZbXaEHd3d9jb2yMrK6vaMQEBASgrK8PZs2drjLOyDwBmzZqFF154AWPGjIGPjw+ee+45xMTEIDY2VrneSufOncP27dsxZsyYOse/Y8cObNmyBevWrUNQUBA6deqEpUuXQqfT4Ysvvqh1vLeys7ODRqNRbm0HgObNm6NJkyb49ddfbxvX5s2b8eyzzyr77dq1w5kzZ3Dz5s1aXdfVq1fRvHnzWo0lIiIiIiKqiwYrrO3s7NCrVy989NFHKCwsrJdzBAYGIjExUdWWkJCAwMDAejkfAPz++++4cuVKjSu7er1eucW6Ms7k5GRVkZiQkABPT0/l9uuioiI0aqT+z1V5q/mtq7fx8fFwcHBAWFhYneOvXNW99VyNGjVSCvjAwEAcPXoUly5dUsVrY2OD9u3bG5xXq9Wiffv2OH78uGrOoUOHYs2aNfjzzz+rHFNQUICysjKICL7//nvl+9UAMHz4cBQUFGDp0qUGz/f314idPn0axcXF6Nix422unoiIiIiIqO4a9HVbS5cuRVlZGfz9/bF+/XqcOHECmZmZWL16NX799VelcLxTr776Ks6cOYPo6Gj8+uuvWLp0KTZs2IBJkyYpYwoKCqDX66HX6wH89YouvV6P8+fP33b+goICTJkyBXv37sXZs2eRmJiI8PBweHh4KN/7Tk1NxeLFi5Geno4zZ85gzZo1mDRpEkaMGKEUzcOHD4dWq8VLL72EjIwMrF+/HkuWLFHdQt6/f38sW7YM69atQ3Z2NhISEjBr1iz0799flaeKigrEx8cjMjJS9eCvSjk5OdDr9cqK+tGjR6HX65X3PgcGBqJp06aIjIxEeno6Tp48iSlTpiivLgOAkJAQtG/fHi+88ALS09Oxbds2zJw5E1FRUTXeUh0aGordu3er2t555x24uLggICAAq1atwvHjx3Hq1Cl8/vnn6NixIwoKCpCWloaioiJ069ZNOS4gIADR0dGYPHkyoqOjkZqainPnziExMRGDBw9WVtcBICUlBe7u7mjTps1t/5sSERERERHVWT0/nfy2/vzzTxk3bpy4ubmJmZmZWFtbS5cuXWThwoVSWFiojAPUr8LKzs4WAHL48GGlzdArpHbu3CmPPfaYaLVacXd3l/j4eNX5K4+5dYuMjLxt7EVFRRISEiLNmzcXMzMzcXV1lbFjx6peQ5WWliYBAQFia2srFhYW4u3tLTExMVJcXKyaKz09Xbp16ybm5ubSsmVLmT9/vqr/5s2bMmfOHGnTpo1YWFiIi4uLvP7661Vel7Vt2zYBIJmZmQZjnj17tsHr/XteDhw4ICEhIdKsWTNp3LixPPHEE/Ljjz+q5jl79qz06dNHdDqd2Nvby+TJk1WvxjIkIyNDdDqd5Obmqtpzc3Nl2rRp0rZtW9FqteLo6CjBwcGyceNGqaiokJkzZ0pERITBOdevXy9PPfWUNG7cWKysrMTX11fmzp2ryktISIjExsbWGNvfVT5Sn9s/ZyMiIiIiMqS2r9vSiNTxKVBERhg8eDA6deqE6dOn1/oYX19fzJw5U3kifF1kZGTgmWeewcmTJ2Fra1urY/Lz82s9lh4O/GOQiIiIiAyprA3y8vJqfEB2g94KTv88CxcuhLW1da3Hl5aWYtCgQejTp88dne/ChQtYtWoVC2UiIiIiIqo3XLGuQUpKSo0FXUFBwT2Mhu4Vrlj/8/CPQSIiIiIypLYr1vfde6zvJ/7+/spDzYiIiIiIiIgMYWFdA51OBw8Pj4YOg4iIiIiIiO5j/I41ERERERERkRFYWBMREREREREZgbeCE1Xjdg8oICIiIiIiArhiTURERERERGQUFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQEFtZERERERERERmBhTURERERERGQE04YOgOh+IyIAgPz8/AaOhIiIiIiIGlJlTVBZI1SHhTXRLa5cuQIAcHFxaeBIiIiIiIjofnD9+nXY2tpW28/CmugWzZo1AwCcP3++xv95qO7y8/Ph4uKC3377DTY2Ng0dzkOFua0/zG39YW7rD3Nbf5jb+sX81h/m9s6ICK5fvw5nZ+cax7GwJrpFo0Z/PXrA1taWf+jUExsbG+a2njC39Ye5rT/Mbf1hbusPc1u/mN/6w9zWXW0W2/jwMiIiIiIiIiIjsLAmIiIiIiIiMgILa6JbmJubY/bs2TA3N2/oUB46zG39YW7rD3Nbf5jb+sPc1h/mtn4xv/WHua1fGrndc8OJiIiIiIiIqFpcsSYiIiIiIiIyAgtrIiIiIiIiIiOwsCYiIiIiIiIyAgtrIiIiIiIiIiOwsCb6m48//hitW7eGhYUFAgICsH///oYO6YGUnJyM/v37w9nZGRqNBps2bVL1iwj+/e9/o0WLFtDpdAgODsapU6caJtgHSGxsLB5//HE0btwYDg4OGDBgADIzM1VjiouLERUVBTs7O1hbW2PQoEG4ePFiA0X84Fi2bBl8fX1hY2MDGxsbBAYGYuvWrUo/83r3zJ8/HxqNBhMnTlTamN87N2fOHGg0GtXm5eWl9DO3xvnjjz8wYsQI2NnZQafTwcfHBwcPHlT6+fvszrRu3brK51aj0SAqKgoAP7fGKC8vx6xZs+Dm5gadToc2bdpg3rx5+Pvzqvm5rR8srIn+z/r16/Hmm29i9uzZOHToEPz8/BAaGopLly41dGgPnMLCQvj5+eHjjz822P/uu+/igw8+wPLly7Fv3z5YWVkhNDQUxcXF9zjSB8uuXbsQFRWFvXv3IiEhATdv3kRISAgKCwuVMZMmTcL333+Pr776Crt27cKff/6JgQMHNmDUD4ZHHnkE8+fPR1paGg4ePIhnnnkG4eHhyMjIAMC83i0HDhzAihUr4Ovrq2pnfo3ToUMHXLhwQdl2796t9DG3d+7atWsICgqCmZkZtm7diuPHj+O9995D06ZNlTH8fXZnDhw4oPrMJiQkAAAGDx4MgJ9bYyxYsADLli3DRx99hBMnTmDBggV499138eGHHypj+LmtJ0JEIiLSpUsXiYqKUvbLy8vF2dlZYmNjGzCqBx8A2bhxo7JfUVEhTk5OsnDhQqUtNzdXzM3N5csvv2yACB9cly5dEgCya9cuEfkrj2ZmZvLVV18pY06cOCEAJDU1taHCfGA1bdpUPvvsM+b1Lrl+/bq0bdtWEhISpHv37jJhwgQR4efWWLNnzxY/Pz+DfcytcaZOnSrdunWrtp+/z+6eCRMmSJs2baSiooKfWyOFhYXJ6NGjVW0DBw6UiIgIEeHntj5xxZoIQGlpKdLS0hAcHKy0NWrUCMHBwUhNTW3AyB4+2dnZyMnJUeXa1tYWAQEBzHUd5eXlAQCaNWsGAEhLS8PNmzdVufXy8kKrVq2Y2zooLy/HunXrUFhYiMDAQOb1LomKikJYWJgqjwA/t3fDqVOn4OzsDHd3d0REROD8+fMAmFtjbd68Gf7+/hg8eDAcHBzQsWNHfPrpp0o/f5/dHaWlpVi9ejVGjx4NjUbDz62RunbtisTERJw8eRIAkJ6ejt27d6NPnz4A+LmtT6YNHQDR/eDy5csoLy+Ho6Ojqt3R0RG//vprA0X1cMrJyQEAg7mu7KPbq6iowMSJExEUFIRHH30UwF+51Wq1aNKkiWosc1s7R48eRWBgIIqLi2FtbY2NGzeiffv20Ov1zKuR1q1bh0OHDuHAgQNV+vi5NU5AQABWrlwJT09PXLhwAf/zP/+DJ598EseOHWNujXTmzBksW7YMb775JmbMmIEDBw5g/Pjx0Gq1iIyM5O+zu2TTpk3Izc3FqFGjAPDPBGNNmzYN+fn58PLygomJCcrLy/HOO+8gIiICAP8eVp9YWBMRPYCioqJw7Ngx1XcpyTienp7Q6/XIy8vD119/jcjISOzatauhw3rg/fbbb5gwYQISEhJgYWHR0OE8dCpXoQDA19cXAQEBcHV1xYYNG6DT6RowsgdfRUUF/P39ERMTAwDo2LEjjh07huXLlyMyMrKBo3t4xMXFoU+fPnB2dm7oUB4KGzZswJo1a7B27Vp06NABer0eEydOhLOzMz+39Yy3ghMBsLe3h4mJSZUnTl68eBFOTk4NFNXDqTKfzPWdGzduHLZs2YKdO3fikUceUdqdnJxQWlqK3Nxc1Xjmtna0Wi08PDzQuXNnxMbGws/PD0uWLGFejZSWloZLly6hU6dOMDU1hampKXbt2oUPPvgApqamcHR0ZH7voiZNmqBdu3bIysriZ9dILVq0QPv27VVt3t7eyq32/H1mvHPnzmH79u0YM2aM0sbPrXGmTJmCadOmYejQofDx8cELL7yASZMmITY2FgA/t/WJhTUR/voLdefOnZGYmKi0VVRUIDExEYGBgQ0Y2cPHzc0NTk5Oqlzn5+dj3759zPVtiAjGjRuHjRs3YseOHXBzc1P1d+7cGWZmZqrcZmZm4vz588ztHaioqEBJSQnzaqSePXvi6NGj0Ov1yubv74+IiAjlZ+b37ikoKMDp06fRokULfnaNFBQUVOWVhidPnoSrqysA/j67G+Lj4+Hg4ICwsDCljZ9b4xQVFaFRI3WJZ2JigoqKCgD83Narhn56GtH9Yt26dWJubi4rV66U48ePy8svvyxNmjSRnJychg7tgXP9+nU5fPiwHD58WADI+++/L4cPH5Zz586JiMj8+fOlSZMm8t1338mRI0ckPDxc3Nzc5MaNGw0c+f3ttddeE1tbW0lKSpILFy4oW1FRkTLm1VdflVatWsmOHTvk4MGDEhgYKIGBgQ0Y9YNh2rRpsmvXLsnOzpYjR47ItGnTRKPRyM8//ywizOvd9vengoswv8aYPHmyJCUlSXZ2tvzyyy8SHBws9vb2cunSJRFhbo2xf/9+MTU1lXfeeUdOnTola9asEUtLS1m9erUyhr/P7lx5ebm0atVKpk6dWqWPn9s7FxkZKS1btpQtW7ZIdna2fPvtt2Jvby/R0dHKGH5u6wcLa6K/+fDDD6VVq1ai1WqlS5cusnfv3oYO6YG0c+dOAVBli4yMFJG/XvUwa9YscXR0FHNzc+nZs6dkZmY2bNAPAEM5BSDx8fHKmBs3bsjrr78uTZs2FUtLS3nuuefkwoULDRf0A2L06NHi6uoqWq1WmjdvLj179lSKahHm9W67tbBmfu/ckCFDpEWLFqLVaqVly5YyZMgQycrKUvqZW+N8//338uijj4q5ubl4eXnJJ598ourn77M7t23bNgFgMF/83N65/Px8mTBhgrRq1UosLCzE3d1d3n77bSkpKVHG8HNbPzQiIg2yVE5ERERERET0EOB3rImIiIiIiIiMwMKaiIiIiIiIyAgsrImIiIiIiIiMwMKaiIiIiIiIyAgsrImIiIiIiIiMwMKaiIiIiIiIyAgsrImIiIiIiIiMwMKaiIiIiIiIyAgsrImIiMigpKQkaDQa5Obm3hfz0P+XmJgIb29vlJeXN3QoVTzxxBP45ptvGjoMIqJ7ioU1ERHRQ2jUqFHQaDTQaDQwMzODm5sboqOjUVxcXK/n7dGjByZOnKhq69q1Ky5cuABbW9t6O+/Zs2eV6/37NmLEiFodv3HjRjzxxBOwtbVF48aN0aFDhyrXcT+Jjo7GzJkzYWJiorSVlpZi4cKF6NSpE6ysrGBraws/Pz/MnDkTf/75Z5U5UlNTYWJigrCwsCp9lfnU6/WqfQcHB1y/fl019rHHHsOcOXOU/ZkzZ2LatGmoqKi4OxdLRPQAYGFNRET0kOrduzcuXLiAM2fOYNGiRVixYgVmz559z+PQarVwcnKCRqOp93Nt374dFy5cULaPP/74tsckJiZiyJAhGDRoEPbv34+0tDS88847uHnzZr3FWV5efseF5+7du3H69GkMGjRIaSspKUGvXr0QExODUaNGITk5GUePHsUHH3yAy5cv48MPP6wyT1xcHN544w0kJycbLLwNuX79Ov73f/+3xjF9+vTB9evXsXXr1rpdGBHRA4yFNRER0UPK3NwcTk5OcHFxwYABAxAcHIyEhASlv6KiArGxsXBzc4NOp4Ofnx++/vrraue7cuUKhg0bhpYtW8LS0hI+Pj748ssvlf5Ro0Zh165dWLJkibJifPbsWdWt4Pn5+dDpdFWKro0bN6Jx48YoKioCAPz22294/vnn0aRJEzRr1gzh4eE4e/bsba/Zzs4OTk5OylabVfLvv/8eQUFBmDJlCjw9PdGuXTsMGDCgSlH+/fff4/HHH4eFhQXs7e3x3HPPKX3Xrl3DyJEj0bRpU1haWqJPnz44deqU0r9y5Uo0adIEmzdvRvv27WFubo7z58+jpKQEb731Flq2bAkrKysEBAQgKSmpxnjXrVuHXr16wcLCQmlbtGgRdu/ejR07dmD8+PHo3LkzWrVqhe7du2P58uWIiYlRzVFQUID169fjtddeQ1hYGFauXHnbPAHAG2+8gffffx+XLl2qdoyJiQn69u2LdevW1WpOIqKHAQtrIiKif4Bjx45hz5490Gq1SltsbCxWrVqF5cuXIyMjA5MmTcKIESOwa9cug3MUFxejc+fO+OGHH3Ds2DG8/PLLeOGFF7B//34AwJIlSxAYGIixY8cqK8YuLi6qOWxsbNCvXz+sXbtW1b5mzRoMGDAAlpaWuHnzJkJDQ9G4cWOkpKTgl19+gbW1NXr37o3S0tK7nBnAyckJGRkZOHbsWLVjfvjhBzz33HPo27cvDh8+jMTERHTp0kXpHzVqFA4ePIjNmzcjNTUVIoK+ffuqVr2LioqwYMECfPbZZ8jIyICDgwPGjRuH1NRUrFu3DkeOHMHgwYPRu3dvVVF+q5SUFPj7+6vavvzyS/Tq1QsdO3Y0eMytdwts2LABXl5e8PT0xIgRI/D5559DRGrMEwAMGzYMHh4emDt3bo3junTpgpSUlNvOR0T00BAiIiJ66ERGRoqJiYlYWVmJubm5AJBGjRrJ119/LSIixcXFYmlpKXv27FEd99JLL8mwYcNERGTnzp0CQK5du1btecLCwmTy5MnKfvfu3WXChAmqMbfOs3HjRrG2tpbCwkIREcnLyxMLCwvZunWriIj85z//EU9PT6moqFDmKCkpEZ1OJ9u2bTMYR3Z2tgAQnU4nVlZWynbo0KHb5qqgoED69u0rAMTV1VWGDBkicXFxUlxcrIwJDAyUiIgIg8efPHlSAMgvv/yitF2+fFl0Op1s2LBBRETi4+MFgOj1emXMuXPnxMTERP744w/VfD179pTp06dXG6+tra2sWrVK1WZhYSHjx49XtQ0YMEDJQ2BgoKqva9eusnjxYhERuXnzptjb28vOnTuV/sp8Hj58uMr+Tz/9JGZmZpKVlSUiIn5+fjJ79mzV/N999500atRIysvLq70OIqKHiWmDVfRERERUr55++mksW7YMhYWFWLRoEUxNTZXv5WZlZaGoqAi9evVSHVNaWlrtqmd5eTliYmKwYcMG/PHHHygtLUVJSQksLS3rFFffvn1hZmaGzZs3Y+jQofjmm29gY2OD4OBgAEB6ejqysrLQuHFj1XHFxcU4ffp0jXOvX78e3t7eyv6tK+aGWFlZ4YcffsDp06exc+dO7N27F5MnT8aSJUuQmpoKS0tL6PV6jB071uDxJ06cgKmpKQICApQ2Ozs7eHp64sSJE0qbVquFr6+vsn/06FGUl5ejXbt2qvlKSkpgZ2dXbbw3btxQ3QZenaVLl6KwsBAffPABkpOTlfbMzEzs378fGzduBACYmppiyJAhiIuLQ48ePW47b2hoKLp164ZZs2ZVufOgkk6nQ0VFBUpKSqDT6W47JxHRg46FNRER0UPKysoKHh4eAIDPP/8cfn5+iIuLw0svvYSCggIAf93i3LJlS9Vx5ubmBudbuHAhlixZgsWLF8PHxwdWVlaYOHFinW/P1mq1+Ne//oW1a9di6NChWLt2LYYMGQJT07/+WlJQUIDOnTtjzZo1VY5t3rx5jXO7uLgo11xXbdq0QZs2bTBmzBi8/fbbaNeuHdavX48XX3zxrhSHOp1OdUt2QUEBTExMkJaWpnq6NwBYW1tXO4+9vT2uXbumamvbti0yMzNVbS1atAAANGvWTNUeFxeHsrIyODs7K20iAnNzc3z00Ue1+l76/PnzERgYiClTphjsv3r1KqysrFhUE9E/Br9jTURE9A/QqFEjzJgxAzNnzsSNGzdUD9Dy8PBQbdWt8v7yyy8IDw/HiBEj4OfnB3d3d5w8eVI1RqvV1urdyhEREfjpp5+QkZGBHTt2ICIiQunr1KkTTp06BQcHhyqx1ecru/6udevWsLS0RGFhIQDA19cXiYmJBsd6e3ujrKwM+/btU9quXLmCzMxMtG/fvtpzdOzYEeXl5bh06VKV63RycqrxuOPHj6vahg0bhoSEBBw+fLjG6yorK8OqVavw3nvvQa/XK1t6ejqcnZ1VD6OrSZcuXTBw4EBMmzbNYP+xY8eqvfOBiOhhxMKaiIjoH2Lw4MEwMTHBxx9/jMaNG+Ott97CpEmT8MUXX+D06dM4dOgQPvzwQ3zxxRcGj2/bti0SEhKwZ88enDhxAq+88gouXryoGtO6dWvs27cPZ8+exeXLl6t9pdRTTz0FJycnREREwM3NTXUbdUREBOzt7REeHo6UlBRkZ2cjKSkJ48ePx++//373EvJ/5syZg+joaCQlJSE7OxuHDx/G6NGjcfPmTeVW+dmzZ+PLL7/E7NmzceLECRw9ehQLFixQ8hIeHo6xY8di9+7dSE9Px4gRI9CyZUuEh4dXe9527dohIiICI0eOxLfffovs7Gzs378fsbGx+OGHH6o9LjQ0FLt371a1TZo0CYGBgejZsyeWLFmCQ4cOITs7G9u2bcPWrVuVFfEtW7bg2rVreOmll/Doo4+qtkGDBiEuLq7WeXvnnXewY8eOKivlwF8PWAsJCan1XEREDzoW1kRERP8QpqamGDduHN59910UFhZi3rx5mDVrFmJjY+Ht7Y3evXvjhx9+gJubm8HjZ86ciU6dOiE0NBQ9evSAk5MTBgwYoBrz1ltvwcTEBO3bt0fz5s1x/vx5g3NpNBoMGzYM6enpqtVqALC0tERycjJatWqFgQMHwtvbGy+99BKKi4thY2NzV3Lxd927d8eZM2cwcuRIeHl5oU+fPsjJycHPP/8MT09PAECPHj3w1VdfYfPmzXjsscfwzDPPKE9DB4D4+Hh07twZ/fr1Q2BgIEQEP/74I8zMzGo8d3x8PEaOHInJkyfD09MTAwYMwIEDB9CqVatqj4mIiEBGRoaqoLWwsEBiYiKmTp2K+Ph4dOvWDd7e3pg4cSKCgoKwadMmAH/dBh4cHGxw5X/QoEE4ePAgjhw5Uqu8tWvXDqNHj0ZxcbGq/Y8//sCePXvw4osv1moeIqKHgUakFu9WICIiIqL7xpQpU5Cfn48VK1Y0dChVTJ06FdeuXcMnn3zS0KEQEd0zXLEmIiIiesC8/fbbcHV1rfZW+4bk4OCAefPmNXQYRET3FFesiYiI6KH26quvYvXq1Qb7RowYgeXLl9/jiIiI6GHDwpqIiIgeapcuXUJ+fr7BPhsbGzg4ONzjiIiI6GHDwpqIiIiIiIjICPyONREREREREZERWFgTERERERERGYGFNREREREREZERWFgTERERERERGYGFNREREREREZERWFgTERERERERGYGFNREREREREZER/h91dwwrdB+sEwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#generate figure object\n",
    "figure(num=None, figsize=(10, 10), dpi=100, facecolor='w', edgecolor='k')\n",
    "#load in the 20 lardest values and their SNP label\n",
    "indexes = df.nlargest(20, \"F_Score(GAIN)\").index\n",
    "values = df.nlargest(20, \"F_Score(GAIN)\").values.ravel()\n",
    "#reverse to make the largest be at the front\n",
    "indexes = indexes[::-1]\n",
    "values = values[::-1]\n",
    "#for each different chromosome you want to colour add a index(*_i) and value (*_v) array\n",
    "#black would be colour for singular/notinteresting chromosomes\n",
    "r_i = []\n",
    "r_v = []\n",
    "b_i = []\n",
    "b_v = []\n",
    "g_i = []\n",
    "g_v = []\n",
    "y_i = []\n",
    "y_v = []\n",
    "bl_i = []\n",
    "bl_v = []\n",
    "p_i = []\n",
    "p_v = []\n",
    "br_i = []\n",
    "br_v = []\n",
    "pu_i = []\n",
    "pu_v = []\n",
    "#for each value in the top n (default 20) check which chromosome it belongs to and add it to the colour array\n",
    "i = 0\n",
    "while i < len(indexes):\n",
    "    if('Gm03' in indexes[i]):\n",
    "        r_i.append(indexes[i])\n",
    "        r_v.append(values[i])\n",
    "    elif('Gm19' in indexes[i]):\n",
    "        b_i.append(indexes[i])\n",
    "        b_v.append(values[i])\n",
    "    elif('Gm05' in indexes[i]):\n",
    "        g_i.append(indexes[i])\n",
    "        g_v.append(values[i])\n",
    "    elif('Gm02' in indexes[i]):\n",
    "        y_i.append(indexes[i])\n",
    "        y_v.append(values[i])\n",
    "    elif('Gm07' in indexes[i]):\n",
    "        p_i.append(indexes[i])\n",
    "        p_v.append(values[i])\n",
    "   # elif('Gm04' in indexes[i]):\n",
    "   #     br_i.append(indexes[i])\n",
    "   #     br_v.append(values[i])\n",
    "   # elif('Gm13' in indexes[i]):\n",
    "   #     pu_i.append(indexes[i])\n",
    "   #     pu_v.append(values[i])\n",
    "    else:\n",
    "        bl_i.append(indexes[i])\n",
    "        bl_v.append(values[i])\n",
    "    i = i + 1\n",
    "#plot each of the arrays with appropriate colour and label graph\n",
    "plt.barh(bl_i, bl_v, color=\"black\")\n",
    "plt.barh(br_i, br_v, color=\"brown\")\n",
    "plt.barh(pu_i, pu_v, color=\"purple\")\n",
    "plt.barh(y_i, y_v, color=\"yellow\")\n",
    "plt.barh(p_i, p_v, color=\"orange\")\n",
    "plt.barh(g_i, g_v, color=\"green\")\n",
    "plt.barh(r_i, r_v, color=\"red\")\n",
    "plt.barh(b_i, b_v, color=\"blue\")\n",
    "plt.title('SNP Importance XGBoost Pod Colour')\n",
    "plt.ylabel('SNP Label')\n",
    "plt.xlabel('Relative F_Score (GAIN)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "(617,)\n",
      "(617, 1)\n",
      "60000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "(155,)\n",
      "(155, 1)\n",
      "60000\n",
      "(617, 58574)\n",
      "(155, 58574)\n",
      "(617, 1)\n",
      "0.0\n",
      "(617, 1)\n",
      "(155, 1)\n"
     ]
    }
   ],
   "source": [
    "tt_vcf, ho_vcf, tt_pheno, ho_pheno = new_prep_data(\"PoC_Merged_filtered.csv_train_testQTL_SNPS.csv\", \"PoC_Merged_filtered.csv_holdoutQTL_SNPS.csv\")\n",
    "r_t = tt_pheno.ravel()\n",
    "r_h = ho_pheno.ravel()\n",
    "print(r_t[10])\n",
    "i = 0\n",
    "for x in r_t:\n",
    "    if(x==0.5):\n",
    "        r_t[i]=2.0\n",
    "    i = i+1\n",
    "i = 0\n",
    "for x in r_h:\n",
    "    if(x==0.5):\n",
    "        r_h[i]=2.0\n",
    "    i = i+1\n",
    "r_t = np.reshape(r_t,(len(r_t),1))\n",
    "r_h = np.reshape(r_h,(len(r_h),1))\n",
    "tt_pheno = r_t\n",
    "ho_pheno = r_h\n",
    "print(tt_pheno.shape)\n",
    "print(ho_pheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if need or have new holdout data etc.\n",
    "ohe = pickle.load(open(\"PoC_QTL_ohe.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 58574)\n",
      "(617, 170396)\n",
      "(155, 58574)\n",
      "(155, 170396)\n"
     ]
    }
   ],
   "source": [
    "print(tt_vcf.shape)\n",
    "tt_vcf = ohe.transform(tt_vcf)\n",
    "print(tt_vcf.shape)\n",
    "print(ho_vcf.shape)\n",
    "ho_vcf = ohe.transform(ho_vcf)\n",
    "print(ho_vcf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 170396)\n",
      "(617, 1)\n",
      "(155, 170396)\n",
      "(155, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is88.88888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is95.23809523809523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is93.65079365079364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is87.3015873015873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is90.32258064516128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is90.1639344262295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is91.80327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is80.32786885245902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is73.33333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is70.0\n",
      "81.29032258064515\n",
      "Training Testing Accuracy: 86.10% (8.20%)\n",
      "Holdout Accuracy: 81.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(tt_vcf.shape)\n",
    "print(tt_pheno.shape)\n",
    "print(ho_vcf.shape)\n",
    "print(ho_pheno.shape)\n",
    "seed = randint(0,5000)\n",
    " #if optimised in same session, other enter manually below\n",
    "#this function should average out 10 folds and training, with inital params optimised\n",
    "#average accuracy and std should be calculated along with a nice AUROC graph of train/test models\n",
    "#best model should be extracted for use on holdout set\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=seed, max_features = 'sqrt',n_jobs=1, verbose = 1)\n",
    "best_model = eval_k_fold(model, tt_vcf, tt_pheno, 10, ho_vcf, ho_pheno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (based off primer paper and Philipp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "(617,)\n",
      "(617, 1)\n",
      "60000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "(155,)\n",
      "(155, 1)\n",
      "60000\n",
      "(617, 58574)\n",
      "(155, 58574)\n",
      "(617, 1)\n",
      "0.0\n",
      "(617, 1)\n",
      "(155, 1)\n"
     ]
    }
   ],
   "source": [
    "tt_vcf, ho_vcf, tt_pheno, ho_pheno = new_prep_data(\"PoC_Merged_filtered.csv_train_testQTL_SNPS.csv\", \"PoC_Merged_filtered.csv_holdoutQTL_SNPS.csv\")\n",
    "r_t = tt_pheno.ravel()\n",
    "r_h = ho_pheno.ravel()\n",
    "print(r_t[10])\n",
    "i = 0\n",
    "for x in r_t:\n",
    "    if(x==0.5):\n",
    "        r_t[i]=2.0\n",
    "    i = i+1\n",
    "i = 0\n",
    "for x in r_h:\n",
    "    if(x==0.5):\n",
    "        r_h[i]=2.0\n",
    "    i = i+1\n",
    "r_t = np.reshape(r_t,(len(r_t),1))\n",
    "r_h = np.reshape(r_h,(len(r_h),1))\n",
    "tt_pheno = r_t\n",
    "ho_pheno = r_h\n",
    "print(tt_pheno.shape)\n",
    "print(ho_pheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 170396)\n",
      "(155, 58574)\n",
      "(155, 170396)\n"
     ]
    }
   ],
   "source": [
    "ohe = pickle.load(open(\"PoC_QTL_ohe.dat\", \"rb\"))\n",
    "tt_vcf = ohe.transform(tt_vcf)\n",
    "print(tt_vcf.shape)\n",
    "print(ho_vcf.shape)\n",
    "ho_vcf = ohe.transform(ho_vcf)\n",
    "print(ho_vcf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##how to mlb both tt and ho for same scheme? do i even need to?\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb = mlb.fit(tt_pheno)\n",
    "##print(tt_pheno.shape)\n",
    "#print(ho_pheno.shape)\n",
    "#tt_pheno = mlb.transform(tt_pheno)\n",
    "#print(tt_pheno.shape)\n",
    "#ho_pheno = mlb.transform(ho_pheno)\n",
    "#print(ho_pheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN_model(x_len):    \n",
    "    #del model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=10, kernel_size=10, \n",
    "                     input_shape=(x_len, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(filters=8, kernel_size=8, \n",
    "                     input_shape=(10, 1)))\n",
    "    model.add(Activation('linear'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv1D(filters=6, kernel_size=6, \n",
    "                     input_shape=(8, 1)))\n",
    "    model.add(Activation('linear'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(24, activation='linear'))\n",
    "    model.add(Dense(16, activation='linear'))\n",
    "    model.add(Dense(8, activation='linear'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    opt = tf.keras.optimizers.Adamax(learning_rate=0.003)#, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cnn(x,y,k,mlb):\n",
    "    cv = StratifiedKFold(n_splits=k,shuffle=False)\n",
    "    best_model = []\n",
    "    results = []\n",
    "    highest = 0\n",
    "    i = 1\n",
    "    for train,test in cv.split(x,y):\n",
    "        print(y.shape)\n",
    "        print(y[train])\n",
    "        if(i==1):\n",
    "            y = mlb.transform(y)\n",
    "            print(y.shape)\n",
    "            print(y[train])\n",
    "        x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "        model = build_CNN_model(x[train].shape[1])\n",
    "        bs = ((x[train].shape[0])/20)\n",
    "        bs = round(bs)\n",
    "        history = model.fit(x[train], y[train], validation_data=(x[test], y[test]), epochs=100, batch_size=bs)\n",
    "        _, accuracy = model.evaluate(x[test], y[test], batch_size=bs, verbose=0)\n",
    "        accuracy = accuracy *100\n",
    "        print(\"accuracy for model \" + str(i) + \" is \" + str(accuracy))\n",
    "        if(accuracy > highest):\n",
    "            highest = accuracy\n",
    "            best_model = model\n",
    "        results.append(accuracy)\n",
    "        del model\n",
    "        i = i + 1\n",
    "    print(\"Training Testing Accuracy: %.2f%% (%.2f%%)\" % (np.mean(results), np.std(results))) \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 1)\n",
      "[[1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]]\n",
      "(617, 3)\n",
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 170387, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 170380, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 170375, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 170375, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 85187, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 85187, 6)          24        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 511122)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                12266952  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 12,268,623\n",
      "Trainable params: 12,268,595\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 15s 28ms/sample - loss: 0.8891 - accuracy: 0.6083 - val_loss: 0.7587 - val_accuracy: 0.6825\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7465 - accuracy: 0.6986 - val_loss: 1.4285 - val_accuracy: 0.1746\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7534 - accuracy: 0.6841 - val_loss: 2.1039 - val_accuracy: 0.1746\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7291 - accuracy: 0.6787 - val_loss: 2.5883 - val_accuracy: 0.1746\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6997 - accuracy: 0.7148 - val_loss: 2.8552 - val_accuracy: 0.1746\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6991 - accuracy: 0.7383 - val_loss: 3.1602 - val_accuracy: 0.1746\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.5974 - accuracy: 0.7906 - val_loss: 3.2473 - val_accuracy: 0.1746\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.5429 - accuracy: 0.8213 - val_loss: 3.3503 - val_accuracy: 0.1746\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.4664 - accuracy: 0.8321 - val_loss: 3.5675 - val_accuracy: 0.1746\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3894 - accuracy: 0.8827 - val_loss: 3.5030 - val_accuracy: 0.1746\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3247 - accuracy: 0.9097 - val_loss: 3.4077 - val_accuracy: 0.1746\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.2398 - accuracy: 0.9495 - val_loss: 2.9975 - val_accuracy: 0.1746\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.2129 - accuracy: 0.9621 - val_loss: 2.8887 - val_accuracy: 0.1746\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1697 - accuracy: 0.9729 - val_loss: 2.8275 - val_accuracy: 0.1746\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1334 - accuracy: 0.9783 - val_loss: 2.5885 - val_accuracy: 0.1746\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0987 - accuracy: 0.9910 - val_loss: 2.6283 - val_accuracy: 0.1746\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0944 - accuracy: 0.9892 - val_loss: 1.6928 - val_accuracy: 0.1746\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0847 - accuracy: 0.9892 - val_loss: 1.5458 - val_accuracy: 0.1746\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0807 - accuracy: 0.9910 - val_loss: 1.3906 - val_accuracy: 0.1746\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0899 - accuracy: 0.9856 - val_loss: 1.2371 - val_accuracy: 0.3175\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0782 - accuracy: 0.9910 - val_loss: 0.6389 - val_accuracy: 0.8571\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0528 - accuracy: 0.9982 - val_loss: 0.9566 - val_accuracy: 0.5238\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0422 - accuracy: 0.9982 - val_loss: 0.8930 - val_accuracy: 0.5079\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0445 - accuracy: 0.9982 - val_loss: 0.9146 - val_accuracy: 0.5714\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0314 - accuracy: 0.9982 - val_loss: 0.6909 - val_accuracy: 0.8095\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0295 - accuracy: 1.0000 - val_loss: 0.5975 - val_accuracy: 0.8254\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.5794 - val_accuracy: 0.8571\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.5416 - val_accuracy: 0.8571\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0251 - accuracy: 0.9982 - val_loss: 0.5496 - val_accuracy: 0.9048\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0311 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 0.8889\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.5564 - val_accuracy: 0.8889\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.5191 - val_accuracy: 0.8889\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.5942 - val_accuracy: 0.8889\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.5659 - val_accuracy: 0.8730\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.5582 - val_accuracy: 0.8730\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.7160 - val_accuracy: 0.8571\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.5605 - val_accuracy: 0.8730\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0347 - accuracy: 0.9892 - val_loss: 0.6664 - val_accuracy: 0.8095\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.6329 - val_accuracy: 0.8413\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.6598 - val_accuracy: 0.8571\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.6303 - val_accuracy: 0.8889\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.6451 - val_accuracy: 0.8571\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.6028 - val_accuracy: 0.8730\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.5943 - val_accuracy: 0.8730\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.6805 - val_accuracy: 0.9048\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.6124 - val_accuracy: 0.9048\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.5987 - val_accuracy: 0.8730\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.6723 - val_accuracy: 0.8730\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.6831 - val_accuracy: 0.8413\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.8332 - val_accuracy: 0.8730\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.9205 - val_accuracy: 0.8254\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.7488 - val_accuracy: 0.8889\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0278 - accuracy: 0.9964 - val_loss: 1.1964 - val_accuracy: 0.7619\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0205 - accuracy: 0.9982 - val_loss: 0.8944 - val_accuracy: 0.8254\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.8061 - val_accuracy: 0.8571\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0106 - accuracy: 1.0000 - val_loss: 1.0159 - val_accuracy: 0.8254\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.6855 - val_accuracy: 0.8571\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.8331 - val_accuracy: 0.8254\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0141 - accuracy: 0.9982 - val_loss: 1.0050 - val_accuracy: 0.8413\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.7817 - val_accuracy: 0.8730\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0096 - accuracy: 0.9982 - val_loss: 0.7457 - val_accuracy: 0.8571\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.9140 - val_accuracy: 0.8571\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.7917 - val_accuracy: 0.8413\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.7345 - val_accuracy: 0.8730\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.7051 - val_accuracy: 0.8571\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.8201 - val_accuracy: 0.8730\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.9598 - val_accuracy: 0.8571\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.9845 - val_accuracy: 0.8413\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0100 - accuracy: 1.0000 - val_loss: 1.1930 - val_accuracy: 0.8254\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.8960 - val_accuracy: 0.8730\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.8273 - val_accuracy: 0.8730\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.8415 - val_accuracy: 0.8571\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.7816 - val_accuracy: 0.8730\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.8144 - val_accuracy: 0.8571\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.8143 - val_accuracy: 0.8571\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.7759 - val_accuracy: 0.8730\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.8687 - val_accuracy: 0.8571\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.8870 - val_accuracy: 0.8730\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.8449 - val_accuracy: 0.8730\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8597 - val_accuracy: 0.8571\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8866 - val_accuracy: 0.8571\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.8654 - val_accuracy: 0.8571\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.9908 - val_accuracy: 0.8571\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.7436 - val_accuracy: 0.8730\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.9419 - val_accuracy: 0.8571\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.8540 - val_accuracy: 0.8889\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.9302 - val_accuracy: 0.8571\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.9153 - val_accuracy: 0.8571\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.9185 - val_accuracy: 0.8413\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8356 - val_accuracy: 0.8730\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8706 - val_accuracy: 0.8571\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9082 - val_accuracy: 0.8571\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.9113 - val_accuracy: 0.8730\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8674 - val_accuracy: 0.8571\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.9371 - val_accuracy: 0.8571\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.8751 - val_accuracy: 0.8571\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8909 - val_accuracy: 0.8571\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.9016 - val_accuracy: 0.8571\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.8953 - val_accuracy: 0.8571\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.8331 - val_accuracy: 0.8889\n",
      "accuracy for model 1 is 88.88888955116272\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 170387, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 170380, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 170375, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 170375, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 85187, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 85187, 6)          24        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 511122)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                12266952  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 12,268,623\n",
      "Trainable params: 12,268,595\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 2s 4ms/sample - loss: 0.8587 - accuracy: 0.6336 - val_loss: 0.7097 - val_accuracy: 0.7302\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.7349 - accuracy: 0.6841 - val_loss: 1.0197 - val_accuracy: 0.5397\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7435 - accuracy: 0.6841 - val_loss: 1.3781 - val_accuracy: 0.3810\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.6820 - accuracy: 0.7076 - val_loss: 1.5733 - val_accuracy: 0.2063\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.6770 - accuracy: 0.7329 - val_loss: 1.7771 - val_accuracy: 0.1746\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.5930 - accuracy: 0.7726 - val_loss: 2.0759 - val_accuracy: 0.1746\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.5870 - accuracy: 0.7708 - val_loss: 2.4006 - val_accuracy: 0.1746\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.4773 - accuracy: 0.8394 - val_loss: 2.3878 - val_accuracy: 0.1746\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3853 - accuracy: 0.8556 - val_loss: 2.1668 - val_accuracy: 0.1746\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.2625 - accuracy: 0.9224 - val_loss: 2.2144 - val_accuracy: 0.1746\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.1923 - accuracy: 0.9675 - val_loss: 2.1811 - val_accuracy: 0.1746\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1706 - accuracy: 0.9603 - val_loss: 1.9184 - val_accuracy: 0.1746\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.1654 - accuracy: 0.9639 - val_loss: 1.6050 - val_accuracy: 0.1746\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.1220 - accuracy: 0.9856 - val_loss: 1.6356 - val_accuracy: 0.2063\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.1361 - accuracy: 0.9747 - val_loss: 1.6114 - val_accuracy: 0.1905\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.1093 - accuracy: 0.9765 - val_loss: 1.1331 - val_accuracy: 0.2540\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0815 - accuracy: 0.9928 - val_loss: 1.1340 - val_accuracy: 0.3175\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0739 - accuracy: 0.9946 - val_loss: 1.0959 - val_accuracy: 0.2698\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0489 - accuracy: 0.9964 - val_loss: 0.9577 - val_accuracy: 0.4921\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0436 - accuracy: 0.9964 - val_loss: 0.9502 - val_accuracy: 0.4762\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0584 - accuracy: 0.9964 - val_loss: 0.9035 - val_accuracy: 0.6349\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0436 - accuracy: 1.0000 - val_loss: 0.9145 - val_accuracy: 0.6190\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.4841 - val_accuracy: 0.8095\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.6025 - val_accuracy: 0.7937\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.5499 - val_accuracy: 0.8571\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0277 - accuracy: 0.9982 - val_loss: 0.5576 - val_accuracy: 0.7937\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0323 - accuracy: 0.9982 - val_loss: 0.4993 - val_accuracy: 0.8254\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0318 - accuracy: 0.9982 - val_loss: 0.7122 - val_accuracy: 0.7619\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.4455 - val_accuracy: 0.8254\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.5209 - val_accuracy: 0.8095\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.5036 - val_accuracy: 0.8254\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0218 - accuracy: 0.9982 - val_loss: 0.5433 - val_accuracy: 0.7937\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0239 - accuracy: 0.9982 - val_loss: 0.4977 - val_accuracy: 0.8413\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.7196 - val_accuracy: 0.7619\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0328 - accuracy: 0.9946 - val_loss: 0.5238 - val_accuracy: 0.8095\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0240 - accuracy: 0.9982 - val_loss: 0.4065 - val_accuracy: 0.8413\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.5544 - val_accuracy: 0.8095\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.6262 - val_accuracy: 0.8413\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.4851 - val_accuracy: 0.8095\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0230 - accuracy: 0.9946 - val_loss: 0.4607 - val_accuracy: 0.8095\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0340 - accuracy: 0.9946 - val_loss: 0.6224 - val_accuracy: 0.7937\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0318 - accuracy: 0.9982 - val_loss: 0.4511 - val_accuracy: 0.8254\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.5247 - val_accuracy: 0.8254\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.5100 - val_accuracy: 0.7937\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.4749 - val_accuracy: 0.8730\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.4730 - val_accuracy: 0.8413\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.4132 - val_accuracy: 0.8254\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.5504 - val_accuracy: 0.8095\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0126 - accuracy: 0.9982 - val_loss: 0.4807 - val_accuracy: 0.8095\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.4285 - val_accuracy: 0.8571\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.4490 - val_accuracy: 0.8571\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.4534 - val_accuracy: 0.8413\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.4163 - val_accuracy: 0.8413\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.4263 - val_accuracy: 0.7937\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.5417 - val_accuracy: 0.8730\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.5385 - val_accuracy: 0.8254\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.4492 - val_accuracy: 0.8413\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.4837 - val_accuracy: 0.8095\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5092 - val_accuracy: 0.7937\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.4383 - val_accuracy: 0.8254\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.4902 - val_accuracy: 0.8095\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.4618 - val_accuracy: 0.8254\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.4189 - val_accuracy: 0.8254\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.4771 - val_accuracy: 0.8254\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.4626 - val_accuracy: 0.8254\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.4491 - val_accuracy: 0.8254\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4621 - val_accuracy: 0.8413\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.5073 - val_accuracy: 0.8095\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.4767 - val_accuracy: 0.8571\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.4924 - val_accuracy: 0.8889\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4940 - val_accuracy: 0.8571\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.4682 - val_accuracy: 0.8095\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.6158 - val_accuracy: 0.7778\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 1.0342 - val_accuracy: 0.7460\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5376 - val_accuracy: 0.8413\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.4992 - val_accuracy: 0.8413\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0084 - accuracy: 0.9982 - val_loss: 0.6912 - val_accuracy: 0.7778\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.9613 - val_accuracy: 0.8095\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0173 - accuracy: 0.9964 - val_loss: 0.7089 - val_accuracy: 0.7937\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0280 - accuracy: 0.9910 - val_loss: 1.4486 - val_accuracy: 0.7619\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0415 - accuracy: 0.9874 - val_loss: 1.5842 - val_accuracy: 0.7778\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0547 - accuracy: 0.9819 - val_loss: 0.4936 - val_accuracy: 0.8095\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0382 - accuracy: 0.9892 - val_loss: 0.8904 - val_accuracy: 0.7619\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0413 - accuracy: 0.9892 - val_loss: 2.6291 - val_accuracy: 0.7302\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0323 - accuracy: 0.9892 - val_loss: 0.7726 - val_accuracy: 0.7778\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0206 - accuracy: 0.9982 - val_loss: 0.6721 - val_accuracy: 0.8571\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.5657 - val_accuracy: 0.8413\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.5240 - val_accuracy: 0.8254\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.7153 - val_accuracy: 0.8095\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.5311 - val_accuracy: 0.8254\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.4762 - val_accuracy: 0.8095\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.5458 - val_accuracy: 0.8095\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5323 - val_accuracy: 0.8254\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 0.8254\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.5403 - val_accuracy: 0.8254\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5592 - val_accuracy: 0.8254\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5850 - val_accuracy: 0.8095\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.5653 - val_accuracy: 0.8254\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5349 - val_accuracy: 0.8413\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5708 - val_accuracy: 0.8254\n",
      "accuracy for model 2 is 82.53968358039856\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 170387, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 170380, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 170375, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 170375, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 85187, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 85187, 6)          24        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 511122)            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 24)                12266952  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 12,268,623\n",
      "Trainable params: 12,268,595\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 2s 4ms/sample - loss: 0.8560 - accuracy: 0.6191 - val_loss: 0.7616 - val_accuracy: 0.7460\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7454 - accuracy: 0.6931 - val_loss: 1.0957 - val_accuracy: 0.1746\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6715 - accuracy: 0.7419 - val_loss: 1.4339 - val_accuracy: 0.1746\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6289 - accuracy: 0.7888 - val_loss: 1.6244 - val_accuracy: 0.1746\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.5437 - accuracy: 0.8303 - val_loss: 1.8733 - val_accuracy: 0.1746\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.4510 - accuracy: 0.8773 - val_loss: 2.0833 - val_accuracy: 0.1746\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3844 - accuracy: 0.8953 - val_loss: 2.2267 - val_accuracy: 0.1746\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3263 - accuracy: 0.9206 - val_loss: 2.3500 - val_accuracy: 0.1746\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.2472 - accuracy: 0.9549 - val_loss: 2.4160 - val_accuracy: 0.1746\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1902 - accuracy: 0.9783 - val_loss: 2.0117 - val_accuracy: 0.1746\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1631 - accuracy: 0.9747 - val_loss: 1.9651 - val_accuracy: 0.1746\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.1185 - accuracy: 0.9964 - val_loss: 1.8032 - val_accuracy: 0.1746\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.1005 - accuracy: 0.9946 - val_loss: 1.9048 - val_accuracy: 0.1746\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0950 - accuracy: 0.9982 - val_loss: 1.2825 - val_accuracy: 0.1746\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0807 - accuracy: 0.9964 - val_loss: 1.2613 - val_accuracy: 0.3333\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0688 - accuracy: 1.0000 - val_loss: 1.6867 - val_accuracy: 0.3810\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0585 - accuracy: 1.0000 - val_loss: 1.4114 - val_accuracy: 0.3175\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0402 - accuracy: 1.0000 - val_loss: 1.2194 - val_accuracy: 0.3651\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.6846 - val_accuracy: 0.7302\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.7967 - val_accuracy: 0.7302\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0395 - accuracy: 0.9982 - val_loss: 0.4982 - val_accuracy: 0.8730\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0324 - accuracy: 0.9982 - val_loss: 0.5008 - val_accuracy: 0.9048\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0256 - accuracy: 1.0000 - val_loss: 0.5516 - val_accuracy: 0.8889\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0244 - accuracy: 1.0000 - val_loss: 0.6060 - val_accuracy: 0.8730\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 0.8730\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.4917 - val_accuracy: 0.8889\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.4921 - val_accuracy: 0.8889\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.3338 - val_accuracy: 0.9048\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.3653 - val_accuracy: 0.8889\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.3343 - val_accuracy: 0.9048\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.4851 - val_accuracy: 0.8571\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.3375 - val_accuracy: 0.9048\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.5517 - val_accuracy: 0.8889\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.3947 - val_accuracy: 0.8889\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.3532 - val_accuracy: 0.9206\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0130 - accuracy: 0.9982 - val_loss: 0.4173 - val_accuracy: 0.9048\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0280 - accuracy: 0.9982 - val_loss: 0.4744 - val_accuracy: 0.8889\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.3360 - val_accuracy: 0.9365\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.3588 - val_accuracy: 0.8889\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.3881 - val_accuracy: 0.9048\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.3779 - val_accuracy: 0.9048\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.4067 - val_accuracy: 0.8730\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.3621 - val_accuracy: 0.8889\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.5366 - val_accuracy: 0.9048\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.3440 - val_accuracy: 0.9048\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.3936 - val_accuracy: 0.9048\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.3389 - val_accuracy: 0.9365\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.3457 - val_accuracy: 0.9206\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.3913 - val_accuracy: 0.9048\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.3452 - val_accuracy: 0.9206\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.3369 - val_accuracy: 0.9206\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.5192 - val_accuracy: 0.8889\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.3809 - val_accuracy: 0.9048\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.3518 - val_accuracy: 0.9206\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.3699 - val_accuracy: 0.9365\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.3767 - val_accuracy: 0.9048\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3514 - val_accuracy: 0.9365\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.5391 - val_accuracy: 0.8571\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0205 - accuracy: 0.9946 - val_loss: 0.4395 - val_accuracy: 0.9048\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0440 - accuracy: 0.9892 - val_loss: 0.4525 - val_accuracy: 0.9048\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0196 - accuracy: 0.9982 - val_loss: 0.6240 - val_accuracy: 0.8889\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.4471 - val_accuracy: 0.9048\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.4764 - val_accuracy: 0.9048\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.3705 - val_accuracy: 0.9048\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.3442 - val_accuracy: 0.9206\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.3752 - val_accuracy: 0.9206\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.3917 - val_accuracy: 0.9048\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.3445 - val_accuracy: 0.9206\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.3281 - val_accuracy: 0.9206\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.4843 - val_accuracy: 0.9206\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.5506 - val_accuracy: 0.9048\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.4213 - val_accuracy: 0.8889\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.3886 - val_accuracy: 0.9048\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.4283 - val_accuracy: 0.8889\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.3866 - val_accuracy: 0.9048\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.4039 - val_accuracy: 0.8889\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0093 - accuracy: 0.9982 - val_loss: 0.3945 - val_accuracy: 0.9206\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.3849 - val_accuracy: 0.9365\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.3766 - val_accuracy: 0.9206\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.3812 - val_accuracy: 0.9048\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.3927 - val_accuracy: 0.9048\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3856 - val_accuracy: 0.9206\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3922 - val_accuracy: 0.9206\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4583 - val_accuracy: 0.9206\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4060 - val_accuracy: 0.9048\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3962 - val_accuracy: 0.9206\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.3970 - val_accuracy: 0.9048\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5409 - val_accuracy: 0.8889\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.3976 - val_accuracy: 0.9206\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4469 - val_accuracy: 0.9206\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3930 - val_accuracy: 0.9048\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4746 - val_accuracy: 0.9048\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4174 - val_accuracy: 0.9206\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4629 - val_accuracy: 0.9048\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3543 - val_accuracy: 0.9206\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3624 - val_accuracy: 0.9206\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3514 - val_accuracy: 0.9206\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0169 - accuracy: 0.9964 - val_loss: 0.6525 - val_accuracy: 0.8413\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0210 - accuracy: 0.9964 - val_loss: 0.4844 - val_accuracy: 0.9048\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0397 - accuracy: 0.9892 - val_loss: 0.8694 - val_accuracy: 0.8571\n",
      "accuracy for model 3 is 85.71428656578064\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 170387, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 170380, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 170375, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 170375, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 85187, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 85187, 6)          24        \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 511122)            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 24)                12266952  \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 12,268,623\n",
      "Trainable params: 12,268,595\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 2s 4ms/sample - loss: 0.8752 - accuracy: 0.6209 - val_loss: 0.6693 - val_accuracy: 0.7778\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7423 - accuracy: 0.7256 - val_loss: 1.0027 - val_accuracy: 0.2063\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.6607 - accuracy: 0.7545 - val_loss: 1.2951 - val_accuracy: 0.1746\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6134 - accuracy: 0.7744 - val_loss: 1.5146 - val_accuracy: 0.1746\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.5300 - accuracy: 0.8213 - val_loss: 1.7681 - val_accuracy: 0.1746\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.4353 - accuracy: 0.8682 - val_loss: 1.8989 - val_accuracy: 0.1746\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3806 - accuracy: 0.8827 - val_loss: 2.0687 - val_accuracy: 0.1746\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3145 - accuracy: 0.9025 - val_loss: 2.4535 - val_accuracy: 0.1746\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.2474 - accuracy: 0.9314 - val_loss: 1.9610 - val_accuracy: 0.1746\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.2063 - accuracy: 0.9513 - val_loss: 1.8802 - val_accuracy: 0.1746\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1515 - accuracy: 0.9783 - val_loss: 2.0216 - val_accuracy: 0.1746\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1050 - accuracy: 0.9838 - val_loss: 1.7922 - val_accuracy: 0.1905\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0855 - accuracy: 0.9928 - val_loss: 1.6713 - val_accuracy: 0.1905\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0655 - accuracy: 0.9982 - val_loss: 1.2394 - val_accuracy: 0.2063\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0743 - accuracy: 0.9910 - val_loss: 1.6423 - val_accuracy: 0.1905\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0592 - accuracy: 0.9982 - val_loss: 0.9279 - val_accuracy: 0.5397\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0517 - accuracy: 0.9982 - val_loss: 1.2087 - val_accuracy: 0.2698\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0422 - accuracy: 1.0000 - val_loss: 0.8291 - val_accuracy: 0.6190\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.7943 - val_accuracy: 0.6190\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0395 - accuracy: 0.9982 - val_loss: 0.6802 - val_accuracy: 0.6825\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0324 - accuracy: 0.9982 - val_loss: 0.6899 - val_accuracy: 0.7302\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.5615 - val_accuracy: 0.7937\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0268 - accuracy: 0.9982 - val_loss: 0.8944 - val_accuracy: 0.6508\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 3s 5ms/sample - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.5591 - val_accuracy: 0.7302\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.4648 - val_accuracy: 0.7937\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0206 - accuracy: 0.9982 - val_loss: 0.5801 - val_accuracy: 0.7937\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.3818 - val_accuracy: 0.9048\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0225 - accuracy: 0.9982 - val_loss: 0.5620 - val_accuracy: 0.8095\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0252 - accuracy: 0.9982 - val_loss: 0.9038 - val_accuracy: 0.7143\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.3848 - val_accuracy: 0.9048\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.4667 - val_accuracy: 0.8571\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.3991 - val_accuracy: 0.8889\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.3831 - val_accuracy: 0.9206\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.3980 - val_accuracy: 0.8571\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.4500 - val_accuracy: 0.8889\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3780 - val_accuracy: 0.9048\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.3335 - val_accuracy: 0.8889\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.3240 - val_accuracy: 0.9048\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.4147 - val_accuracy: 0.9206\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.4642 - val_accuracy: 0.8730\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.5301 - val_accuracy: 0.8571\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.4654 - val_accuracy: 0.9048\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.4069 - val_accuracy: 0.8730\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0119 - accuracy: 0.9982 - val_loss: 0.5947 - val_accuracy: 0.7778\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.3834 - val_accuracy: 0.9048\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.4487 - val_accuracy: 0.9048\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.4047 - val_accuracy: 0.8730\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.5230 - val_accuracy: 0.8254\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.4353 - val_accuracy: 0.8889\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.4328 - val_accuracy: 0.8889\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.4300 - val_accuracy: 0.9048\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.5053 - val_accuracy: 0.8413\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.4598 - val_accuracy: 0.9048\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.4880 - val_accuracy: 0.8571\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4659 - val_accuracy: 0.8889\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5312 - val_accuracy: 0.8413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.3992 - val_accuracy: 0.9206\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.3771 - val_accuracy: 0.8889\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.4175 - val_accuracy: 0.8413\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.4573 - val_accuracy: 0.9048\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4701 - val_accuracy: 0.8413\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3642 - val_accuracy: 0.9206\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.4844 - val_accuracy: 0.8571\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.5136 - val_accuracy: 0.8571\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4900 - val_accuracy: 0.8571\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4523 - val_accuracy: 0.8730\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4209 - val_accuracy: 0.8730\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4303 - val_accuracy: 0.8889\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3997 - val_accuracy: 0.8889\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3989 - val_accuracy: 0.9048\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4242 - val_accuracy: 0.9048\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.4527 - val_accuracy: 0.8730\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.7329 - val_accuracy: 0.8889\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.6785 - val_accuracy: 0.8571\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0312 - accuracy: 0.9910 - val_loss: 0.8524 - val_accuracy: 0.7778\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0262 - accuracy: 0.9928 - val_loss: 0.8981 - val_accuracy: 0.8254\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0314 - accuracy: 0.9946 - val_loss: 1.2652 - val_accuracy: 0.8095\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0464 - accuracy: 0.9838 - val_loss: 1.6919 - val_accuracy: 0.7778\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0555 - accuracy: 0.9856 - val_loss: 2.1987 - val_accuracy: 0.5397\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0248 - accuracy: 0.9964 - val_loss: 0.5661 - val_accuracy: 0.8095\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.5378 - val_accuracy: 0.8413\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.4356 - val_accuracy: 0.8730\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.4471 - val_accuracy: 0.8730\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.4663 - val_accuracy: 0.9048\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.5151 - val_accuracy: 0.8730\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.4543 - val_accuracy: 0.9048\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.4603 - val_accuracy: 0.8730\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.5908 - val_accuracy: 0.8095\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.6727 - val_accuracy: 0.7937\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4397 - val_accuracy: 0.8889\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4808 - val_accuracy: 0.8730\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5162 - val_accuracy: 0.8413\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5718 - val_accuracy: 0.8254\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4794 - val_accuracy: 0.8730\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5449 - val_accuracy: 0.8730\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4932 - val_accuracy: 0.8730\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.4066 - val_accuracy: 0.8413\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.7354 - val_accuracy: 0.7778\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5751 - val_accuracy: 0.8730\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 1s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5356 - val_accuracy: 0.8889\n",
      "accuracy for model 4 is 88.88888955116272\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 170387, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 170380, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 170375, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 170375, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 85187, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 85187, 6)          24        \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 511122)            0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 24)                12266952  \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 12,268,623\n",
      "Trainable params: 12,268,595\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 555 samples, validate on 62 samples\n",
      "Epoch 1/100\n",
      "555/555 [==============================] - 3s 5ms/sample - loss: 0.9246 - accuracy: 0.6324 - val_loss: 0.7795 - val_accuracy: 0.6774\n",
      "Epoch 2/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.8034 - accuracy: 0.6865 - val_loss: 1.1811 - val_accuracy: 0.1774\n",
      "Epoch 3/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.7251 - accuracy: 0.6991 - val_loss: 1.5325 - val_accuracy: 0.1774\n",
      "Epoch 4/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.6333 - accuracy: 0.7477 - val_loss: 1.8613 - val_accuracy: 0.1774\n",
      "Epoch 5/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.5460 - accuracy: 0.8162 - val_loss: 2.1020 - val_accuracy: 0.1774\n",
      "Epoch 6/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.4557 - accuracy: 0.8324 - val_loss: 2.4015 - val_accuracy: 0.1774\n",
      "Epoch 7/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.3585 - accuracy: 0.8649 - val_loss: 2.2380 - val_accuracy: 0.1774\n",
      "Epoch 8/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.2817 - accuracy: 0.9117 - val_loss: 2.4211 - val_accuracy: 0.1774\n",
      "Epoch 9/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.2162 - accuracy: 0.9477 - val_loss: 2.4469 - val_accuracy: 0.1774\n",
      "Epoch 10/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.1572 - accuracy: 0.9694 - val_loss: 2.4523 - val_accuracy: 0.1774\n",
      "Epoch 11/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.1373 - accuracy: 0.9730 - val_loss: 2.1283 - val_accuracy: 0.1774\n",
      "Epoch 12/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.1227 - accuracy: 0.9766 - val_loss: 2.3105 - val_accuracy: 0.1774\n",
      "Epoch 13/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0880 - accuracy: 0.9874 - val_loss: 2.1121 - val_accuracy: 0.1774\n",
      "Epoch 14/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0712 - accuracy: 0.9928 - val_loss: 1.7937 - val_accuracy: 0.1774\n",
      "Epoch 15/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0625 - accuracy: 1.0000 - val_loss: 1.7501 - val_accuracy: 0.3871\n",
      "Epoch 16/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0566 - accuracy: 0.9964 - val_loss: 1.5000 - val_accuracy: 0.2903\n",
      "Epoch 17/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0436 - accuracy: 1.0000 - val_loss: 1.2505 - val_accuracy: 0.3065\n",
      "Epoch 18/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0397 - accuracy: 1.0000 - val_loss: 1.2102 - val_accuracy: 0.3871\n",
      "Epoch 19/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0368 - accuracy: 0.9982 - val_loss: 0.5128 - val_accuracy: 0.7903\n",
      "Epoch 20/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0310 - accuracy: 0.9982 - val_loss: 0.9254 - val_accuracy: 0.4677\n",
      "Epoch 21/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.5843 - val_accuracy: 0.7258\n",
      "Epoch 22/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.3805 - val_accuracy: 0.8710\n",
      "Epoch 23/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.6155 - val_accuracy: 0.7742\n",
      "Epoch 24/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0225 - accuracy: 1.0000 - val_loss: 0.5969 - val_accuracy: 0.7419\n",
      "Epoch 25/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.4288 - val_accuracy: 0.8065\n",
      "Epoch 26/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.5354 - val_accuracy: 0.7742\n",
      "Epoch 27/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.4415 - val_accuracy: 0.8065\n",
      "Epoch 28/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.3612 - val_accuracy: 0.8710\n",
      "Epoch 29/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.5759 - val_accuracy: 0.7419\n",
      "Epoch 30/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.3648 - val_accuracy: 0.8710\n",
      "Epoch 31/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.2958 - val_accuracy: 0.9194\n",
      "Epoch 32/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.3050 - val_accuracy: 0.9194\n",
      "Epoch 33/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.2300 - val_accuracy: 0.9194\n",
      "Epoch 34/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.2904 - val_accuracy: 0.9194\n",
      "Epoch 35/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.2753 - val_accuracy: 0.9032\n",
      "Epoch 36/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.2630 - val_accuracy: 0.9194\n",
      "Epoch 37/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2165 - val_accuracy: 0.9355\n",
      "Epoch 38/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2722 - val_accuracy: 0.9032\n",
      "Epoch 39/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.3088 - val_accuracy: 0.8871\n",
      "Epoch 40/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.2419 - val_accuracy: 0.9355\n",
      "Epoch 41/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2526 - val_accuracy: 0.9032\n",
      "Epoch 42/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.2246 - val_accuracy: 0.9355\n",
      "Epoch 43/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.3564 - val_accuracy: 0.8710\n",
      "Epoch 44/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9355\n",
      "Epoch 45/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.2495 - val_accuracy: 0.9355\n",
      "Epoch 46/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2914 - val_accuracy: 0.9032\n",
      "Epoch 47/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.2519 - val_accuracy: 0.9032\n",
      "Epoch 48/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.3373 - val_accuracy: 0.8710\n",
      "Epoch 49/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.7009 - val_accuracy: 0.7097\n",
      "Epoch 50/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.4612 - val_accuracy: 0.8548\n",
      "Epoch 51/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.3029 - val_accuracy: 0.8871\n",
      "Epoch 52/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.3086 - val_accuracy: 0.9032\n",
      "Epoch 53/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2626 - val_accuracy: 0.9194\n",
      "Epoch 54/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.2525 - val_accuracy: 0.9194\n",
      "Epoch 55/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.2614 - val_accuracy: 0.8710\n",
      "Epoch 56/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.3071 - val_accuracy: 0.8548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2342 - val_accuracy: 0.8871\n",
      "Epoch 58/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2342 - val_accuracy: 0.9032\n",
      "Epoch 59/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2387 - val_accuracy: 0.9194\n",
      "Epoch 60/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2340 - val_accuracy: 0.8871\n",
      "Epoch 61/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.2791 - val_accuracy: 0.8871\n",
      "Epoch 62/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.2262 - val_accuracy: 0.9194\n",
      "Epoch 63/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2476 - val_accuracy: 0.9194\n",
      "Epoch 64/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0165 - accuracy: 0.9946 - val_loss: 0.3496 - val_accuracy: 0.8548\n",
      "Epoch 65/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0195 - accuracy: 0.9946 - val_loss: 0.2701 - val_accuracy: 0.9032\n",
      "Epoch 66/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0388 - accuracy: 0.9874 - val_loss: 0.4577 - val_accuracy: 0.8710\n",
      "Epoch 67/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0429 - accuracy: 0.9856 - val_loss: 0.8367 - val_accuracy: 0.8387\n",
      "Epoch 68/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0370 - accuracy: 0.9946 - val_loss: 0.9532 - val_accuracy: 0.8065\n",
      "Epoch 69/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0265 - accuracy: 0.9964 - val_loss: 0.5335 - val_accuracy: 0.8387\n",
      "Epoch 70/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.5405 - val_accuracy: 0.8710\n",
      "Epoch 71/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0111 - accuracy: 0.9982 - val_loss: 0.3537 - val_accuracy: 0.8710\n",
      "Epoch 72/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.2818 - val_accuracy: 0.8871\n",
      "Epoch 73/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.2832 - val_accuracy: 0.9194\n",
      "Epoch 74/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3902 - val_accuracy: 0.8710\n",
      "Epoch 75/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2783 - val_accuracy: 0.9355\n",
      "Epoch 76/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2706 - val_accuracy: 0.9516\n",
      "Epoch 77/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.3878 - val_accuracy: 0.8710\n",
      "Epoch 78/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4232 - val_accuracy: 0.8710\n",
      "Epoch 79/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.3483 - val_accuracy: 0.8548\n",
      "Epoch 80/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.3270 - val_accuracy: 0.8871\n",
      "Epoch 81/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2502 - val_accuracy: 0.9194\n",
      "Epoch 82/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2745 - val_accuracy: 0.9355\n",
      "Epoch 83/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.3225 - val_accuracy: 0.8871\n",
      "Epoch 84/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3100 - val_accuracy: 0.8871\n",
      "Epoch 85/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2713 - val_accuracy: 0.9032\n",
      "Epoch 86/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2585 - val_accuracy: 0.9355\n",
      "Epoch 87/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2400 - val_accuracy: 0.9355\n",
      "Epoch 88/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0067 - accuracy: 0.9982 - val_loss: 0.3299 - val_accuracy: 0.8710\n",
      "Epoch 89/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.3228 - val_accuracy: 0.8548\n",
      "Epoch 90/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0088 - accuracy: 0.9982 - val_loss: 0.3303 - val_accuracy: 0.8710\n",
      "Epoch 91/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 0.7258\n",
      "Epoch 92/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2307 - val_accuracy: 0.8871\n",
      "Epoch 93/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2502 - val_accuracy: 0.8710\n",
      "Epoch 94/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2225 - val_accuracy: 0.9194\n",
      "Epoch 95/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.7423 - val_accuracy: 0.8226\n",
      "Epoch 96/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2078 - val_accuracy: 0.9355\n",
      "Epoch 97/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2125 - val_accuracy: 0.9194\n",
      "Epoch 98/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2129 - val_accuracy: 0.9194\n",
      "Epoch 99/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1738 - val_accuracy: 0.9516\n",
      "Epoch 100/100\n",
      "555/555 [==============================] - 1s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1979 - val_accuracy: 0.9516\n",
      "accuracy for model 5 is 95.16128897666931\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 170387, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 170380, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 170375, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 170375, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 85187, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 85187, 6)          24        \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 511122)            0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 24)                12266952  \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 12,268,623\n",
      "Trainable params: 12,268,595\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "556/556 [==============================] - 3s 6ms/sample - loss: 1.0447 - accuracy: 0.5396 - val_loss: 0.5638 - val_accuracy: 0.7869\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.7322 - accuracy: 0.6709 - val_loss: 0.7220 - val_accuracy: 0.7869\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.6691 - accuracy: 0.7266 - val_loss: 1.0117 - val_accuracy: 0.3443\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.5951 - accuracy: 0.7698 - val_loss: 1.2652 - val_accuracy: 0.1967\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.5189 - accuracy: 0.8273 - val_loss: 1.7666 - val_accuracy: 0.1803\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.4219 - accuracy: 0.8741 - val_loss: 1.2259 - val_accuracy: 0.2131\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.3399 - accuracy: 0.9191 - val_loss: 1.6568 - val_accuracy: 0.1803\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2755 - accuracy: 0.9424 - val_loss: 1.4763 - val_accuracy: 0.1803\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2373 - accuracy: 0.9640 - val_loss: 1.1937 - val_accuracy: 0.2131\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1947 - accuracy: 0.9676 - val_loss: 1.3609 - val_accuracy: 0.1803\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1599 - accuracy: 0.9910 - val_loss: 1.5973 - val_accuracy: 0.1803\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1307 - accuracy: 0.9910 - val_loss: 1.7136 - val_accuracy: 0.1803\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0911 - accuracy: 0.9946 - val_loss: 1.8011 - val_accuracy: 0.1803\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1131 - accuracy: 0.9766 - val_loss: 0.5862 - val_accuracy: 0.7705\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.1307 - accuracy: 0.9730 - val_loss: 1.1173 - val_accuracy: 0.3934\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0779 - accuracy: 0.9910 - val_loss: 1.2837 - val_accuracy: 0.3279\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0587 - accuracy: 0.9964 - val_loss: 1.2260 - val_accuracy: 0.3770\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0469 - accuracy: 0.9982 - val_loss: 0.5775 - val_accuracy: 0.7541\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.6701 - val_accuracy: 0.7705\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.3621 - val_accuracy: 0.8689\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0307 - accuracy: 1.0000 - val_loss: 0.4128 - val_accuracy: 0.8197\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0288 - accuracy: 1.0000 - val_loss: 0.4288 - val_accuracy: 0.8197\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0298 - accuracy: 0.9982 - val_loss: 0.5609 - val_accuracy: 0.8689\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0324 - accuracy: 1.0000 - val_loss: 0.3565 - val_accuracy: 0.8689\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.5403 - val_accuracy: 0.7869\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.3830 - val_accuracy: 0.8525\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.3815 - val_accuracy: 0.8525\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.4384 - val_accuracy: 0.8689\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.4799 - val_accuracy: 0.9016\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0220 - accuracy: 0.9982 - val_loss: 0.5433 - val_accuracy: 0.8197\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0218 - accuracy: 1.0000 - val_loss: 0.3772 - val_accuracy: 0.8525\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.3451 - val_accuracy: 0.8361\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.3417 - val_accuracy: 0.8852\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.4458 - val_accuracy: 0.8197\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.5431 - val_accuracy: 0.8689\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.3888 - val_accuracy: 0.8361\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.4004 - val_accuracy: 0.8525\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3869 - val_accuracy: 0.8852\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.4736 - val_accuracy: 0.8852\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.5469 - val_accuracy: 0.7705\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.3368 - val_accuracy: 0.8689\n",
      "Epoch 42/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.3716 - val_accuracy: 0.8525\n",
      "Epoch 43/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.5100 - val_accuracy: 0.8852\n",
      "Epoch 44/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.4378 - val_accuracy: 0.8525\n",
      "Epoch 45/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.4111 - val_accuracy: 0.8852\n",
      "Epoch 46/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.3947 - val_accuracy: 0.8852\n",
      "Epoch 47/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.3817 - val_accuracy: 0.8525\n",
      "Epoch 48/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.3718 - val_accuracy: 0.8525\n",
      "Epoch 49/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.3853 - val_accuracy: 0.8689\n",
      "Epoch 50/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.3258 - val_accuracy: 0.8525\n",
      "Epoch 51/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0148 - accuracy: 0.9982 - val_loss: 0.6895 - val_accuracy: 0.8525\n",
      "Epoch 52/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0419 - accuracy: 0.9910 - val_loss: 1.0794 - val_accuracy: 0.6557\n",
      "Epoch 53/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.5100 - val_accuracy: 0.8852\n",
      "Epoch 54/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.5465 - val_accuracy: 0.8197\n",
      "Epoch 55/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.4905 - val_accuracy: 0.8197\n",
      "Epoch 56/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.4392 - val_accuracy: 0.8852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.4509 - val_accuracy: 0.8361\n",
      "Epoch 58/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.4495 - val_accuracy: 0.8361\n",
      "Epoch 59/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4331 - val_accuracy: 0.8689\n",
      "Epoch 60/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4063 - val_accuracy: 0.8361\n",
      "Epoch 61/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.4642 - val_accuracy: 0.8361\n",
      "Epoch 62/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.4461 - val_accuracy: 0.9016\n",
      "Epoch 63/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.3882 - val_accuracy: 0.8852\n",
      "Epoch 64/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.3781 - val_accuracy: 0.8525\n",
      "Epoch 65/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.4021 - val_accuracy: 0.9016\n",
      "Epoch 66/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.4093 - val_accuracy: 0.8689\n",
      "Epoch 67/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.4009 - val_accuracy: 0.8525\n",
      "Epoch 68/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.4172 - val_accuracy: 0.8361\n",
      "Epoch 69/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4211 - val_accuracy: 0.8525\n",
      "Epoch 70/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4137 - val_accuracy: 0.8361\n",
      "Epoch 71/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4469 - val_accuracy: 0.8361\n",
      "Epoch 72/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4228 - val_accuracy: 0.8525\n",
      "Epoch 73/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4494 - val_accuracy: 0.8525\n",
      "Epoch 74/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4402 - val_accuracy: 0.8361\n",
      "Epoch 75/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.3645 - val_accuracy: 0.8689\n",
      "Epoch 76/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4025 - val_accuracy: 0.9016\n",
      "Epoch 77/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4211 - val_accuracy: 0.9016\n",
      "Epoch 78/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4969 - val_accuracy: 0.8689\n",
      "Epoch 79/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.5344 - val_accuracy: 0.7869\n",
      "Epoch 80/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4014 - val_accuracy: 0.8689\n",
      "Epoch 81/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4130 - val_accuracy: 0.8852\n",
      "Epoch 82/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5489 - val_accuracy: 0.8689\n",
      "Epoch 83/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5210 - val_accuracy: 0.8033\n",
      "Epoch 84/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.8850 - val_accuracy: 0.8525\n",
      "Epoch 85/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.3703 - val_accuracy: 0.8525\n",
      "Epoch 86/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3252 - val_accuracy: 0.8689\n",
      "Epoch 87/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4786 - val_accuracy: 0.8852\n",
      "Epoch 88/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4557 - val_accuracy: 0.8852\n",
      "Epoch 89/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4321 - val_accuracy: 0.8525\n",
      "Epoch 90/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3494 - val_accuracy: 0.8852\n",
      "Epoch 91/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3942 - val_accuracy: 0.8689\n",
      "Epoch 92/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.9627 - val_accuracy: 0.8689\n",
      "Epoch 93/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4906 - val_accuracy: 0.8197\n",
      "Epoch 94/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5074 - val_accuracy: 0.7869\n",
      "Epoch 95/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4356 - val_accuracy: 0.8689\n",
      "Epoch 96/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4695 - val_accuracy: 0.8689\n",
      "Epoch 97/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4518 - val_accuracy: 0.8525\n",
      "Epoch 98/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4288 - val_accuracy: 0.8525\n",
      "Epoch 99/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6071 - val_accuracy: 0.8689\n",
      "Epoch 100/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3641 - val_accuracy: 0.8689\n",
      "accuracy for model 6 is 86.8852436542511\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 170387, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 170380, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 170375, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 170375, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 85187, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 85187, 6)          24        \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 511122)            0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 24)                12266952  \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 12,268,623\n",
      "Trainable params: 12,268,595\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "556/556 [==============================] - 6s 11ms/sample - loss: 0.8386 - accuracy: 0.6673 - val_loss: 0.5718 - val_accuracy: 0.7869\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.7090 - accuracy: 0.7086 - val_loss: 1.0053 - val_accuracy: 0.4098\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.7024 - accuracy: 0.7212 - val_loss: 1.5012 - val_accuracy: 0.2787\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.6126 - accuracy: 0.7752 - val_loss: 2.0523 - val_accuracy: 0.1803\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.5571 - accuracy: 0.8058 - val_loss: 2.3340 - val_accuracy: 0.1803\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.4690 - accuracy: 0.8435 - val_loss: 2.6125 - val_accuracy: 0.1803\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.4004 - accuracy: 0.8813 - val_loss: 2.8002 - val_accuracy: 0.1803\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.3623 - accuracy: 0.9083 - val_loss: 2.8744 - val_accuracy: 0.1803\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2965 - accuracy: 0.9227 - val_loss: 2.9781 - val_accuracy: 0.1803\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2524 - accuracy: 0.9335 - val_loss: 2.8639 - val_accuracy: 0.1803\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1979 - accuracy: 0.9604 - val_loss: 2.3768 - val_accuracy: 0.1803\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1884 - accuracy: 0.9586 - val_loss: 2.3099 - val_accuracy: 0.1803\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1470 - accuracy: 0.9604 - val_loss: 1.4734 - val_accuracy: 0.2459\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1432 - accuracy: 0.9676 - val_loss: 1.6707 - val_accuracy: 0.1967\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1086 - accuracy: 0.9874 - val_loss: 1.3891 - val_accuracy: 0.2951\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.1064 - accuracy: 0.9784 - val_loss: 1.5752 - val_accuracy: 0.2951\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0692 - accuracy: 0.9946 - val_loss: 1.6025 - val_accuracy: 0.3115\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0613 - accuracy: 0.9964 - val_loss: 1.2596 - val_accuracy: 0.3607\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0476 - accuracy: 1.0000 - val_loss: 0.8307 - val_accuracy: 0.5738\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0507 - accuracy: 0.9964 - val_loss: 1.1109 - val_accuracy: 0.4590\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0518 - accuracy: 0.9946 - val_loss: 0.5992 - val_accuracy: 0.8689\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0496 - accuracy: 0.9964 - val_loss: 1.3093 - val_accuracy: 0.3607\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.5736 - val_accuracy: 0.7705\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0486 - accuracy: 0.9928 - val_loss: 0.4373 - val_accuracy: 0.8689\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.9519 - val_accuracy: 0.6066\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.3802 - val_accuracy: 0.8689\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0286 - accuracy: 0.9982 - val_loss: 0.6961 - val_accuracy: 0.7705\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0373 - accuracy: 0.9946 - val_loss: 0.6266 - val_accuracy: 0.7705\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.2424 - val_accuracy: 0.9344\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.7199 - val_accuracy: 0.7705\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.3191 - val_accuracy: 0.9016\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.3847 - val_accuracy: 0.8852\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.4415 - val_accuracy: 0.9016\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.4246 - val_accuracy: 0.8689\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.3455 - val_accuracy: 0.9180\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.4410 - val_accuracy: 0.8689\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.3874 - val_accuracy: 0.9016\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.4189 - val_accuracy: 0.9016\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.2996 - val_accuracy: 0.9180\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.2974 - val_accuracy: 0.9016\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.3163 - val_accuracy: 0.9016\n",
      "Epoch 42/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.3608 - val_accuracy: 0.9016\n",
      "Epoch 43/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.3072 - val_accuracy: 0.9016\n",
      "Epoch 44/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.3281 - val_accuracy: 0.9016\n",
      "Epoch 45/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0238 - accuracy: 0.9946 - val_loss: 0.2653 - val_accuracy: 0.9180\n",
      "Epoch 46/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.3282 - val_accuracy: 0.9016\n",
      "Epoch 47/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9344\n",
      "Epoch 48/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.3454 - val_accuracy: 0.9016\n",
      "Epoch 49/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.3397 - val_accuracy: 0.9180\n",
      "Epoch 50/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.3976 - val_accuracy: 0.9180\n",
      "Epoch 51/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.3390 - val_accuracy: 0.9180\n",
      "Epoch 52/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.3534 - val_accuracy: 0.9016\n",
      "Epoch 53/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.4095 - val_accuracy: 0.9016\n",
      "Epoch 54/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.3618 - val_accuracy: 0.9180\n",
      "Epoch 55/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.3700 - val_accuracy: 0.9180\n",
      "Epoch 56/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.4486 - val_accuracy: 0.8852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.3912 - val_accuracy: 0.9180\n",
      "Epoch 58/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.3783 - val_accuracy: 0.9016\n",
      "Epoch 59/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.3931 - val_accuracy: 0.9180\n",
      "Epoch 60/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.3778 - val_accuracy: 0.9180\n",
      "Epoch 61/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.3442 - val_accuracy: 0.9016\n",
      "Epoch 62/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3915 - val_accuracy: 0.9016\n",
      "Epoch 63/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.3915 - val_accuracy: 0.9180\n",
      "Epoch 64/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.6387 - val_accuracy: 0.8525\n",
      "Epoch 65/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.4476 - val_accuracy: 0.9016\n",
      "Epoch 66/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.3082 - val_accuracy: 0.9016\n",
      "Epoch 67/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.3277 - val_accuracy: 0.9180\n",
      "Epoch 68/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.3667 - val_accuracy: 0.9344\n",
      "Epoch 69/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.3492 - val_accuracy: 0.9344\n",
      "Epoch 70/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.3876 - val_accuracy: 0.9016\n",
      "Epoch 71/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.3123 - val_accuracy: 0.9016\n",
      "Epoch 72/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.3494 - val_accuracy: 0.9180\n",
      "Epoch 73/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4128 - val_accuracy: 0.9180\n",
      "Epoch 74/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3634 - val_accuracy: 0.9180\n",
      "Epoch 75/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4365 - val_accuracy: 0.9180\n",
      "Epoch 76/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.9298 - val_accuracy: 0.8197\n",
      "Epoch 77/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.3145 - val_accuracy: 0.9180\n",
      "Epoch 78/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0270 - accuracy: 0.9964 - val_loss: 0.6185 - val_accuracy: 0.9016\n",
      "Epoch 79/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0132 - accuracy: 0.9964 - val_loss: 0.4869 - val_accuracy: 0.9180\n",
      "Epoch 80/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0187 - accuracy: 0.9946 - val_loss: 0.7099 - val_accuracy: 0.8689\n",
      "Epoch 81/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0132 - accuracy: 0.9982 - val_loss: 0.3390 - val_accuracy: 0.9180\n",
      "Epoch 82/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.3969 - val_accuracy: 0.9180\n",
      "Epoch 83/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.3979 - val_accuracy: 0.9180\n",
      "Epoch 84/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.4167 - val_accuracy: 0.9180\n",
      "Epoch 85/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0084 - accuracy: 0.9982 - val_loss: 0.3479 - val_accuracy: 0.9180\n",
      "Epoch 86/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0165 - accuracy: 0.9982 - val_loss: 0.6661 - val_accuracy: 0.8361\n",
      "Epoch 87/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.4630 - val_accuracy: 0.9180\n",
      "Epoch 88/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.9940 - val_accuracy: 0.8033\n",
      "Epoch 89/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.3834 - val_accuracy: 0.9180\n",
      "Epoch 90/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.3447 - val_accuracy: 0.8852\n",
      "Epoch 91/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3312 - val_accuracy: 0.9180\n",
      "Epoch 92/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.3447 - val_accuracy: 0.9180\n",
      "Epoch 93/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3691 - val_accuracy: 0.9180\n",
      "Epoch 94/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.4620 - val_accuracy: 0.9016\n",
      "Epoch 95/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4343 - val_accuracy: 0.9180\n",
      "Epoch 96/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.5764 - val_accuracy: 0.8689\n",
      "Epoch 97/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5034 - val_accuracy: 0.9016\n",
      "Epoch 98/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4752 - val_accuracy: 0.9180\n",
      "Epoch 99/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4239 - val_accuracy: 0.9180\n",
      "Epoch 100/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4504 - val_accuracy: 0.9016\n",
      "accuracy for model 7 is 90.16393423080444\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 170387, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 170380, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 170375, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 170375, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 85187, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 85187, 6)          24        \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 511122)            0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 24)                12266952  \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 12,268,623\n",
      "Trainable params: 12,268,595\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "556/556 [==============================] - 2s 4ms/sample - loss: 0.8462 - accuracy: 0.6223 - val_loss: 0.7968 - val_accuracy: 0.6557\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.7054 - accuracy: 0.7392 - val_loss: 1.1800 - val_accuracy: 0.1803\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.6830 - accuracy: 0.7284 - val_loss: 1.7083 - val_accuracy: 0.1803\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.6324 - accuracy: 0.7446 - val_loss: 2.0543 - val_accuracy: 0.1803\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.6059 - accuracy: 0.7680 - val_loss: 2.3167 - val_accuracy: 0.1803\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.5229 - accuracy: 0.8112 - val_loss: 2.7211 - val_accuracy: 0.1803\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.3949 - accuracy: 0.8633 - val_loss: 2.7175 - val_accuracy: 0.1803\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.3585 - accuracy: 0.8903 - val_loss: 2.5868 - val_accuracy: 0.1803\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.2901 - accuracy: 0.9227 - val_loss: 2.5680 - val_accuracy: 0.1803\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.2286 - accuracy: 0.9353 - val_loss: 2.4893 - val_accuracy: 0.1803\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1635 - accuracy: 0.9730 - val_loss: 2.3835 - val_accuracy: 0.1803\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1427 - accuracy: 0.9802 - val_loss: 1.7744 - val_accuracy: 0.1803\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.1160 - accuracy: 0.9892 - val_loss: 1.8908 - val_accuracy: 0.1803\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.1024 - accuracy: 0.9892 - val_loss: 1.3760 - val_accuracy: 0.1803\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0833 - accuracy: 0.9946 - val_loss: 1.8265 - val_accuracy: 0.1803\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0685 - accuracy: 0.9982 - val_loss: 1.4439 - val_accuracy: 0.1967\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0529 - accuracy: 0.9964 - val_loss: 1.3673 - val_accuracy: 0.1967\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0419 - accuracy: 1.0000 - val_loss: 1.3523 - val_accuracy: 0.1967\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0358 - accuracy: 0.9964 - val_loss: 0.7491 - val_accuracy: 0.6066\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0414 - accuracy: 0.9964 - val_loss: 0.7843 - val_accuracy: 0.5738\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.8493 - val_accuracy: 0.5246\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.7244 - val_accuracy: 0.7049\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.6059 - val_accuracy: 0.7705\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0305 - accuracy: 0.9982 - val_loss: 0.5645 - val_accuracy: 0.8033\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 0.7705\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.5680 - val_accuracy: 0.7705\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.5724 - val_accuracy: 0.8033\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.5212 - val_accuracy: 0.7869\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.7824 - val_accuracy: 0.7377\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.6228 - val_accuracy: 0.8033\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.5537 - val_accuracy: 0.8033\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.6184 - val_accuracy: 0.7869\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.5728 - val_accuracy: 0.7869\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0214 - accuracy: 0.9982 - val_loss: 0.7135 - val_accuracy: 0.7705\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0177 - accuracy: 0.9964 - val_loss: 0.7925 - val_accuracy: 0.8033\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.5872 - val_accuracy: 0.7705\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.7071 - val_accuracy: 0.7869\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.6725 - val_accuracy: 0.7541\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.7449 - val_accuracy: 0.8033\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.6059 - val_accuracy: 0.7705\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.6493 - val_accuracy: 0.7869\n",
      "Epoch 42/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.6545 - val_accuracy: 0.7705\n",
      "Epoch 43/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0150 - accuracy: 0.9982 - val_loss: 0.9089 - val_accuracy: 0.7049\n",
      "Epoch 44/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.7705\n",
      "Epoch 45/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 0.7705\n",
      "Epoch 46/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.7990 - val_accuracy: 0.7869\n",
      "Epoch 47/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 0.8033\n",
      "Epoch 48/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.7353 - val_accuracy: 0.8197\n",
      "Epoch 49/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.6505 - val_accuracy: 0.8197\n",
      "Epoch 50/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.6140 - val_accuracy: 0.8033\n",
      "Epoch 51/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.6473 - val_accuracy: 0.8197\n",
      "Epoch 52/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.5991 - val_accuracy: 0.7869\n",
      "Epoch 53/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.6760 - val_accuracy: 0.8033\n",
      "Epoch 54/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.7623 - val_accuracy: 0.8197\n",
      "Epoch 55/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.6491 - val_accuracy: 0.7705\n",
      "Epoch 56/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.6592 - val_accuracy: 0.8197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.7481 - val_accuracy: 0.8197\n",
      "Epoch 58/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.6508 - val_accuracy: 0.7705\n",
      "Epoch 59/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.6541 - val_accuracy: 0.7705\n",
      "Epoch 60/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.6682 - val_accuracy: 0.7705\n",
      "Epoch 61/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.7889 - val_accuracy: 0.8197\n",
      "Epoch 62/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.6125 - val_accuracy: 0.7869\n",
      "Epoch 63/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.7015 - val_accuracy: 0.7705\n",
      "Epoch 64/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1112 - val_accuracy: 0.8197\n",
      "Epoch 65/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.6652 - val_accuracy: 0.7705\n",
      "Epoch 66/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.6755 - val_accuracy: 0.8033\n",
      "Epoch 67/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.6719 - val_accuracy: 0.7705\n",
      "Epoch 68/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.6529 - val_accuracy: 0.7869\n",
      "Epoch 69/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6577 - val_accuracy: 0.8033\n",
      "Epoch 70/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.6321 - val_accuracy: 0.7869\n",
      "Epoch 71/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.6464 - val_accuracy: 0.8361\n",
      "Epoch 72/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6685 - val_accuracy: 0.7869\n",
      "Epoch 73/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6934 - val_accuracy: 0.8033\n",
      "Epoch 74/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 0.8033\n",
      "Epoch 75/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.8282 - val_accuracy: 0.8033\n",
      "Epoch 76/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6649 - val_accuracy: 0.8033\n",
      "Epoch 77/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.7276 - val_accuracy: 0.8033\n",
      "Epoch 78/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.6445 - val_accuracy: 0.8033\n",
      "Epoch 79/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.8279 - val_accuracy: 0.7705\n",
      "Epoch 80/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7196 - val_accuracy: 0.7869\n",
      "Epoch 81/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.7586 - val_accuracy: 0.8197\n",
      "Epoch 82/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.7342 - val_accuracy: 0.7541\n",
      "Epoch 83/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.1423 - val_accuracy: 0.7705\n",
      "Epoch 84/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.1677 - val_accuracy: 0.7705\n",
      "Epoch 85/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.6787 - val_accuracy: 0.8033\n",
      "Epoch 86/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.2134 - val_accuracy: 0.7869\n",
      "Epoch 87/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.7149 - val_accuracy: 0.7869\n",
      "Epoch 88/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7536 - val_accuracy: 0.8033\n",
      "Epoch 89/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.8664 - val_accuracy: 0.8033\n",
      "Epoch 90/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.7072 - val_accuracy: 0.8361\n",
      "Epoch 91/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.0477 - val_accuracy: 0.6885\n",
      "Epoch 92/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0348 - accuracy: 0.9964 - val_loss: 1.4839 - val_accuracy: 0.5410\n",
      "Epoch 93/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0224 - accuracy: 0.9964 - val_loss: 4.9809 - val_accuracy: 0.2787\n",
      "Epoch 94/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0155 - accuracy: 0.9964 - val_loss: 1.0176 - val_accuracy: 0.7705\n",
      "Epoch 95/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0149 - accuracy: 0.9982 - val_loss: 1.3882 - val_accuracy: 0.7049\n",
      "Epoch 96/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.8922 - val_accuracy: 0.7541\n",
      "Epoch 97/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.8872 - val_accuracy: 0.8197\n",
      "Epoch 98/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.9253 - val_accuracy: 0.8033\n",
      "Epoch 99/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.9508 - val_accuracy: 0.8033\n",
      "Epoch 100/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.0224 - accuracy: 0.9946 - val_loss: 6.7651 - val_accuracy: 0.6066\n",
      "accuracy for model 8 is 60.65573692321777\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_24 (Conv1D)           (None, 170387, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 170380, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 170375, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 170375, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 85187, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 85187, 6)          24        \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 511122)            0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 24)                12266952  \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 12,268,623\n",
      "Trainable params: 12,268,595\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 557 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      "557/557 [==============================] - 3s 5ms/sample - loss: 0.7889 - accuracy: 0.6750 - val_loss: 0.9108 - val_accuracy: 0.6500\n",
      "Epoch 2/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.6747 - accuracy: 0.7271 - val_loss: 1.3719 - val_accuracy: 0.1833\n",
      "Epoch 3/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.5979 - accuracy: 0.7666 - val_loss: 1.7466 - val_accuracy: 0.1667\n",
      "Epoch 4/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.5535 - accuracy: 0.8043 - val_loss: 1.8820 - val_accuracy: 0.1667\n",
      "Epoch 5/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.4521 - accuracy: 0.8618 - val_loss: 2.2036 - val_accuracy: 0.1667\n",
      "Epoch 6/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.4134 - accuracy: 0.8654 - val_loss: 2.4063 - val_accuracy: 0.1667\n",
      "Epoch 7/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.3363 - accuracy: 0.8977 - val_loss: 2.4881 - val_accuracy: 0.1667\n",
      "Epoch 8/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.2908 - accuracy: 0.9228 - val_loss: 2.5941 - val_accuracy: 0.1667\n",
      "Epoch 9/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.2478 - accuracy: 0.9425 - val_loss: 2.6182 - val_accuracy: 0.1667\n",
      "Epoch 10/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.1966 - accuracy: 0.9623 - val_loss: 2.4930 - val_accuracy: 0.1667\n",
      "Epoch 11/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.1763 - accuracy: 0.9641 - val_loss: 2.7095 - val_accuracy: 0.1667\n",
      "Epoch 12/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1434 - accuracy: 0.9820 - val_loss: 2.3153 - val_accuracy: 0.1667\n",
      "Epoch 13/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1255 - accuracy: 0.9820 - val_loss: 2.1307 - val_accuracy: 0.1667\n",
      "Epoch 14/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0998 - accuracy: 0.9928 - val_loss: 1.8586 - val_accuracy: 0.1667\n",
      "Epoch 15/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0944 - accuracy: 0.9946 - val_loss: 1.8255 - val_accuracy: 0.1667\n",
      "Epoch 16/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0839 - accuracy: 0.9892 - val_loss: 1.8017 - val_accuracy: 0.1833\n",
      "Epoch 17/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0657 - accuracy: 0.9928 - val_loss: 1.7541 - val_accuracy: 0.2000\n",
      "Epoch 18/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0609 - accuracy: 0.9964 - val_loss: 1.8420 - val_accuracy: 0.2167\n",
      "Epoch 19/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0461 - accuracy: 1.0000 - val_loss: 1.4524 - val_accuracy: 0.2167\n",
      "Epoch 20/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0406 - accuracy: 0.9982 - val_loss: 1.2543 - val_accuracy: 0.2500\n",
      "Epoch 21/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0372 - accuracy: 0.9982 - val_loss: 0.7593 - val_accuracy: 0.7333\n",
      "Epoch 22/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0381 - accuracy: 0.9982 - val_loss: 1.6454 - val_accuracy: 0.2167\n",
      "Epoch 23/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0453 - accuracy: 0.9964 - val_loss: 1.5662 - val_accuracy: 0.2500\n",
      "Epoch 24/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0425 - accuracy: 0.9946 - val_loss: 1.6450 - val_accuracy: 0.6500\n",
      "Epoch 25/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.8917 - val_accuracy: 0.7000\n",
      "Epoch 26/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.8462 - val_accuracy: 0.7667\n",
      "Epoch 27/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0347 - accuracy: 0.9982 - val_loss: 1.0732 - val_accuracy: 0.7000\n",
      "Epoch 28/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0251 - accuracy: 0.9982 - val_loss: 0.7865 - val_accuracy: 0.7500\n",
      "Epoch 29/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.8263 - val_accuracy: 0.7500\n",
      "Epoch 30/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.8107 - val_accuracy: 0.7833\n",
      "Epoch 31/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.7618 - val_accuracy: 0.7667\n",
      "Epoch 32/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.7880 - val_accuracy: 0.6833\n",
      "Epoch 33/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.8264 - val_accuracy: 0.7833\n",
      "Epoch 34/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.8833 - val_accuracy: 0.7667\n",
      "Epoch 35/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0207 - accuracy: 0.9982 - val_loss: 0.9291 - val_accuracy: 0.7333\n",
      "Epoch 36/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0196 - accuracy: 0.9982 - val_loss: 0.8202 - val_accuracy: 0.7500\n",
      "Epoch 37/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.8082 - val_accuracy: 0.7500\n",
      "Epoch 38/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.8568 - val_accuracy: 0.6833\n",
      "Epoch 39/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0101 - accuracy: 1.0000 - val_loss: 1.2007 - val_accuracy: 0.7333\n",
      "Epoch 40/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.8701 - val_accuracy: 0.7500\n",
      "Epoch 41/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.9770 - val_accuracy: 0.7167\n",
      "Epoch 42/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0095 - accuracy: 1.0000 - val_loss: 1.0301 - val_accuracy: 0.7333\n",
      "Epoch 43/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0376 - accuracy: 0.9928 - val_loss: 1.6654 - val_accuracy: 0.7000\n",
      "Epoch 44/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0300 - accuracy: 0.9946 - val_loss: 1.6647 - val_accuracy: 0.7333\n",
      "Epoch 45/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0295 - accuracy: 0.9964 - val_loss: 1.9357 - val_accuracy: 0.7000\n",
      "Epoch 46/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0143 - accuracy: 1.0000 - val_loss: 1.1380 - val_accuracy: 0.6833\n",
      "Epoch 47/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0100 - accuracy: 1.0000 - val_loss: 1.4152 - val_accuracy: 0.7000\n",
      "Epoch 48/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0096 - accuracy: 0.9982 - val_loss: 1.0501 - val_accuracy: 0.7333\n",
      "Epoch 49/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0101 - accuracy: 1.0000 - val_loss: 1.7766 - val_accuracy: 0.7000\n",
      "Epoch 50/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.8996 - val_accuracy: 0.7167\n",
      "Epoch 51/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.0706 - val_accuracy: 0.7667\n",
      "Epoch 52/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.9349 - val_accuracy: 0.7167\n",
      "Epoch 53/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 1.1404 - val_accuracy: 0.7500\n",
      "Epoch 54/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.0591 - val_accuracy: 0.7333\n",
      "Epoch 55/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 1.2759 - val_accuracy: 0.7167\n",
      "Epoch 56/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0141 - accuracy: 0.9964 - val_loss: 1.0711 - val_accuracy: 0.7333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.9936 - val_accuracy: 0.7667\n",
      "Epoch 58/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.2092 - val_accuracy: 0.7167\n",
      "Epoch 59/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.0898 - val_accuracy: 0.7333\n",
      "Epoch 60/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.0171 - val_accuracy: 0.7667\n",
      "Epoch 61/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.1329 - val_accuracy: 0.7500\n",
      "Epoch 62/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.4529 - val_accuracy: 0.7000\n",
      "Epoch 63/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.0949 - val_accuracy: 0.7500\n",
      "Epoch 64/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.1553 - val_accuracy: 0.7333\n",
      "Epoch 65/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.0890 - val_accuracy: 0.7333\n",
      "Epoch 66/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.3648 - val_accuracy: 0.7667\n",
      "Epoch 67/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.4079 - val_accuracy: 0.7500\n",
      "Epoch 68/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.7926 - val_accuracy: 0.7000\n",
      "Epoch 69/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.8158 - val_accuracy: 0.7000\n",
      "Epoch 70/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.4221 - val_accuracy: 0.6833\n",
      "Epoch 71/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.4703 - val_accuracy: 0.7167\n",
      "Epoch 72/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.1448 - val_accuracy: 0.7500\n",
      "Epoch 73/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.2342 - val_accuracy: 0.7500\n",
      "Epoch 74/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.1766 - val_accuracy: 0.7500\n",
      "Epoch 75/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2672 - val_accuracy: 0.7500\n",
      "Epoch 76/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.0061 - val_accuracy: 0.7000\n",
      "Epoch 77/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.6472 - val_accuracy: 0.7167\n",
      "Epoch 78/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.3509 - val_accuracy: 0.7500\n",
      "Epoch 79/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.3544 - val_accuracy: 0.7000\n",
      "Epoch 80/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.5798 - val_accuracy: 0.7167\n",
      "Epoch 81/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.2326 - val_accuracy: 0.7333\n",
      "Epoch 82/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.2148 - val_accuracy: 0.7167\n",
      "Epoch 83/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.3879 - val_accuracy: 0.7000\n",
      "Epoch 84/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.8380 - val_accuracy: 0.7167\n",
      "Epoch 85/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.2627 - val_accuracy: 0.7167\n",
      "Epoch 86/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.5263 - val_accuracy: 0.7333\n",
      "Epoch 87/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.4099 - val_accuracy: 0.7000\n",
      "Epoch 88/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.3534 - val_accuracy: 0.7000\n",
      "Epoch 89/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.4889 - val_accuracy: 0.7500\n",
      "Epoch 90/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.3909 - val_accuracy: 0.7333\n",
      "Epoch 91/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.2660 - val_accuracy: 0.7167\n",
      "Epoch 92/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.3493 - val_accuracy: 0.7000\n",
      "Epoch 93/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.3569 - val_accuracy: 0.7167\n",
      "Epoch 94/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.7286 - val_accuracy: 0.7167\n",
      "Epoch 95/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.2756 - val_accuracy: 0.7167\n",
      "Epoch 96/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.5495 - val_accuracy: 0.7000\n",
      "Epoch 97/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.5551 - val_accuracy: 0.7000\n",
      "Epoch 98/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.4054 - val_accuracy: 0.7167\n",
      "Epoch 99/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.6075 - val_accuracy: 0.7333\n",
      "Epoch 100/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.3462 - val_accuracy: 0.7500\n",
      "accuracy for model 9 is 75.0\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_27 (Conv1D)           (None, 170387, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 170387, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 170380, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 170380, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 170375, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 170375, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 85187, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 85187, 6)          24        \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 511122)            0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 24)                12266952  \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 12,268,623\n",
      "Trainable params: 12,268,595\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 557 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      "557/557 [==============================] - 2s 4ms/sample - loss: 0.8527 - accuracy: 0.6014 - val_loss: 1.0551 - val_accuracy: 0.5500\n",
      "Epoch 2/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.6775 - accuracy: 0.7110 - val_loss: 1.5029 - val_accuracy: 0.1667\n",
      "Epoch 3/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.6457 - accuracy: 0.7253 - val_loss: 2.1507 - val_accuracy: 0.1667\n",
      "Epoch 4/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.6045 - accuracy: 0.7558 - val_loss: 2.8803 - val_accuracy: 0.1667\n",
      "Epoch 5/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.5274 - accuracy: 0.7953 - val_loss: 3.8869 - val_accuracy: 0.1667\n",
      "Epoch 6/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.4955 - accuracy: 0.8276 - val_loss: 3.7879 - val_accuracy: 0.1667\n",
      "Epoch 7/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.3616 - accuracy: 0.8923 - val_loss: 3.6739 - val_accuracy: 0.1667\n",
      "Epoch 8/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.2917 - accuracy: 0.9174 - val_loss: 3.3629 - val_accuracy: 0.1667\n",
      "Epoch 9/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.2260 - accuracy: 0.9443 - val_loss: 3.2033 - val_accuracy: 0.1667\n",
      "Epoch 10/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1812 - accuracy: 0.9641 - val_loss: 2.7336 - val_accuracy: 0.1667\n",
      "Epoch 11/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1634 - accuracy: 0.9659 - val_loss: 2.5980 - val_accuracy: 0.1667\n",
      "Epoch 12/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1208 - accuracy: 0.9856 - val_loss: 1.5146 - val_accuracy: 0.2500\n",
      "Epoch 13/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1104 - accuracy: 0.9785 - val_loss: 2.2823 - val_accuracy: 0.2500\n",
      "Epoch 14/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0896 - accuracy: 0.9892 - val_loss: 1.5900 - val_accuracy: 0.2500\n",
      "Epoch 15/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0805 - accuracy: 0.9910 - val_loss: 1.1038 - val_accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0788 - accuracy: 0.9910 - val_loss: 1.9572 - val_accuracy: 0.2833\n",
      "Epoch 17/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0573 - accuracy: 0.9946 - val_loss: 1.2705 - val_accuracy: 0.4000\n",
      "Epoch 18/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0504 - accuracy: 0.9964 - val_loss: 0.9147 - val_accuracy: 0.6667\n",
      "Epoch 19/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0544 - accuracy: 0.9946 - val_loss: 0.8650 - val_accuracy: 0.7833\n",
      "Epoch 20/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0419 - accuracy: 0.9982 - val_loss: 0.8902 - val_accuracy: 0.7333\n",
      "Epoch 21/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0286 - accuracy: 1.0000 - val_loss: 1.0319 - val_accuracy: 0.5667\n",
      "Epoch 22/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.9433 - val_accuracy: 0.6833\n",
      "Epoch 23/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.8810 - val_accuracy: 0.7667\n",
      "Epoch 24/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0198 - accuracy: 1.0000 - val_loss: 0.8995 - val_accuracy: 0.7333\n",
      "Epoch 25/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0169 - accuracy: 1.0000 - val_loss: 1.0431 - val_accuracy: 0.6333\n",
      "Epoch 26/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.9951 - val_accuracy: 0.7333\n",
      "Epoch 27/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.8693 - val_accuracy: 0.7500\n",
      "Epoch 28/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0176 - accuracy: 1.0000 - val_loss: 1.0237 - val_accuracy: 0.7333\n",
      "Epoch 29/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0144 - accuracy: 1.0000 - val_loss: 1.2450 - val_accuracy: 0.5833\n",
      "Epoch 30/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0141 - accuracy: 1.0000 - val_loss: 1.0460 - val_accuracy: 0.7667\n",
      "Epoch 31/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0122 - accuracy: 1.0000 - val_loss: 1.0276 - val_accuracy: 0.7667\n",
      "Epoch 32/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0125 - accuracy: 1.0000 - val_loss: 1.1275 - val_accuracy: 0.7333\n",
      "Epoch 33/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0111 - accuracy: 1.0000 - val_loss: 1.0289 - val_accuracy: 0.7500\n",
      "Epoch 34/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0145 - accuracy: 1.0000 - val_loss: 1.4217 - val_accuracy: 0.7333\n",
      "Epoch 35/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0157 - accuracy: 0.9982 - val_loss: 1.2197 - val_accuracy: 0.7000\n",
      "Epoch 36/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0331 - accuracy: 0.9964 - val_loss: 1.6582 - val_accuracy: 0.7000\n",
      "Epoch 37/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0248 - accuracy: 0.9964 - val_loss: 1.5924 - val_accuracy: 0.6500\n",
      "Epoch 38/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0195 - accuracy: 0.9982 - val_loss: 1.3375 - val_accuracy: 0.6667\n",
      "Epoch 39/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0128 - accuracy: 1.0000 - val_loss: 1.2924 - val_accuracy: 0.7333\n",
      "Epoch 40/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.2686 - val_accuracy: 0.7500\n",
      "Epoch 41/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0097 - accuracy: 1.0000 - val_loss: 1.1559 - val_accuracy: 0.6833\n",
      "Epoch 42/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0224 - accuracy: 0.9982 - val_loss: 0.9632 - val_accuracy: 0.8000\n",
      "Epoch 43/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0134 - accuracy: 1.0000 - val_loss: 1.5571 - val_accuracy: 0.7000\n",
      "Epoch 44/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0145 - accuracy: 1.0000 - val_loss: 1.1548 - val_accuracy: 0.7500\n",
      "Epoch 45/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0069 - accuracy: 1.0000 - val_loss: 1.1765 - val_accuracy: 0.7500\n",
      "Epoch 46/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0087 - accuracy: 1.0000 - val_loss: 1.6798 - val_accuracy: 0.7167\n",
      "Epoch 47/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.2779 - val_accuracy: 0.7333\n",
      "Epoch 48/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0133 - accuracy: 1.0000 - val_loss: 1.2943 - val_accuracy: 0.7500\n",
      "Epoch 49/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.1547 - val_accuracy: 0.7333\n",
      "Epoch 50/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0277 - accuracy: 0.9928 - val_loss: 3.6084 - val_accuracy: 0.6500\n",
      "Epoch 51/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0591 - accuracy: 0.9803 - val_loss: 2.7490 - val_accuracy: 0.4167\n",
      "Epoch 52/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0351 - accuracy: 0.9892 - val_loss: 2.6629 - val_accuracy: 0.3500\n",
      "Epoch 53/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0145 - accuracy: 0.9982 - val_loss: 1.6186 - val_accuracy: 0.7167\n",
      "Epoch 54/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0163 - accuracy: 1.0000 - val_loss: 1.5133 - val_accuracy: 0.7167\n",
      "Epoch 55/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0100 - accuracy: 1.0000 - val_loss: 1.3580 - val_accuracy: 0.6833\n",
      "Epoch 56/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.3334 - val_accuracy: 0.7667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3843 - val_accuracy: 0.7667\n",
      "Epoch 58/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0163 - accuracy: 0.9982 - val_loss: 2.1556 - val_accuracy: 0.7167\n",
      "Epoch 59/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.4926 - val_accuracy: 0.7333\n",
      "Epoch 60/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0121 - accuracy: 0.9982 - val_loss: 1.2988 - val_accuracy: 0.7667\n",
      "Epoch 61/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.4231 - val_accuracy: 0.7167\n",
      "Epoch 62/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.5270 - val_accuracy: 0.7667\n",
      "Epoch 63/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3631 - val_accuracy: 0.7833\n",
      "Epoch 64/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.2717 - val_accuracy: 0.7500\n",
      "Epoch 65/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.5188 - val_accuracy: 0.7667\n",
      "Epoch 66/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.3581 - val_accuracy: 0.7667\n",
      "Epoch 67/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.3337 - val_accuracy: 0.7500\n",
      "Epoch 68/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.4102 - val_accuracy: 0.7500\n",
      "Epoch 69/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3417 - val_accuracy: 0.7500\n",
      "Epoch 70/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2745 - val_accuracy: 0.7500\n",
      "Epoch 71/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.2817 - val_accuracy: 0.7667\n",
      "Epoch 72/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.3072 - val_accuracy: 0.7500\n",
      "Epoch 73/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.3133 - val_accuracy: 0.7667\n",
      "Epoch 74/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.3172 - val_accuracy: 0.7667\n",
      "Epoch 75/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.3570 - val_accuracy: 0.7667\n",
      "Epoch 76/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.3633 - val_accuracy: 0.7500\n",
      "Epoch 77/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.3057 - val_accuracy: 0.7500\n",
      "Epoch 78/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.3569 - val_accuracy: 0.7667\n",
      "Epoch 79/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.3421 - val_accuracy: 0.7667\n",
      "Epoch 80/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.3022 - val_accuracy: 0.7500\n",
      "Epoch 81/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.3589 - val_accuracy: 0.7667\n",
      "Epoch 82/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.3753 - val_accuracy: 0.7667\n",
      "Epoch 83/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.3825 - val_accuracy: 0.7667\n",
      "Epoch 84/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.4089 - val_accuracy: 0.7667\n",
      "Epoch 85/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.4975 - val_accuracy: 0.7667\n",
      "Epoch 86/100\n",
      "557/557 [==============================] - 1s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.4945 - val_accuracy: 0.7500\n",
      "Epoch 87/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 8.4314e-04 - accuracy: 1.0000 - val_loss: 1.4590 - val_accuracy: 0.7500\n",
      "Epoch 88/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.4742 - val_accuracy: 0.7500\n",
      "Epoch 89/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.6545 - val_accuracy: 0.7167\n",
      "Epoch 90/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.4336 - val_accuracy: 0.7667\n",
      "Epoch 91/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.5755 - val_accuracy: 0.7500\n",
      "Epoch 92/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.4924 - val_accuracy: 0.7500\n",
      "Epoch 93/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.6047 - val_accuracy: 0.7333\n",
      "Epoch 94/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.6726 - val_accuracy: 0.7667\n",
      "Epoch 95/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.5443 - val_accuracy: 0.7333\n",
      "Epoch 96/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.5076 - val_accuracy: 0.7500\n",
      "Epoch 97/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 8.1033e-04 - accuracy: 1.0000 - val_loss: 1.4492 - val_accuracy: 0.7500\n",
      "Epoch 98/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.4834 - val_accuracy: 0.7333\n",
      "Epoch 99/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.6294 - val_accuracy: 0.7500\n",
      "Epoch 100/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.5631 - val_accuracy: 0.7500\n",
      "accuracy for model 10 is 75.0\n",
      "Training Testing Accuracy: 82.89% (9.58%)\n"
     ]
    }
   ],
   "source": [
    "best_CNN = eval_cnn(tt_vcf, tt_pheno, 10, mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4b2685813010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_CNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PoC_CNN_model.pickle.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(best_CNN, open(\"PoC_CNN_model.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout accuracy is 71.61290049552917\n"
     ]
    }
   ],
   "source": [
    "bs = ((ho_vcf.shape[0])/40)\n",
    "bs = round(bs)\n",
    "ho_vcf = ho_vcf.reshape(ho_vcf.shape[0], ho_vcf.shape[1],1)\n",
    "ho_pheno = mlb.transform(ho_pheno)\n",
    "_, accuracy = best_CNN.evaluate(ho_vcf, ho_pheno, batch_size=bs, verbose=0)\n",
    "print(\"Holdout accuracy is \" + str(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN (based off yield prediction paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "(617,)\n",
      "(617, 1)\n",
      "60000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "(155,)\n",
      "(155, 1)\n",
      "60000\n",
      "(617, 58574)\n",
      "(155, 58574)\n",
      "(617, 1)\n",
      "0.0\n",
      "(617, 1)\n",
      "(155, 1)\n"
     ]
    }
   ],
   "source": [
    "tt_vcf, ho_vcf, tt_pheno, ho_pheno = new_prep_data(\"PoC_Merged_filtered.csv_train_testQTL_SNPS.csv\", \"PoC_Merged_filtered.csv_holdoutQTL_SNPS.csv\")\n",
    "r_t = tt_pheno.ravel()\n",
    "r_h = ho_pheno.ravel()\n",
    "print(r_t[10])\n",
    "i = 0\n",
    "for x in r_t:\n",
    "    if(x==0.5):\n",
    "        r_t[i]=2.0\n",
    "    i = i+1\n",
    "i = 0\n",
    "for x in r_h:\n",
    "    if(x==0.5):\n",
    "        r_h[i]=2.0\n",
    "    i = i+1\n",
    "r_t = np.reshape(r_t,(len(r_t),1))\n",
    "r_h = np.reshape(r_h,(len(r_h),1))\n",
    "tt_pheno = r_t\n",
    "ho_pheno = r_h\n",
    "print(tt_pheno.shape)\n",
    "print(ho_pheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 170396)\n",
      "(155, 58574)\n",
      "(155, 170396)\n"
     ]
    }
   ],
   "source": [
    "ohe = pickle.load(open(\"PoC_QTL_ohe.dat\", \"rb\"))\n",
    "tt_vcf = ohe.transform(tt_vcf)\n",
    "print(tt_vcf.shape)\n",
    "print(ho_vcf.shape)\n",
    "ho_vcf = ohe.transform(ho_vcf)\n",
    "print(ho_vcf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##how to mlb both tt and ho for same scheme? do i even need to?\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb = mlb.fit(tt_pheno)\n",
    "##print(tt_pheno.shape)\n",
    "#print(ho_pheno.shape)\n",
    "#tt_pheno = mlb.transform(tt_pheno)\n",
    "#print(tt_pheno.shape)\n",
    "#ho_pheno = mlb.transform(ho_pheno)\n",
    "#print(ho_pheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My own DNN model based upon paper\n",
    "#del model #incase its stored a previous model\n",
    "#del history #for redoing shit\n",
    "\n",
    "#do batch size as 64\n",
    "#reduce the inputs by half when you read it in\n",
    "#add XGboost and RF to the one notebook\n",
    "def build_DNN_model(x_len):\n",
    "    model = Sequential()\n",
    "\n",
    "    #add first input layer, with no normalization\n",
    "    model.add(Dense(192, input_dim = x_len))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.03))\n",
    "    \n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.02))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    #add output layer\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    opt = tf.keras.optimizers.Adamax(learning_rate=0.003)#, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dnn(x,y,k,mlb):\n",
    "    cv = StratifiedKFold(n_splits=k,shuffle=False)\n",
    "    best_model = []\n",
    "    results = []\n",
    "    highest = 0\n",
    "    i = 1\n",
    "    for train,test in cv.split(x,y):\n",
    "        print(y.shape)\n",
    "        print(y[train])\n",
    "        if(i==1):\n",
    "            y = mlb.transform(y)\n",
    "            print(y.shape)\n",
    "            print(y[train])\n",
    "        model = build_DNN_model(x[train].shape[1])\n",
    "        bs = ((x[train].shape[0])/20)\n",
    "        bs = round(bs)\n",
    "        history = model.fit(x[train], y[train], validation_data=(x[test], y[test]), epochs=100, batch_size=bs)\n",
    "        _, accuracy = model.evaluate(x[test], y[test], batch_size=bs, verbose=0)\n",
    "        accuracy = accuracy *100\n",
    "        print(\"accuracy for model \" + str(i) + \" is \" + str(accuracy))\n",
    "        if(accuracy > highest):\n",
    "            highest = accuracy\n",
    "            best_model = model\n",
    "        results.append(accuracy)\n",
    "        del model\n",
    "        i = i + 1\n",
    "    print(\"Training Testing Accuracy: %.2f%% (%.2f%%)\" % (np.mean(results), np.std(results))) \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 1)\n",
      "[[1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]]\n",
      "(617, 3)\n",
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 192)               32716224  \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 32,751,971\n",
      "Trainable params: 32,751,907\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 1s 2ms/sample - loss: 0.8998 - accuracy: 0.6191 - val_loss: 7.2462 - val_accuracy: 0.1746\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.7147 - accuracy: 0.7058 - val_loss: 3.1213 - val_accuracy: 0.3016\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6929 - accuracy: 0.7202 - val_loss: 0.9993 - val_accuracy: 0.5556\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6726 - accuracy: 0.7184 - val_loss: 0.8699 - val_accuracy: 0.6508\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6500 - accuracy: 0.7238 - val_loss: 0.8947 - val_accuracy: 0.6032\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6361 - accuracy: 0.7310 - val_loss: 0.7876 - val_accuracy: 0.6190\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6034 - accuracy: 0.7365 - val_loss: 0.8115 - val_accuracy: 0.5714\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.5659 - accuracy: 0.7671 - val_loss: 0.6946 - val_accuracy: 0.7460\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.5139 - accuracy: 0.8069 - val_loss: 0.6579 - val_accuracy: 0.7460\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.4742 - accuracy: 0.8303 - val_loss: 0.6435 - val_accuracy: 0.8095\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.4242 - accuracy: 0.8592 - val_loss: 0.6210 - val_accuracy: 0.8095\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.3508 - accuracy: 0.8718 - val_loss: 1.4937 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.3364 - accuracy: 0.8845 - val_loss: 0.7013 - val_accuracy: 0.6825\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.3087 - accuracy: 0.8863 - val_loss: 0.9283 - val_accuracy: 0.7937\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.3032 - accuracy: 0.8953 - val_loss: 0.6546 - val_accuracy: 0.8254\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2246 - accuracy: 0.9404 - val_loss: 0.6473 - val_accuracy: 0.7937\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2135 - accuracy: 0.9242 - val_loss: 0.7260 - val_accuracy: 0.8571\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1621 - accuracy: 0.9458 - val_loss: 0.7847 - val_accuracy: 0.8254\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1620 - accuracy: 0.9440 - val_loss: 0.6953 - val_accuracy: 0.8254\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1234 - accuracy: 0.9603 - val_loss: 0.7783 - val_accuracy: 0.8254\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1182 - accuracy: 0.9549 - val_loss: 0.6449 - val_accuracy: 0.8413\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1069 - accuracy: 0.9693 - val_loss: 1.1012 - val_accuracy: 0.8095\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0994 - accuracy: 0.9639 - val_loss: 1.0744 - val_accuracy: 0.8095\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0616 - accuracy: 0.9801 - val_loss: 0.8552 - val_accuracy: 0.8254\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1021 - accuracy: 0.9621 - val_loss: 0.8086 - val_accuracy: 0.7619\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0648 - accuracy: 0.9801 - val_loss: 0.7633 - val_accuracy: 0.8730\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0575 - accuracy: 0.9892 - val_loss: 1.3765 - val_accuracy: 0.7937\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0324 - accuracy: 0.9946 - val_loss: 0.8369 - val_accuracy: 0.7937\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0378 - accuracy: 0.9874 - val_loss: 0.8531 - val_accuracy: 0.8413\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0430 - accuracy: 0.9892 - val_loss: 0.7129 - val_accuracy: 0.8254\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0427 - accuracy: 0.9856 - val_loss: 0.6068 - val_accuracy: 0.8730\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0222 - accuracy: 0.9964 - val_loss: 0.8783 - val_accuracy: 0.8413\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0297 - accuracy: 0.9892 - val_loss: 1.1349 - val_accuracy: 0.8095\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0422 - accuracy: 0.9928 - val_loss: 1.0696 - val_accuracy: 0.7302\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0349 - accuracy: 0.9910 - val_loss: 0.9412 - val_accuracy: 0.8413\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0294 - accuracy: 0.9874 - val_loss: 0.9761 - val_accuracy: 0.8413\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0270 - accuracy: 0.9892 - val_loss: 0.7368 - val_accuracy: 0.8571\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0162 - accuracy: 0.9964 - val_loss: 1.2688 - val_accuracy: 0.8254\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0148 - accuracy: 0.9964 - val_loss: 0.8388 - val_accuracy: 0.8413\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0224 - accuracy: 0.9928 - val_loss: 1.4004 - val_accuracy: 0.8095\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0248 - accuracy: 0.9946 - val_loss: 0.9663 - val_accuracy: 0.8095\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0136 - accuracy: 0.9982 - val_loss: 0.8404 - val_accuracy: 0.8254\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0145 - accuracy: 0.9964 - val_loss: 1.1745 - val_accuracy: 0.8095\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0277 - accuracy: 0.9892 - val_loss: 0.9266 - val_accuracy: 0.8254\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0354 - accuracy: 0.9910 - val_loss: 1.0274 - val_accuracy: 0.8254\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0487 - accuracy: 0.9765 - val_loss: 1.2349 - val_accuracy: 0.8254\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0307 - accuracy: 0.9910 - val_loss: 1.0305 - val_accuracy: 0.7302\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0184 - accuracy: 0.9928 - val_loss: 0.8546 - val_accuracy: 0.8571\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0335 - accuracy: 0.9910 - val_loss: 0.9997 - val_accuracy: 0.8413\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0377 - accuracy: 0.9838 - val_loss: 1.5027 - val_accuracy: 0.6667\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0177 - accuracy: 0.9946 - val_loss: 1.0551 - val_accuracy: 0.8413\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0194 - accuracy: 0.9928 - val_loss: 1.0148 - val_accuracy: 0.8413\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0288 - accuracy: 0.9928 - val_loss: 1.4531 - val_accuracy: 0.8254\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0204 - accuracy: 0.9928 - val_loss: 0.5919 - val_accuracy: 0.8413\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0111 - accuracy: 0.9964 - val_loss: 0.9709 - val_accuracy: 0.8730\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0177 - accuracy: 0.9928 - val_loss: 1.4119 - val_accuracy: 0.6825\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0273 - accuracy: 0.9910 - val_loss: 1.5026 - val_accuracy: 0.8095\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0164 - accuracy: 0.9964 - val_loss: 1.0515 - val_accuracy: 0.8413\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0214 - accuracy: 0.9946 - val_loss: 1.1542 - val_accuracy: 0.8254\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0154 - accuracy: 0.9910 - val_loss: 1.0434 - val_accuracy: 0.8730\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.9427 - val_accuracy: 0.8413\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0106 - accuracy: 0.9946 - val_loss: 1.1284 - val_accuracy: 0.8413\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.1257 - val_accuracy: 0.8254\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0154 - accuracy: 0.9946 - val_loss: 0.9382 - val_accuracy: 0.7937\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0198 - accuracy: 0.9946 - val_loss: 1.1825 - val_accuracy: 0.7460\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0078 - accuracy: 0.9982 - val_loss: 1.1675 - val_accuracy: 0.8413\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.9779 - val_accuracy: 0.8571\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0068 - accuracy: 0.9982 - val_loss: 1.1021 - val_accuracy: 0.8413\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0068 - accuracy: 0.9982 - val_loss: 1.2081 - val_accuracy: 0.8095\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0055 - accuracy: 0.9964 - val_loss: 0.9306 - val_accuracy: 0.8571\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.1344 - val_accuracy: 0.8413\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0118 - accuracy: 0.9946 - val_loss: 1.3192 - val_accuracy: 0.7460\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0075 - accuracy: 0.9964 - val_loss: 1.1721 - val_accuracy: 0.8571\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0209 - accuracy: 0.9910 - val_loss: 1.0755 - val_accuracy: 0.8254\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0093 - accuracy: 0.9982 - val_loss: 1.1184 - val_accuracy: 0.8413\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0137 - accuracy: 0.9928 - val_loss: 1.0072 - val_accuracy: 0.8571\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0087 - accuracy: 0.9982 - val_loss: 0.9741 - val_accuracy: 0.8730\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.0355 - val_accuracy: 0.8571\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.0944 - val_accuracy: 0.8413\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.1643 - val_accuracy: 0.8571\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.1820 - val_accuracy: 0.8730\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.2757 - val_accuracy: 0.8413\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0174 - accuracy: 0.9910 - val_loss: 1.9972 - val_accuracy: 0.7460\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0224 - accuracy: 0.9946 - val_loss: 2.2709 - val_accuracy: 0.5556\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0367 - accuracy: 0.9874 - val_loss: 1.2963 - val_accuracy: 0.8254\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0290 - accuracy: 0.9892 - val_loss: 1.4225 - val_accuracy: 0.8413\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0427 - accuracy: 0.9801 - val_loss: 3.0542 - val_accuracy: 0.4921\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0134 - accuracy: 0.9928 - val_loss: 1.5010 - val_accuracy: 0.8413\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.9990 - val_accuracy: 0.8571\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0120 - accuracy: 0.9982 - val_loss: 0.9485 - val_accuracy: 0.8730\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0116 - accuracy: 0.9964 - val_loss: 1.1398 - val_accuracy: 0.8254\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.9977 - val_accuracy: 0.8889\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.0931 - val_accuracy: 0.8571\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0110 - accuracy: 0.9946 - val_loss: 0.7741 - val_accuracy: 0.8730\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0054 - accuracy: 0.9964 - val_loss: 0.9932 - val_accuracy: 0.8413\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.9237 - val_accuracy: 0.8571\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0074 - accuracy: 0.9982 - val_loss: 1.2464 - val_accuracy: 0.8571\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0042 - accuracy: 0.9982 - val_loss: 0.9074 - val_accuracy: 0.8571\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.7980 - val_accuracy: 0.9048\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.9314 - val_accuracy: 0.8571\n",
      "accuracy for model 1 is 85.71428656578064\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 192)               32716224  \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 32,751,971\n",
      "Trainable params: 32,751,907\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 1s 2ms/sample - loss: 0.9837 - accuracy: 0.5596 - val_loss: 3.2286 - val_accuracy: 0.7143\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.8124 - accuracy: 0.6697 - val_loss: 2.6358 - val_accuracy: 0.7302\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.7596 - accuracy: 0.6931 - val_loss: 2.0486 - val_accuracy: 0.7302\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.7203 - accuracy: 0.6841 - val_loss: 1.2643 - val_accuracy: 0.7302\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6690 - accuracy: 0.6877 - val_loss: 1.0847 - val_accuracy: 0.7302\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6758 - accuracy: 0.7094 - val_loss: 0.6055 - val_accuracy: 0.7460\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6021 - accuracy: 0.7347 - val_loss: 0.5410 - val_accuracy: 0.7619\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.5444 - accuracy: 0.7635 - val_loss: 0.5669 - val_accuracy: 0.7460\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.4965 - accuracy: 0.7924 - val_loss: 0.5166 - val_accuracy: 0.8095\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.4921 - accuracy: 0.8321 - val_loss: 0.5937 - val_accuracy: 0.7937\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.4406 - accuracy: 0.8357 - val_loss: 0.6456 - val_accuracy: 0.6667\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.3921 - accuracy: 0.8574 - val_loss: 0.5741 - val_accuracy: 0.8254\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.3388 - accuracy: 0.8736 - val_loss: 0.8270 - val_accuracy: 0.7460\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.3332 - accuracy: 0.8899 - val_loss: 0.7415 - val_accuracy: 0.7302\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2962 - accuracy: 0.8827 - val_loss: 0.8598 - val_accuracy: 0.7302\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2624 - accuracy: 0.8971 - val_loss: 0.6794 - val_accuracy: 0.8254\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2293 - accuracy: 0.9079 - val_loss: 0.6122 - val_accuracy: 0.7937\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2059 - accuracy: 0.9206 - val_loss: 0.6826 - val_accuracy: 0.8571\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1814 - accuracy: 0.9368 - val_loss: 0.7014 - val_accuracy: 0.7619\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1719 - accuracy: 0.9422 - val_loss: 0.7281 - val_accuracy: 0.7619\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1850 - accuracy: 0.9350 - val_loss: 0.7049 - val_accuracy: 0.8254\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1595 - accuracy: 0.9386 - val_loss: 0.8205 - val_accuracy: 0.6984\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1373 - accuracy: 0.9585 - val_loss: 0.6741 - val_accuracy: 0.7302\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1145 - accuracy: 0.9621 - val_loss: 0.7250 - val_accuracy: 0.7778\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0966 - accuracy: 0.9729 - val_loss: 0.8925 - val_accuracy: 0.7619\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0983 - accuracy: 0.9675 - val_loss: 0.8770 - val_accuracy: 0.8254\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0756 - accuracy: 0.9801 - val_loss: 1.0894 - val_accuracy: 0.6667\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0994 - accuracy: 0.9693 - val_loss: 1.3032 - val_accuracy: 0.7619\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0893 - accuracy: 0.9657 - val_loss: 3.5798 - val_accuracy: 0.4127\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0891 - accuracy: 0.9693 - val_loss: 0.8981 - val_accuracy: 0.7937\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0775 - accuracy: 0.9783 - val_loss: 0.9059 - val_accuracy: 0.8254\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0960 - accuracy: 0.9657 - val_loss: 0.7744 - val_accuracy: 0.7619\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0868 - accuracy: 0.9711 - val_loss: 0.9160 - val_accuracy: 0.8095\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0743 - accuracy: 0.9693 - val_loss: 0.8381 - val_accuracy: 0.8254\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0502 - accuracy: 0.9856 - val_loss: 0.7777 - val_accuracy: 0.7302\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0359 - accuracy: 0.9892 - val_loss: 0.8891 - val_accuracy: 0.7302\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0342 - accuracy: 0.9910 - val_loss: 1.0449 - val_accuracy: 0.8254\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0227 - accuracy: 0.9946 - val_loss: 1.0243 - val_accuracy: 0.8254\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0351 - accuracy: 0.9928 - val_loss: 1.0491 - val_accuracy: 0.8095\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0559 - accuracy: 0.9783 - val_loss: 1.0111 - val_accuracy: 0.7937\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0294 - accuracy: 0.9928 - val_loss: 1.1417 - val_accuracy: 0.6667\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0302 - accuracy: 0.9892 - val_loss: 1.1297 - val_accuracy: 0.7937\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0205 - accuracy: 0.9982 - val_loss: 1.0236 - val_accuracy: 0.7460\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0205 - accuracy: 0.9946 - val_loss: 1.0855 - val_accuracy: 0.7619\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0263 - accuracy: 0.9910 - val_loss: 1.1998 - val_accuracy: 0.7302\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0320 - accuracy: 0.9892 - val_loss: 0.9561 - val_accuracy: 0.7937\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0349 - accuracy: 0.9856 - val_loss: 0.9813 - val_accuracy: 0.7778\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0363 - accuracy: 0.9801 - val_loss: 1.6276 - val_accuracy: 0.7619\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0494 - accuracy: 0.9856 - val_loss: 1.1014 - val_accuracy: 0.6984\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0599 - accuracy: 0.9729 - val_loss: 2.1282 - val_accuracy: 0.5556\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0561 - accuracy: 0.9765 - val_loss: 0.8439 - val_accuracy: 0.8095\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0305 - accuracy: 0.9928 - val_loss: 1.1873 - val_accuracy: 0.6508\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0130 - accuracy: 0.9982 - val_loss: 0.9157 - val_accuracy: 0.7778\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0359 - accuracy: 0.9838 - val_loss: 1.2296 - val_accuracy: 0.8095\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0130 - accuracy: 0.9982 - val_loss: 1.0737 - val_accuracy: 0.7302\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0239 - accuracy: 0.9928 - val_loss: 0.9795 - val_accuracy: 0.7937\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0279 - accuracy: 0.9928 - val_loss: 1.3532 - val_accuracy: 0.7937\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0263 - accuracy: 0.9928 - val_loss: 1.1155 - val_accuracy: 0.7143\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0129 - accuracy: 0.9982 - val_loss: 1.0994 - val_accuracy: 0.8095\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0156 - accuracy: 0.9964 - val_loss: 1.4615 - val_accuracy: 0.6825\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0268 - accuracy: 0.9910 - val_loss: 1.1511 - val_accuracy: 0.8095\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0412 - accuracy: 0.9892 - val_loss: 1.6242 - val_accuracy: 0.7778\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0376 - accuracy: 0.9856 - val_loss: 0.8068 - val_accuracy: 0.8095\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0104 - accuracy: 0.9982 - val_loss: 0.9804 - val_accuracy: 0.8254\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0284 - accuracy: 0.9910 - val_loss: 0.9259 - val_accuracy: 0.7937\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0551 - accuracy: 0.9783 - val_loss: 1.4107 - val_accuracy: 0.7937\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0233 - accuracy: 0.9910 - val_loss: 1.0436 - val_accuracy: 0.7937\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0396 - accuracy: 0.9892 - val_loss: 1.4791 - val_accuracy: 0.7778\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0139 - accuracy: 0.9964 - val_loss: 0.9876 - val_accuracy: 0.7778\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0121 - accuracy: 1.0000 - val_loss: 1.1377 - val_accuracy: 0.8095\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0236 - accuracy: 0.9928 - val_loss: 1.2135 - val_accuracy: 0.7937\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0088 - accuracy: 0.9982 - val_loss: 1.0682 - val_accuracy: 0.8254\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0211 - accuracy: 0.9928 - val_loss: 1.0544 - val_accuracy: 0.7619\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0088 - accuracy: 0.9964 - val_loss: 1.0567 - val_accuracy: 0.8254\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0251 - accuracy: 0.9892 - val_loss: 1.3975 - val_accuracy: 0.6508\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0233 - accuracy: 0.9910 - val_loss: 0.9420 - val_accuracy: 0.7937\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0360 - accuracy: 0.9838 - val_loss: 1.1906 - val_accuracy: 0.7937\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0196 - accuracy: 0.9964 - val_loss: 1.4204 - val_accuracy: 0.8254\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0269 - accuracy: 0.9892 - val_loss: 1.2176 - val_accuracy: 0.7937\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0339 - accuracy: 0.9892 - val_loss: 1.2301 - val_accuracy: 0.8254\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0173 - accuracy: 0.9910 - val_loss: 1.0605 - val_accuracy: 0.7460\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0186 - accuracy: 0.9964 - val_loss: 1.1236 - val_accuracy: 0.8254\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0194 - accuracy: 0.9928 - val_loss: 1.5526 - val_accuracy: 0.7619\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0166 - accuracy: 0.9964 - val_loss: 1.2467 - val_accuracy: 0.6667\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0089 - accuracy: 0.9964 - val_loss: 1.2998 - val_accuracy: 0.8254\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0126 - accuracy: 0.9946 - val_loss: 1.4176 - val_accuracy: 0.7778\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0119 - accuracy: 0.9928 - val_loss: 1.8579 - val_accuracy: 0.6190\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0107 - accuracy: 0.9946 - val_loss: 1.5189 - val_accuracy: 0.7778\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0086 - accuracy: 0.9964 - val_loss: 1.5553 - val_accuracy: 0.6825\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0078 - accuracy: 0.9982 - val_loss: 1.1708 - val_accuracy: 0.8095\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.1219 - val_accuracy: 0.8413\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0058 - accuracy: 0.9982 - val_loss: 1.0088 - val_accuracy: 0.7778\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0102 - accuracy: 0.9964 - val_loss: 1.1311 - val_accuracy: 0.8254\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0065 - accuracy: 0.9982 - val_loss: 1.1623 - val_accuracy: 0.8413\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0153 - accuracy: 0.9964 - val_loss: 1.8852 - val_accuracy: 0.6032\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0356 - accuracy: 0.9928 - val_loss: 1.3218 - val_accuracy: 0.8095\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0417 - accuracy: 0.9892 - val_loss: 1.0440 - val_accuracy: 0.7302\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0250 - accuracy: 0.9946 - val_loss: 1.0219 - val_accuracy: 0.7619\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0091 - accuracy: 0.9982 - val_loss: 0.9813 - val_accuracy: 0.8095\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0169 - accuracy: 0.9946 - val_loss: 1.3489 - val_accuracy: 0.7143\n",
      "accuracy for model 2 is 71.42857313156128\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_52 (Dense)             (None, 192)               32716224  \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 32,751,971\n",
      "Trainable params: 32,751,907\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 1s 2ms/sample - loss: 0.8956 - accuracy: 0.5939 - val_loss: 4.0526 - val_accuracy: 0.5873\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.7720 - accuracy: 0.6805 - val_loss: 1.9653 - val_accuracy: 0.7143\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.7318 - accuracy: 0.6895 - val_loss: 1.1105 - val_accuracy: 0.7143\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.7153 - accuracy: 0.7022 - val_loss: 0.7652 - val_accuracy: 0.7302\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6999 - accuracy: 0.6968 - val_loss: 0.6513 - val_accuracy: 0.7460\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.7017 - accuracy: 0.7022 - val_loss: 0.5391 - val_accuracy: 0.7619\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6611 - accuracy: 0.7148 - val_loss: 0.4798 - val_accuracy: 0.7460\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6376 - accuracy: 0.7274 - val_loss: 0.4621 - val_accuracy: 0.7460\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.5797 - accuracy: 0.7491 - val_loss: 0.4627 - val_accuracy: 0.8571\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.5470 - accuracy: 0.7780 - val_loss: 0.5608 - val_accuracy: 0.8095\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.5011 - accuracy: 0.7888 - val_loss: 0.5066 - val_accuracy: 0.7460\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.4614 - accuracy: 0.8321 - val_loss: 1.5963 - val_accuracy: 0.3492\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.3878 - accuracy: 0.8610 - val_loss: 0.3682 - val_accuracy: 0.8571\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.3648 - accuracy: 0.8791 - val_loss: 0.4155 - val_accuracy: 0.8571\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.3165 - accuracy: 0.8953 - val_loss: 0.3987 - val_accuracy: 0.8571\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2868 - accuracy: 0.8899 - val_loss: 0.6179 - val_accuracy: 0.7619\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2360 - accuracy: 0.9242 - val_loss: 0.4967 - val_accuracy: 0.7937\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2017 - accuracy: 0.9296 - val_loss: 0.3228 - val_accuracy: 0.9048\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1801 - accuracy: 0.9477 - val_loss: 0.3747 - val_accuracy: 0.8730\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1390 - accuracy: 0.9621 - val_loss: 0.3337 - val_accuracy: 0.8730\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1638 - accuracy: 0.9513 - val_loss: 0.6206 - val_accuracy: 0.7619\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1549 - accuracy: 0.9495 - val_loss: 0.5267 - val_accuracy: 0.8254\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1233 - accuracy: 0.9657 - val_loss: 0.3234 - val_accuracy: 0.8889\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1017 - accuracy: 0.9675 - val_loss: 0.6402 - val_accuracy: 0.8254\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0857 - accuracy: 0.9801 - val_loss: 0.4167 - val_accuracy: 0.9048\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1038 - accuracy: 0.9693 - val_loss: 0.4857 - val_accuracy: 0.8571\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0931 - accuracy: 0.9693 - val_loss: 0.4568 - val_accuracy: 0.8571\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0506 - accuracy: 0.9892 - val_loss: 0.3994 - val_accuracy: 0.8889\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0522 - accuracy: 0.9874 - val_loss: 0.4639 - val_accuracy: 0.8730\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0543 - accuracy: 0.9819 - val_loss: 1.3314 - val_accuracy: 0.6508\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0834 - accuracy: 0.9765 - val_loss: 0.6158 - val_accuracy: 0.7937\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0725 - accuracy: 0.9765 - val_loss: 1.1947 - val_accuracy: 0.7619\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0549 - accuracy: 0.9819 - val_loss: 0.3804 - val_accuracy: 0.8889\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0515 - accuracy: 0.9838 - val_loss: 0.6970 - val_accuracy: 0.8571\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0582 - accuracy: 0.9874 - val_loss: 0.7359 - val_accuracy: 0.7302\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1079 - accuracy: 0.9567 - val_loss: 0.3208 - val_accuracy: 0.8889\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0558 - accuracy: 0.9856 - val_loss: 0.4073 - val_accuracy: 0.8889\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1044 - accuracy: 0.9657 - val_loss: 0.6093 - val_accuracy: 0.8254\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0907 - accuracy: 0.9675 - val_loss: 0.4160 - val_accuracy: 0.8889\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0639 - accuracy: 0.9819 - val_loss: 0.3829 - val_accuracy: 0.8889\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0507 - accuracy: 0.9838 - val_loss: 0.3603 - val_accuracy: 0.8730\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0574 - accuracy: 0.9783 - val_loss: 0.5538 - val_accuracy: 0.8730\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0321 - accuracy: 0.9874 - val_loss: 0.4496 - val_accuracy: 0.8889\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0294 - accuracy: 0.9928 - val_loss: 0.5053 - val_accuracy: 0.8889\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0397 - accuracy: 0.9856 - val_loss: 0.4683 - val_accuracy: 0.8889\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0348 - accuracy: 0.9910 - val_loss: 0.4753 - val_accuracy: 0.9048\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0378 - accuracy: 0.9892 - val_loss: 0.4849 - val_accuracy: 0.8889\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0323 - accuracy: 0.9910 - val_loss: 0.4111 - val_accuracy: 0.9048\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0433 - accuracy: 0.9856 - val_loss: 0.4826 - val_accuracy: 0.9048\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0595 - accuracy: 0.9838 - val_loss: 0.4417 - val_accuracy: 0.8889\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0937 - accuracy: 0.9729 - val_loss: 0.9199 - val_accuracy: 0.8095\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0396 - accuracy: 0.9856 - val_loss: 0.5745 - val_accuracy: 0.8889\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0327 - accuracy: 0.9874 - val_loss: 0.4566 - val_accuracy: 0.8730\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0291 - accuracy: 0.9910 - val_loss: 0.7824 - val_accuracy: 0.8413\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0232 - accuracy: 0.9946 - val_loss: 0.6820 - val_accuracy: 0.8730\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0272 - accuracy: 0.9928 - val_loss: 0.5725 - val_accuracy: 0.9048\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0155 - accuracy: 0.9982 - val_loss: 0.4963 - val_accuracy: 0.8889\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0262 - accuracy: 0.9946 - val_loss: 0.4974 - val_accuracy: 0.9048\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0084 - accuracy: 0.9964 - val_loss: 0.4827 - val_accuracy: 0.9048\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0075 - accuracy: 0.9982 - val_loss: 0.4795 - val_accuracy: 0.8889\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0228 - accuracy: 0.9928 - val_loss: 0.6995 - val_accuracy: 0.8571\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0135 - accuracy: 0.9946 - val_loss: 0.5061 - val_accuracy: 0.8889\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0229 - accuracy: 0.9928 - val_loss: 0.5032 - val_accuracy: 0.9206\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0101 - accuracy: 0.9982 - val_loss: 0.4437 - val_accuracy: 0.9048\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0129 - accuracy: 0.9982 - val_loss: 0.4865 - val_accuracy: 0.8889\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.7580 - val_accuracy: 0.8571\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0140 - accuracy: 0.9946 - val_loss: 0.5362 - val_accuracy: 0.8889\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0194 - accuracy: 0.9910 - val_loss: 0.7035 - val_accuracy: 0.8730\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.4942 - val_accuracy: 0.9048\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.7956 - val_accuracy: 0.8571\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0144 - accuracy: 0.9964 - val_loss: 0.6452 - val_accuracy: 0.8889\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0086 - accuracy: 0.9982 - val_loss: 0.7550 - val_accuracy: 0.8413\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0417 - accuracy: 0.9819 - val_loss: 0.4946 - val_accuracy: 0.9048\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0266 - accuracy: 0.9910 - val_loss: 0.5508 - val_accuracy: 0.9048\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0422 - accuracy: 0.9856 - val_loss: 0.9882 - val_accuracy: 0.8254\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0131 - accuracy: 0.9964 - val_loss: 0.6611 - val_accuracy: 0.8730\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0159 - accuracy: 0.9946 - val_loss: 0.5166 - val_accuracy: 0.8889\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0278 - accuracy: 0.9892 - val_loss: 0.4601 - val_accuracy: 0.9048\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0152 - accuracy: 0.9946 - val_loss: 0.4022 - val_accuracy: 0.9048\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0187 - accuracy: 0.9946 - val_loss: 0.4565 - val_accuracy: 0.8889\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0245 - accuracy: 0.9910 - val_loss: 0.4550 - val_accuracy: 0.8889\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0113 - accuracy: 0.9982 - val_loss: 0.7052 - val_accuracy: 0.8730\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0131 - accuracy: 0.9964 - val_loss: 0.5302 - val_accuracy: 0.8730\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.6123 - val_accuracy: 0.8730\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0165 - accuracy: 0.9946 - val_loss: 0.5549 - val_accuracy: 0.9206\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.5161 - val_accuracy: 0.9048\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0137 - accuracy: 0.9946 - val_loss: 0.6870 - val_accuracy: 0.8889\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0114 - accuracy: 0.9946 - val_loss: 0.5852 - val_accuracy: 0.9048\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0258 - accuracy: 0.9928 - val_loss: 0.6489 - val_accuracy: 0.8889\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0163 - accuracy: 0.9946 - val_loss: 0.8050 - val_accuracy: 0.8730\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0264 - accuracy: 0.9910 - val_loss: 0.6818 - val_accuracy: 0.8889\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0126 - accuracy: 0.9964 - val_loss: 0.5916 - val_accuracy: 0.9048\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0314 - accuracy: 0.9856 - val_loss: 0.5295 - val_accuracy: 0.8889\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0304 - accuracy: 0.9910 - val_loss: 0.5908 - val_accuracy: 0.8730\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0171 - accuracy: 0.9946 - val_loss: 0.4569 - val_accuracy: 0.9206\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0107 - accuracy: 0.9964 - val_loss: 1.0252 - val_accuracy: 0.8413\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0120 - accuracy: 0.9964 - val_loss: 0.5508 - val_accuracy: 0.9048\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0128 - accuracy: 0.9946 - val_loss: 0.8770 - val_accuracy: 0.8095\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0211 - accuracy: 0.9928 - val_loss: 0.6517 - val_accuracy: 0.8889\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0079 - accuracy: 0.9964 - val_loss: 0.7405 - val_accuracy: 0.8571\n",
      "accuracy for model 3 is 85.71428656578064\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_58 (Dense)             (None, 192)               32716224  \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 32,751,971\n",
      "Trainable params: 32,751,907\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 1s 2ms/sample - loss: 0.8908 - accuracy: 0.6011 - val_loss: 1.3412 - val_accuracy: 0.5714\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.7439 - accuracy: 0.6841 - val_loss: 1.0165 - val_accuracy: 0.7302\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.7194 - accuracy: 0.6841 - val_loss: 1.0564 - val_accuracy: 0.7302\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6821 - accuracy: 0.7058 - val_loss: 0.8494 - val_accuracy: 0.7619\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6474 - accuracy: 0.7220 - val_loss: 0.3975 - val_accuracy: 0.7778\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.6314 - accuracy: 0.7365 - val_loss: 0.4878 - val_accuracy: 0.7619\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.5856 - accuracy: 0.7563 - val_loss: 0.4569 - val_accuracy: 0.7619\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.5387 - accuracy: 0.7942 - val_loss: 0.4168 - val_accuracy: 0.8571\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.4732 - accuracy: 0.8213 - val_loss: 0.3596 - val_accuracy: 0.8730\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.4570 - accuracy: 0.8069 - val_loss: 0.4607 - val_accuracy: 0.7778\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.4168 - accuracy: 0.8303 - val_loss: 0.4198 - val_accuracy: 0.8095\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.3160 - accuracy: 0.8773 - val_loss: 0.6675 - val_accuracy: 0.7143\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.3472 - accuracy: 0.8592 - val_loss: 0.4503 - val_accuracy: 0.8571\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2711 - accuracy: 0.9097 - val_loss: 0.4576 - val_accuracy: 0.7619\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2773 - accuracy: 0.8899 - val_loss: 0.5224 - val_accuracy: 0.7619\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2737 - accuracy: 0.8863 - val_loss: 0.7693 - val_accuracy: 0.6667\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2418 - accuracy: 0.9152 - val_loss: 0.5193 - val_accuracy: 0.7460\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.2038 - accuracy: 0.9332 - val_loss: 0.3454 - val_accuracy: 0.8730\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1748 - accuracy: 0.9422 - val_loss: 0.4712 - val_accuracy: 0.7937\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1529 - accuracy: 0.9458 - val_loss: 0.2991 - val_accuracy: 0.8889\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1258 - accuracy: 0.9567 - val_loss: 0.3459 - val_accuracy: 0.8413\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1872 - accuracy: 0.9314 - val_loss: 0.7613 - val_accuracy: 0.7302\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1266 - accuracy: 0.9549 - val_loss: 0.2894 - val_accuracy: 0.8730\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1205 - accuracy: 0.9567 - val_loss: 0.4704 - val_accuracy: 0.8413\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0676 - accuracy: 0.9856 - val_loss: 0.3396 - val_accuracy: 0.8413\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.1058 - accuracy: 0.9711 - val_loss: 0.2471 - val_accuracy: 0.9048\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0865 - accuracy: 0.9639 - val_loss: 0.2912 - val_accuracy: 0.9048\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0785 - accuracy: 0.9765 - val_loss: 0.6891 - val_accuracy: 0.7619\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0753 - accuracy: 0.9838 - val_loss: 0.4749 - val_accuracy: 0.7937\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0829 - accuracy: 0.9765 - val_loss: 0.4383 - val_accuracy: 0.8413\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0754 - accuracy: 0.9747 - val_loss: 0.7196 - val_accuracy: 0.7937\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0553 - accuracy: 0.9765 - val_loss: 1.0040 - val_accuracy: 0.7302\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0583 - accuracy: 0.9819 - val_loss: 0.3731 - val_accuracy: 0.8730\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0356 - accuracy: 0.9892 - val_loss: 0.5161 - val_accuracy: 0.8413\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0583 - accuracy: 0.9819 - val_loss: 0.9574 - val_accuracy: 0.7460\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0532 - accuracy: 0.9819 - val_loss: 0.7114 - val_accuracy: 0.7778\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0831 - accuracy: 0.9729 - val_loss: 1.0471 - val_accuracy: 0.7619\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0601 - accuracy: 0.9801 - val_loss: 0.3884 - val_accuracy: 0.8889\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0350 - accuracy: 0.9874 - val_loss: 0.8643 - val_accuracy: 0.7778\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0430 - accuracy: 0.9838 - val_loss: 0.8861 - val_accuracy: 0.7302\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0300 - accuracy: 0.9856 - val_loss: 0.4887 - val_accuracy: 0.8254\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0579 - accuracy: 0.9783 - val_loss: 0.4055 - val_accuracy: 0.8730\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0334 - accuracy: 0.9910 - val_loss: 0.4846 - val_accuracy: 0.8254\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0290 - accuracy: 0.9928 - val_loss: 0.3883 - val_accuracy: 0.8571\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0340 - accuracy: 0.9819 - val_loss: 0.5674 - val_accuracy: 0.8413\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0490 - accuracy: 0.9874 - val_loss: 0.6645 - val_accuracy: 0.7937\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0176 - accuracy: 0.9946 - val_loss: 0.6723 - val_accuracy: 0.8095\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0195 - accuracy: 0.9964 - val_loss: 0.5559 - val_accuracy: 0.8571\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0280 - accuracy: 0.9946 - val_loss: 0.3021 - val_accuracy: 0.9206\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0261 - accuracy: 0.9910 - val_loss: 0.3150 - val_accuracy: 0.9206\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0120 - accuracy: 0.9946 - val_loss: 0.5789 - val_accuracy: 0.8254\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0324 - accuracy: 0.9910 - val_loss: 0.7768 - val_accuracy: 0.7937\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0316 - accuracy: 0.9910 - val_loss: 0.7875 - val_accuracy: 0.7619\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0229 - accuracy: 0.9910 - val_loss: 0.4939 - val_accuracy: 0.8889\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0278 - accuracy: 0.9928 - val_loss: 0.4550 - val_accuracy: 0.8889\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0239 - accuracy: 0.9910 - val_loss: 0.4717 - val_accuracy: 0.8571\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0342 - accuracy: 0.9892 - val_loss: 0.4525 - val_accuracy: 0.8730\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0164 - accuracy: 0.9946 - val_loss: 0.7253 - val_accuracy: 0.8095\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0358 - accuracy: 0.9838 - val_loss: 0.5715 - val_accuracy: 0.8571\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0436 - accuracy: 0.9819 - val_loss: 0.9355 - val_accuracy: 0.7778\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0142 - accuracy: 0.9964 - val_loss: 0.7405 - val_accuracy: 0.8095\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0164 - accuracy: 0.9946 - val_loss: 0.4814 - val_accuracy: 0.8730\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0364 - accuracy: 0.9928 - val_loss: 0.9668 - val_accuracy: 0.8095\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0899 - accuracy: 0.9783 - val_loss: 0.8820 - val_accuracy: 0.7619\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0584 - accuracy: 0.9783 - val_loss: 0.4308 - val_accuracy: 0.8254\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0278 - accuracy: 0.9910 - val_loss: 0.6825 - val_accuracy: 0.8095\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0368 - accuracy: 0.9856 - val_loss: 0.4074 - val_accuracy: 0.8730\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0204 - accuracy: 0.9964 - val_loss: 0.4436 - val_accuracy: 0.8889\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0254 - accuracy: 0.9928 - val_loss: 0.4729 - val_accuracy: 0.8571\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0296 - accuracy: 0.9910 - val_loss: 0.5155 - val_accuracy: 0.8254\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0201 - accuracy: 0.9964 - val_loss: 0.4757 - val_accuracy: 0.8730\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0310 - accuracy: 0.9874 - val_loss: 0.3849 - val_accuracy: 0.8730\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0271 - accuracy: 0.9892 - val_loss: 0.4833 - val_accuracy: 0.8571\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0188 - accuracy: 0.9928 - val_loss: 0.6772 - val_accuracy: 0.8889\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0143 - accuracy: 0.9964 - val_loss: 0.4872 - val_accuracy: 0.8571\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0113 - accuracy: 0.9982 - val_loss: 0.6846 - val_accuracy: 0.8095\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.5025 - val_accuracy: 0.8413\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0275 - accuracy: 0.9928 - val_loss: 0.5986 - val_accuracy: 0.8254\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.4015 - val_accuracy: 0.8730\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0162 - accuracy: 0.9910 - val_loss: 0.4821 - val_accuracy: 0.8889\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.4809 - val_accuracy: 0.8730\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.7268 - val_accuracy: 0.8095\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.6384 - val_accuracy: 0.8095\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0208 - accuracy: 0.9946 - val_loss: 0.5259 - val_accuracy: 0.8571\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0151 - accuracy: 0.9946 - val_loss: 0.5234 - val_accuracy: 0.8730\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0268 - accuracy: 0.9910 - val_loss: 1.0444 - val_accuracy: 0.7937\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0305 - accuracy: 0.9910 - val_loss: 0.5488 - val_accuracy: 0.8730\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0088 - accuracy: 0.9982 - val_loss: 0.7268 - val_accuracy: 0.8730\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0118 - accuracy: 0.9982 - val_loss: 0.6175 - val_accuracy: 0.8571\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0125 - accuracy: 0.9928 - val_loss: 0.6792 - val_accuracy: 0.8730\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0226 - accuracy: 0.9964 - val_loss: 0.9112 - val_accuracy: 0.7778\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0146 - accuracy: 0.9964 - val_loss: 0.8956 - val_accuracy: 0.8095\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.5107 - val_accuracy: 0.8730\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0352 - accuracy: 0.9928 - val_loss: 1.0623 - val_accuracy: 0.7460\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0320 - accuracy: 0.9928 - val_loss: 1.0486 - val_accuracy: 0.8095\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0143 - accuracy: 0.9964 - val_loss: 0.5276 - val_accuracy: 0.8730\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.6500 - val_accuracy: 0.8571\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0142 - accuracy: 0.9964 - val_loss: 0.5898 - val_accuracy: 0.8413\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0254 - accuracy: 0.9946 - val_loss: 0.5532 - val_accuracy: 0.8571\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 1s 1ms/sample - loss: 0.0250 - accuracy: 0.9892 - val_loss: 0.8349 - val_accuracy: 0.8254\n",
      "accuracy for model 4 is 82.53968358039856\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_64 (Dense)             (None, 192)               32716224  \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 32,751,971\n",
      "Trainable params: 32,751,907\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 555 samples, validate on 62 samples\n",
      "Epoch 1/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 1.0156 - accuracy: 0.5261 - val_loss: 2.5773 - val_accuracy: 0.7581\n",
      "Epoch 2/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.8582 - accuracy: 0.6595 - val_loss: 1.7422 - val_accuracy: 0.7581\n",
      "Epoch 3/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.7479 - accuracy: 0.6847 - val_loss: 2.0311 - val_accuracy: 0.7581\n",
      "Epoch 4/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.7202 - accuracy: 0.6865 - val_loss: 1.4777 - val_accuracy: 0.7581\n",
      "Epoch 5/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.6717 - accuracy: 0.7225 - val_loss: 1.2900 - val_accuracy: 0.7581\n",
      "Epoch 6/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.6370 - accuracy: 0.7243 - val_loss: 0.6860 - val_accuracy: 0.7742\n",
      "Epoch 7/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.6123 - accuracy: 0.7405 - val_loss: 0.6612 - val_accuracy: 0.7742\n",
      "Epoch 8/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.5740 - accuracy: 0.7622 - val_loss: 0.5727 - val_accuracy: 0.7581\n",
      "Epoch 9/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.5690 - accuracy: 0.7622 - val_loss: 0.7672 - val_accuracy: 0.7581\n",
      "Epoch 10/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.5647 - accuracy: 0.7640 - val_loss: 0.7358 - val_accuracy: 0.7581\n",
      "Epoch 11/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.5538 - accuracy: 0.7568 - val_loss: 0.6540 - val_accuracy: 0.7581\n",
      "Epoch 12/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.5127 - accuracy: 0.7946 - val_loss: 0.7959 - val_accuracy: 0.7581\n",
      "Epoch 13/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.4800 - accuracy: 0.8072 - val_loss: 1.0449 - val_accuracy: 0.7581\n",
      "Epoch 14/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.4211 - accuracy: 0.8360 - val_loss: 0.6127 - val_accuracy: 0.7742\n",
      "Epoch 15/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.4558 - accuracy: 0.8252 - val_loss: 0.7021 - val_accuracy: 0.7097\n",
      "Epoch 16/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.4256 - accuracy: 0.8216 - val_loss: 0.6872 - val_accuracy: 0.6935\n",
      "Epoch 17/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.4395 - accuracy: 0.8306 - val_loss: 0.6779 - val_accuracy: 0.7097\n",
      "Epoch 18/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.4208 - accuracy: 0.8468 - val_loss: 0.4863 - val_accuracy: 0.8226\n",
      "Epoch 19/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.4035 - accuracy: 0.8649 - val_loss: 0.5529 - val_accuracy: 0.7742\n",
      "Epoch 20/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.3896 - accuracy: 0.8432 - val_loss: 0.5466 - val_accuracy: 0.8226\n",
      "Epoch 21/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.3522 - accuracy: 0.8667 - val_loss: 0.6943 - val_accuracy: 0.7419\n",
      "Epoch 22/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.3425 - accuracy: 0.8685 - val_loss: 1.1210 - val_accuracy: 0.7742\n",
      "Epoch 23/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.2857 - accuracy: 0.8919 - val_loss: 0.6491 - val_accuracy: 0.7581\n",
      "Epoch 24/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.2545 - accuracy: 0.9027 - val_loss: 0.4849 - val_accuracy: 0.8065\n",
      "Epoch 25/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.2675 - accuracy: 0.8991 - val_loss: 0.4592 - val_accuracy: 0.8226\n",
      "Epoch 26/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.2480 - accuracy: 0.9189 - val_loss: 0.4824 - val_accuracy: 0.8387\n",
      "Epoch 27/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.2444 - accuracy: 0.9153 - val_loss: 0.5955 - val_accuracy: 0.8065\n",
      "Epoch 28/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.1969 - accuracy: 0.9225 - val_loss: 0.6171 - val_accuracy: 0.6290\n",
      "Epoch 29/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.2295 - accuracy: 0.9171 - val_loss: 0.4014 - val_accuracy: 0.8226\n",
      "Epoch 30/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.1988 - accuracy: 0.9369 - val_loss: 0.4069 - val_accuracy: 0.8065\n",
      "Epoch 31/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.1993 - accuracy: 0.9225 - val_loss: 0.4086 - val_accuracy: 0.8226\n",
      "Epoch 32/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.1937 - accuracy: 0.9297 - val_loss: 0.5774 - val_accuracy: 0.8387\n",
      "Epoch 33/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.1573 - accuracy: 0.9423 - val_loss: 0.3650 - val_accuracy: 0.8548\n",
      "Epoch 34/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.1529 - accuracy: 0.9495 - val_loss: 0.4246 - val_accuracy: 0.8387\n",
      "Epoch 35/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.1540 - accuracy: 0.9387 - val_loss: 0.4666 - val_accuracy: 0.8387\n",
      "Epoch 36/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.1470 - accuracy: 0.9495 - val_loss: 0.4442 - val_accuracy: 0.8065\n",
      "Epoch 37/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.1146 - accuracy: 0.9586 - val_loss: 0.4636 - val_accuracy: 0.8548\n",
      "Epoch 38/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0954 - accuracy: 0.9640 - val_loss: 0.6156 - val_accuracy: 0.8387\n",
      "Epoch 39/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0960 - accuracy: 0.9640 - val_loss: 0.5188 - val_accuracy: 0.8387\n",
      "Epoch 40/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.1074 - accuracy: 0.9604 - val_loss: 0.4159 - val_accuracy: 0.8548\n",
      "Epoch 41/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0809 - accuracy: 0.9694 - val_loss: 0.3750 - val_accuracy: 0.8387\n",
      "Epoch 42/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0933 - accuracy: 0.9694 - val_loss: 0.6232 - val_accuracy: 0.8387\n",
      "Epoch 43/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0910 - accuracy: 0.9676 - val_loss: 0.4407 - val_accuracy: 0.7742\n",
      "Epoch 44/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0964 - accuracy: 0.9658 - val_loss: 0.4768 - val_accuracy: 0.8387\n",
      "Epoch 45/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0633 - accuracy: 0.9748 - val_loss: 0.7729 - val_accuracy: 0.8065\n",
      "Epoch 46/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0588 - accuracy: 0.9766 - val_loss: 0.3985 - val_accuracy: 0.8226\n",
      "Epoch 47/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0643 - accuracy: 0.9784 - val_loss: 0.4409 - val_accuracy: 0.7903\n",
      "Epoch 48/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0644 - accuracy: 0.9730 - val_loss: 0.4475 - val_accuracy: 0.7742\n",
      "Epoch 49/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0651 - accuracy: 0.9784 - val_loss: 0.4816 - val_accuracy: 0.8226\n",
      "Epoch 50/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0272 - accuracy: 0.9928 - val_loss: 0.8525 - val_accuracy: 0.8387\n",
      "Epoch 51/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0471 - accuracy: 0.9802 - val_loss: 0.6442 - val_accuracy: 0.8548\n",
      "Epoch 52/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0575 - accuracy: 0.9838 - val_loss: 0.6021 - val_accuracy: 0.8548\n",
      "Epoch 53/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0441 - accuracy: 0.9874 - val_loss: 0.7844 - val_accuracy: 0.8548\n",
      "Epoch 54/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0355 - accuracy: 0.9838 - val_loss: 0.4788 - val_accuracy: 0.7903\n",
      "Epoch 55/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0368 - accuracy: 0.9874 - val_loss: 0.6658 - val_accuracy: 0.7742\n",
      "Epoch 56/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0385 - accuracy: 0.9874 - val_loss: 1.1096 - val_accuracy: 0.8065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0304 - accuracy: 0.9910 - val_loss: 0.5064 - val_accuracy: 0.8548\n",
      "Epoch 58/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0245 - accuracy: 0.9946 - val_loss: 0.4177 - val_accuracy: 0.8065\n",
      "Epoch 59/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0145 - accuracy: 0.9982 - val_loss: 0.6864 - val_accuracy: 0.8548\n",
      "Epoch 60/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0229 - accuracy: 0.9928 - val_loss: 0.6836 - val_accuracy: 0.8548\n",
      "Epoch 61/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0698 - accuracy: 0.9730 - val_loss: 0.5012 - val_accuracy: 0.8226\n",
      "Epoch 62/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0265 - accuracy: 0.9946 - val_loss: 0.9943 - val_accuracy: 0.8548\n",
      "Epoch 63/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0208 - accuracy: 0.9964 - val_loss: 0.4431 - val_accuracy: 0.8226\n",
      "Epoch 64/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0240 - accuracy: 0.9928 - val_loss: 0.9666 - val_accuracy: 0.8548\n",
      "Epoch 65/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0162 - accuracy: 0.9964 - val_loss: 0.4105 - val_accuracy: 0.8065\n",
      "Epoch 66/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0199 - accuracy: 0.9946 - val_loss: 0.4328 - val_accuracy: 0.8387\n",
      "Epoch 67/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0237 - accuracy: 0.9946 - val_loss: 1.1675 - val_accuracy: 0.8226\n",
      "Epoch 68/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0175 - accuracy: 0.9946 - val_loss: 0.4466 - val_accuracy: 0.8387\n",
      "Epoch 69/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0454 - accuracy: 0.9856 - val_loss: 0.6643 - val_accuracy: 0.8065\n",
      "Epoch 70/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0458 - accuracy: 0.9874 - val_loss: 0.5831 - val_accuracy: 0.8548\n",
      "Epoch 71/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0253 - accuracy: 0.9892 - val_loss: 0.8601 - val_accuracy: 0.7581\n",
      "Epoch 72/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0186 - accuracy: 0.9964 - val_loss: 0.9629 - val_accuracy: 0.8710\n",
      "Epoch 73/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0507 - accuracy: 0.9874 - val_loss: 0.4513 - val_accuracy: 0.8226\n",
      "Epoch 74/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0342 - accuracy: 0.9802 - val_loss: 0.9066 - val_accuracy: 0.7097\n",
      "Epoch 75/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0362 - accuracy: 0.9838 - val_loss: 0.6023 - val_accuracy: 0.8065\n",
      "Epoch 76/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0533 - accuracy: 0.9856 - val_loss: 1.8360 - val_accuracy: 0.7903\n",
      "Epoch 77/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0383 - accuracy: 0.9910 - val_loss: 0.8160 - val_accuracy: 0.7419\n",
      "Epoch 78/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0344 - accuracy: 0.9874 - val_loss: 2.1478 - val_accuracy: 0.7903\n",
      "Epoch 79/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0379 - accuracy: 0.9892 - val_loss: 0.6371 - val_accuracy: 0.8226\n",
      "Epoch 80/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0163 - accuracy: 0.9928 - val_loss: 0.8109 - val_accuracy: 0.8548\n",
      "Epoch 81/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0210 - accuracy: 0.9910 - val_loss: 0.8545 - val_accuracy: 0.8548\n",
      "Epoch 82/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0225 - accuracy: 0.9946 - val_loss: 0.9134 - val_accuracy: 0.8548\n",
      "Epoch 83/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0420 - accuracy: 0.9856 - val_loss: 2.0967 - val_accuracy: 0.7903\n",
      "Epoch 84/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0326 - accuracy: 0.9874 - val_loss: 0.8580 - val_accuracy: 0.8387\n",
      "Epoch 85/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0241 - accuracy: 0.9946 - val_loss: 0.5669 - val_accuracy: 0.7903\n",
      "Epoch 86/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0101 - accuracy: 0.9964 - val_loss: 0.9898 - val_accuracy: 0.8387\n",
      "Epoch 87/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0170 - accuracy: 0.9946 - val_loss: 0.4968 - val_accuracy: 0.8065\n",
      "Epoch 88/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0347 - accuracy: 0.9910 - val_loss: 1.0288 - val_accuracy: 0.8548\n",
      "Epoch 89/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0186 - accuracy: 0.9946 - val_loss: 0.5255 - val_accuracy: 0.8387\n",
      "Epoch 90/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.5883 - val_accuracy: 0.8387\n",
      "Epoch 91/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0177 - accuracy: 0.9946 - val_loss: 0.4568 - val_accuracy: 0.8548\n",
      "Epoch 92/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0098 - accuracy: 0.9964 - val_loss: 1.6780 - val_accuracy: 0.8226\n",
      "Epoch 93/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0107 - accuracy: 0.9928 - val_loss: 0.5119 - val_accuracy: 0.8387\n",
      "Epoch 94/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0154 - accuracy: 0.9964 - val_loss: 0.7701 - val_accuracy: 0.8387\n",
      "Epoch 95/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0216 - accuracy: 0.9946 - val_loss: 0.9657 - val_accuracy: 0.8548\n",
      "Epoch 96/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0174 - accuracy: 0.9946 - val_loss: 0.5912 - val_accuracy: 0.8387\n",
      "Epoch 97/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0183 - accuracy: 0.9928 - val_loss: 0.4740 - val_accuracy: 0.8226\n",
      "Epoch 98/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0122 - accuracy: 0.9910 - val_loss: 0.5651 - val_accuracy: 0.8226\n",
      "Epoch 99/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0074 - accuracy: 0.9982 - val_loss: 0.9178 - val_accuracy: 0.8710\n",
      "Epoch 100/100\n",
      "555/555 [==============================] - 1s 1ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.6152 - val_accuracy: 0.8065\n",
      "accuracy for model 5 is 80.64516186714172\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_70 (Dense)             (None, 192)               32716224  \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 32,751,971\n",
      "Trainable params: 32,751,907\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "556/556 [==============================] - 1s 3ms/sample - loss: 0.8383 - accuracy: 0.6367 - val_loss: 3.1535 - val_accuracy: 0.4098\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.7483 - accuracy: 0.6942 - val_loss: 0.6068 - val_accuracy: 0.5574\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.7302 - accuracy: 0.6978 - val_loss: 1.5592 - val_accuracy: 0.4098\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.6599 - accuracy: 0.7158 - val_loss: 1.5485 - val_accuracy: 0.4098\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.6369 - accuracy: 0.7482 - val_loss: 0.4096 - val_accuracy: 0.8033\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.6109 - accuracy: 0.7500 - val_loss: 0.3979 - val_accuracy: 0.8361\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.5398 - accuracy: 0.7932 - val_loss: 0.7224 - val_accuracy: 0.5738\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.5035 - accuracy: 0.8147 - val_loss: 0.5520 - val_accuracy: 0.7705\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.4353 - accuracy: 0.8309 - val_loss: 0.7046 - val_accuracy: 0.8033\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.3534 - accuracy: 0.8741 - val_loss: 0.3432 - val_accuracy: 0.8525\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.3263 - accuracy: 0.8867 - val_loss: 0.4250 - val_accuracy: 0.8033\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.2432 - accuracy: 0.9173 - val_loss: 0.6597 - val_accuracy: 0.8361\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.2315 - accuracy: 0.9119 - val_loss: 0.4387 - val_accuracy: 0.8361\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1955 - accuracy: 0.9335 - val_loss: 0.5740 - val_accuracy: 0.7541\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1857 - accuracy: 0.9371 - val_loss: 0.6532 - val_accuracy: 0.8197\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1886 - accuracy: 0.9263 - val_loss: 0.5758 - val_accuracy: 0.7377\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1375 - accuracy: 0.9550 - val_loss: 0.5799 - val_accuracy: 0.8689\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1135 - accuracy: 0.9694 - val_loss: 0.4804 - val_accuracy: 0.8361\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1008 - accuracy: 0.9622 - val_loss: 0.8459 - val_accuracy: 0.8033\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0793 - accuracy: 0.9676 - val_loss: 0.6222 - val_accuracy: 0.8689\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0750 - accuracy: 0.9748 - val_loss: 1.0029 - val_accuracy: 0.8033\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0968 - accuracy: 0.9712 - val_loss: 0.9148 - val_accuracy: 0.7213\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0895 - accuracy: 0.9622 - val_loss: 1.2850 - val_accuracy: 0.8033\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0757 - accuracy: 0.9748 - val_loss: 1.0298 - val_accuracy: 0.8033\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0317 - accuracy: 0.9928 - val_loss: 0.6092 - val_accuracy: 0.8033\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0478 - accuracy: 0.9856 - val_loss: 0.5089 - val_accuracy: 0.8852\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0564 - accuracy: 0.9838 - val_loss: 0.7209 - val_accuracy: 0.7541\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0442 - accuracy: 0.9874 - val_loss: 0.4722 - val_accuracy: 0.8689\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0595 - accuracy: 0.9802 - val_loss: 0.5339 - val_accuracy: 0.8852\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0297 - accuracy: 0.9928 - val_loss: 0.5559 - val_accuracy: 0.8361\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0349 - accuracy: 0.9892 - val_loss: 0.8717 - val_accuracy: 0.8525\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0219 - accuracy: 0.9964 - val_loss: 0.6558 - val_accuracy: 0.7869\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0154 - accuracy: 0.9982 - val_loss: 0.6244 - val_accuracy: 0.8852\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0261 - accuracy: 0.9946 - val_loss: 0.6304 - val_accuracy: 0.7869\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.7243 - val_accuracy: 0.8525\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0297 - accuracy: 0.9910 - val_loss: 1.4515 - val_accuracy: 0.8033\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0730 - accuracy: 0.9676 - val_loss: 0.9254 - val_accuracy: 0.8689\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0273 - accuracy: 0.9892 - val_loss: 0.9878 - val_accuracy: 0.7213\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0160 - accuracy: 0.9964 - val_loss: 0.6893 - val_accuracy: 0.8361\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.7423 - val_accuracy: 0.8689\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0153 - accuracy: 0.9964 - val_loss: 0.7258 - val_accuracy: 0.8361\n",
      "Epoch 42/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0092 - accuracy: 0.9964 - val_loss: 0.7529 - val_accuracy: 0.8525\n",
      "Epoch 43/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0097 - accuracy: 0.9982 - val_loss: 0.8147 - val_accuracy: 0.8852\n",
      "Epoch 44/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.7760 - val_accuracy: 0.8852\n",
      "Epoch 45/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0069 - accuracy: 0.9982 - val_loss: 0.7188 - val_accuracy: 0.8852\n",
      "Epoch 46/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.7534 - val_accuracy: 0.8852\n",
      "Epoch 47/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.7493 - val_accuracy: 0.8689\n",
      "Epoch 48/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.8987 - val_accuracy: 0.8525\n",
      "Epoch 49/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.8302 - val_accuracy: 0.8197\n",
      "Epoch 50/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.6450 - val_accuracy: 0.8361\n",
      "Epoch 51/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.7124 - val_accuracy: 0.8689\n",
      "Epoch 52/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0150 - accuracy: 0.9946 - val_loss: 0.5359 - val_accuracy: 0.8525\n",
      "Epoch 53/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0182 - accuracy: 0.9928 - val_loss: 0.5851 - val_accuracy: 0.8197\n",
      "Epoch 54/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.6913 - val_accuracy: 0.8361\n",
      "Epoch 55/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0143 - accuracy: 0.9964 - val_loss: 0.8888 - val_accuracy: 0.7705\n",
      "Epoch 56/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0131 - accuracy: 0.9982 - val_loss: 0.6260 - val_accuracy: 0.8361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.7580 - val_accuracy: 0.8852\n",
      "Epoch 58/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0154 - accuracy: 0.9946 - val_loss: 1.1869 - val_accuracy: 0.6557\n",
      "Epoch 59/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0116 - accuracy: 0.9946 - val_loss: 0.7533 - val_accuracy: 0.8525\n",
      "Epoch 60/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0080 - accuracy: 0.9982 - val_loss: 0.9876 - val_accuracy: 0.7541\n",
      "Epoch 61/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.9202 - val_accuracy: 0.8689\n",
      "Epoch 62/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.6966 - val_accuracy: 0.8361\n",
      "Epoch 63/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.7529 - val_accuracy: 0.8361\n",
      "Epoch 64/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0028 - accuracy: 0.9982 - val_loss: 0.7581 - val_accuracy: 0.8361\n",
      "Epoch 65/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.8195 - val_accuracy: 0.8852\n",
      "Epoch 66/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.8408 - val_accuracy: 0.8033\n",
      "Epoch 67/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.7272 - val_accuracy: 0.8525\n",
      "Epoch 68/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0155 - accuracy: 0.9928 - val_loss: 0.9234 - val_accuracy: 0.8689\n",
      "Epoch 69/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.0139 - val_accuracy: 0.8689\n",
      "Epoch 70/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0087 - accuracy: 0.9982 - val_loss: 0.6294 - val_accuracy: 0.8361\n",
      "Epoch 71/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0076 - accuracy: 0.9982 - val_loss: 0.6741 - val_accuracy: 0.8689\n",
      "Epoch 72/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0281 - accuracy: 0.9874 - val_loss: 1.5961 - val_accuracy: 0.7377\n",
      "Epoch 73/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0165 - accuracy: 0.9982 - val_loss: 1.0841 - val_accuracy: 0.7049\n",
      "Epoch 74/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0253 - accuracy: 0.9892 - val_loss: 0.7812 - val_accuracy: 0.8361\n",
      "Epoch 75/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0315 - accuracy: 0.9892 - val_loss: 1.0790 - val_accuracy: 0.7705\n",
      "Epoch 76/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0164 - accuracy: 0.9946 - val_loss: 1.4111 - val_accuracy: 0.8197\n",
      "Epoch 77/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0149 - accuracy: 0.9946 - val_loss: 1.0193 - val_accuracy: 0.7377\n",
      "Epoch 78/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0082 - accuracy: 0.9982 - val_loss: 0.9694 - val_accuracy: 0.8361\n",
      "Epoch 79/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0130 - accuracy: 0.9964 - val_loss: 1.0257 - val_accuracy: 0.8361\n",
      "Epoch 80/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0416 - accuracy: 0.9838 - val_loss: 2.4783 - val_accuracy: 0.6230\n",
      "Epoch 81/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0195 - accuracy: 0.9928 - val_loss: 1.3294 - val_accuracy: 0.8197\n",
      "Epoch 82/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0528 - accuracy: 0.9820 - val_loss: 2.2612 - val_accuracy: 0.7705\n",
      "Epoch 83/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0942 - accuracy: 0.9694 - val_loss: 1.1379 - val_accuracy: 0.8197\n",
      "Epoch 84/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0460 - accuracy: 0.9874 - val_loss: 1.5283 - val_accuracy: 0.8033\n",
      "Epoch 85/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0236 - accuracy: 0.9928 - val_loss: 1.2226 - val_accuracy: 0.8197\n",
      "Epoch 86/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0277 - accuracy: 0.9892 - val_loss: 1.1437 - val_accuracy: 0.8361\n",
      "Epoch 87/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0242 - accuracy: 0.9910 - val_loss: 1.1906 - val_accuracy: 0.8361\n",
      "Epoch 88/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0185 - accuracy: 0.9928 - val_loss: 0.7898 - val_accuracy: 0.8852\n",
      "Epoch 89/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0202 - accuracy: 0.9910 - val_loss: 0.6217 - val_accuracy: 0.8361\n",
      "Epoch 90/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0153 - accuracy: 0.9964 - val_loss: 0.7315 - val_accuracy: 0.8689\n",
      "Epoch 91/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0179 - accuracy: 0.9964 - val_loss: 0.7324 - val_accuracy: 0.8852\n",
      "Epoch 92/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0112 - accuracy: 0.9946 - val_loss: 0.7085 - val_accuracy: 0.8689\n",
      "Epoch 93/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0137 - accuracy: 0.9946 - val_loss: 0.5684 - val_accuracy: 0.8689\n",
      "Epoch 94/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0127 - accuracy: 0.9964 - val_loss: 0.6637 - val_accuracy: 0.9016\n",
      "Epoch 95/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0160 - accuracy: 0.9946 - val_loss: 0.8818 - val_accuracy: 0.8689\n",
      "Epoch 96/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0161 - accuracy: 0.9964 - val_loss: 1.2095 - val_accuracy: 0.8361\n",
      "Epoch 97/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0221 - accuracy: 0.9928 - val_loss: 0.6390 - val_accuracy: 0.8525\n",
      "Epoch 98/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.7158 - val_accuracy: 0.8525\n",
      "Epoch 99/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0161 - accuracy: 0.9964 - val_loss: 0.9851 - val_accuracy: 0.8689\n",
      "Epoch 100/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.7816 - val_accuracy: 0.8361\n",
      "accuracy for model 6 is 83.60655903816223\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_76 (Dense)             (None, 192)               32716224  \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 32,751,971\n",
      "Trainable params: 32,751,907\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "556/556 [==============================] - 1s 2ms/sample - loss: 0.9285 - accuracy: 0.5629 - val_loss: 4.2268 - val_accuracy: 0.5902\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.7580 - accuracy: 0.7086 - val_loss: 1.8743 - val_accuracy: 0.7049\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.6929 - accuracy: 0.7050 - val_loss: 1.7643 - val_accuracy: 0.7541\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.6953 - accuracy: 0.7122 - val_loss: 1.3008 - val_accuracy: 0.7705\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.6026 - accuracy: 0.7482 - val_loss: 0.8105 - val_accuracy: 0.7869\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.5590 - accuracy: 0.7716 - val_loss: 0.8787 - val_accuracy: 0.7869\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.5172 - accuracy: 0.7986 - val_loss: 0.6250 - val_accuracy: 0.7869\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.4508 - accuracy: 0.8273 - val_loss: 0.5638 - val_accuracy: 0.7705\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.3959 - accuracy: 0.8615 - val_loss: 0.6183 - val_accuracy: 0.7541\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.3380 - accuracy: 0.8813 - val_loss: 0.4028 - val_accuracy: 0.8361\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.2705 - accuracy: 0.9065 - val_loss: 0.7113 - val_accuracy: 0.7049\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.2627 - accuracy: 0.9155 - val_loss: 0.4265 - val_accuracy: 0.7869\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.2328 - accuracy: 0.9155 - val_loss: 0.4706 - val_accuracy: 0.7705\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1949 - accuracy: 0.9353 - val_loss: 0.3951 - val_accuracy: 0.8525\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1991 - accuracy: 0.9263 - val_loss: 0.6014 - val_accuracy: 0.8033\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1329 - accuracy: 0.9640 - val_loss: 0.6185 - val_accuracy: 0.7705\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1371 - accuracy: 0.9478 - val_loss: 0.4764 - val_accuracy: 0.8361\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1152 - accuracy: 0.9658 - val_loss: 0.4289 - val_accuracy: 0.8361\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0996 - accuracy: 0.9712 - val_loss: 1.8749 - val_accuracy: 0.4262\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0917 - accuracy: 0.9712 - val_loss: 0.6362 - val_accuracy: 0.7705\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0707 - accuracy: 0.9838 - val_loss: 0.8626 - val_accuracy: 0.8197\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0710 - accuracy: 0.9748 - val_loss: 0.3313 - val_accuracy: 0.9344\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0567 - accuracy: 0.9820 - val_loss: 0.5281 - val_accuracy: 0.8525\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0640 - accuracy: 0.9784 - val_loss: 0.4236 - val_accuracy: 0.9016\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0623 - accuracy: 0.9766 - val_loss: 0.6527 - val_accuracy: 0.8197\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0608 - accuracy: 0.9820 - val_loss: 0.4549 - val_accuracy: 0.8852\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0250 - accuracy: 0.9946 - val_loss: 0.4162 - val_accuracy: 0.8852\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0482 - accuracy: 0.9820 - val_loss: 0.4797 - val_accuracy: 0.8689\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0473 - accuracy: 0.9820 - val_loss: 0.5351 - val_accuracy: 0.8852\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0634 - accuracy: 0.9784 - val_loss: 0.3930 - val_accuracy: 0.8689\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0329 - accuracy: 0.9892 - val_loss: 0.7990 - val_accuracy: 0.7869\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0488 - accuracy: 0.9838 - val_loss: 0.8340 - val_accuracy: 0.8361\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0179 - accuracy: 0.9946 - val_loss: 0.3964 - val_accuracy: 0.9344\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0171 - accuracy: 0.9928 - val_loss: 0.6563 - val_accuracy: 0.8361\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0184 - accuracy: 0.9964 - val_loss: 0.3412 - val_accuracy: 0.9180\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0242 - accuracy: 0.9928 - val_loss: 0.8877 - val_accuracy: 0.7705\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0246 - accuracy: 0.9910 - val_loss: 0.4570 - val_accuracy: 0.9016\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0251 - accuracy: 0.9910 - val_loss: 0.4772 - val_accuracy: 0.9180\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0320 - accuracy: 0.9910 - val_loss: 0.6365 - val_accuracy: 0.8525\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0333 - accuracy: 0.9910 - val_loss: 0.6664 - val_accuracy: 0.8197\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.8781 - val_accuracy: 0.8033\n",
      "Epoch 42/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0171 - accuracy: 0.9982 - val_loss: 0.4937 - val_accuracy: 0.8852\n",
      "Epoch 43/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0117 - accuracy: 0.9982 - val_loss: 0.4538 - val_accuracy: 0.8852\n",
      "Epoch 44/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0121 - accuracy: 0.9964 - val_loss: 0.5717 - val_accuracy: 0.9180\n",
      "Epoch 45/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0260 - accuracy: 0.9910 - val_loss: 0.7363 - val_accuracy: 0.8361\n",
      "Epoch 46/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0186 - accuracy: 0.9964 - val_loss: 0.7694 - val_accuracy: 0.8033\n",
      "Epoch 47/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0199 - accuracy: 0.9928 - val_loss: 0.4850 - val_accuracy: 0.9016\n",
      "Epoch 48/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.6517 - val_accuracy: 0.8689\n",
      "Epoch 49/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0156 - accuracy: 0.9928 - val_loss: 0.5009 - val_accuracy: 0.9180\n",
      "Epoch 50/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0103 - accuracy: 0.9982 - val_loss: 0.4736 - val_accuracy: 0.9180\n",
      "Epoch 51/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0322 - accuracy: 0.9892 - val_loss: 0.5878 - val_accuracy: 0.8852\n",
      "Epoch 52/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0176 - accuracy: 0.9946 - val_loss: 0.5253 - val_accuracy: 0.8689\n",
      "Epoch 53/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0104 - accuracy: 0.9964 - val_loss: 0.5651 - val_accuracy: 0.9016\n",
      "Epoch 54/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0400 - accuracy: 0.9874 - val_loss: 0.4989 - val_accuracy: 0.8361\n",
      "Epoch 55/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0282 - accuracy: 0.9856 - val_loss: 0.5155 - val_accuracy: 0.9016\n",
      "Epoch 56/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0215 - accuracy: 0.9928 - val_loss: 0.5861 - val_accuracy: 0.8689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0349 - accuracy: 0.9820 - val_loss: 0.6987 - val_accuracy: 0.8525\n",
      "Epoch 58/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0450 - accuracy: 0.9892 - val_loss: 0.6110 - val_accuracy: 0.8361\n",
      "Epoch 59/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0320 - accuracy: 0.9928 - val_loss: 0.7654 - val_accuracy: 0.9180\n",
      "Epoch 60/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0215 - accuracy: 0.9910 - val_loss: 0.7786 - val_accuracy: 0.8852\n",
      "Epoch 61/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0123 - accuracy: 0.9964 - val_loss: 0.5142 - val_accuracy: 0.9344\n",
      "Epoch 62/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0163 - accuracy: 0.9964 - val_loss: 0.4476 - val_accuracy: 0.9016\n",
      "Epoch 63/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0130 - accuracy: 0.9946 - val_loss: 0.5971 - val_accuracy: 0.8689\n",
      "Epoch 64/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0235 - accuracy: 0.9910 - val_loss: 1.2117 - val_accuracy: 0.8197\n",
      "Epoch 65/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0219 - accuracy: 0.9946 - val_loss: 0.6083 - val_accuracy: 0.8689\n",
      "Epoch 66/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0134 - accuracy: 0.9964 - val_loss: 0.5433 - val_accuracy: 0.8852\n",
      "Epoch 67/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0187 - accuracy: 0.9946 - val_loss: 0.5702 - val_accuracy: 0.8852\n",
      "Epoch 68/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0225 - accuracy: 0.9946 - val_loss: 0.7357 - val_accuracy: 0.9016\n",
      "Epoch 69/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.5669 - val_accuracy: 0.8689\n",
      "Epoch 70/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0135 - accuracy: 0.9946 - val_loss: 0.5509 - val_accuracy: 0.8689\n",
      "Epoch 71/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0097 - accuracy: 0.9964 - val_loss: 0.5106 - val_accuracy: 0.8689\n",
      "Epoch 72/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.6516 - val_accuracy: 0.9180\n",
      "Epoch 73/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0137 - accuracy: 0.9964 - val_loss: 0.6548 - val_accuracy: 0.8852\n",
      "Epoch 74/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0196 - accuracy: 0.9946 - val_loss: 0.5486 - val_accuracy: 0.9016\n",
      "Epoch 75/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0096 - accuracy: 0.9964 - val_loss: 0.5614 - val_accuracy: 0.9344\n",
      "Epoch 76/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0218 - accuracy: 0.9910 - val_loss: 0.5215 - val_accuracy: 0.9016\n",
      "Epoch 77/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0328 - accuracy: 0.9874 - val_loss: 0.7303 - val_accuracy: 0.8525\n",
      "Epoch 78/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0188 - accuracy: 0.9892 - val_loss: 1.4365 - val_accuracy: 0.7213\n",
      "Epoch 79/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0441 - accuracy: 0.9766 - val_loss: 2.7855 - val_accuracy: 0.5410\n",
      "Epoch 80/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0313 - accuracy: 0.9892 - val_loss: 0.9495 - val_accuracy: 0.8852\n",
      "Epoch 81/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0185 - accuracy: 0.9946 - val_loss: 1.2484 - val_accuracy: 0.7377\n",
      "Epoch 82/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0163 - accuracy: 0.9964 - val_loss: 0.6388 - val_accuracy: 0.8689\n",
      "Epoch 83/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.6235 - val_accuracy: 0.8525\n",
      "Epoch 84/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0171 - accuracy: 0.9946 - val_loss: 0.6362 - val_accuracy: 0.8525\n",
      "Epoch 85/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0193 - accuracy: 0.9928 - val_loss: 0.5652 - val_accuracy: 0.9180\n",
      "Epoch 86/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.8863 - val_accuracy: 0.8525\n",
      "Epoch 87/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.5703 - val_accuracy: 0.8689\n",
      "Epoch 88/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0082 - accuracy: 0.9982 - val_loss: 1.5529 - val_accuracy: 0.6721\n",
      "Epoch 89/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0345 - accuracy: 0.9892 - val_loss: 0.9023 - val_accuracy: 0.8361\n",
      "Epoch 90/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0434 - accuracy: 0.9910 - val_loss: 0.6798 - val_accuracy: 0.9016\n",
      "Epoch 91/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0290 - accuracy: 0.9910 - val_loss: 0.9396 - val_accuracy: 0.8525\n",
      "Epoch 92/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0135 - accuracy: 0.9928 - val_loss: 0.5778 - val_accuracy: 0.8852\n",
      "Epoch 93/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0092 - accuracy: 0.9946 - val_loss: 0.5544 - val_accuracy: 0.8525\n",
      "Epoch 94/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0437 - accuracy: 0.9820 - val_loss: 0.4651 - val_accuracy: 0.8525\n",
      "Epoch 95/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0196 - accuracy: 0.9928 - val_loss: 0.5668 - val_accuracy: 0.8525\n",
      "Epoch 96/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0248 - accuracy: 0.9928 - val_loss: 0.7750 - val_accuracy: 0.8033\n",
      "Epoch 97/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0220 - accuracy: 0.9892 - val_loss: 1.2147 - val_accuracy: 0.8197\n",
      "Epoch 98/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0126 - accuracy: 0.9946 - val_loss: 2.2563 - val_accuracy: 0.4754\n",
      "Epoch 99/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0119 - accuracy: 0.9946 - val_loss: 0.5589 - val_accuracy: 0.8852\n",
      "Epoch 100/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0104 - accuracy: 0.9946 - val_loss: 0.8892 - val_accuracy: 0.7705\n",
      "accuracy for model 7 is 77.04917788505554\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_82 (Dense)             (None, 192)               32716224  \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 32,751,971\n",
      "Trainable params: 32,751,907\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "556/556 [==============================] - 1s 2ms/sample - loss: 0.8746 - accuracy: 0.6295 - val_loss: 3.2613 - val_accuracy: 0.6393\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.7228 - accuracy: 0.7140 - val_loss: 3.6356 - val_accuracy: 0.6066\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.6959 - accuracy: 0.7014 - val_loss: 1.6818 - val_accuracy: 0.6393\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.6570 - accuracy: 0.7212 - val_loss: 1.2503 - val_accuracy: 0.6230\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.6278 - accuracy: 0.7230 - val_loss: 0.7697 - val_accuracy: 0.6721\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.5722 - accuracy: 0.7734 - val_loss: 0.7098 - val_accuracy: 0.7213\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.5444 - accuracy: 0.7806 - val_loss: 0.8139 - val_accuracy: 0.7049\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.4451 - accuracy: 0.8417 - val_loss: 0.6498 - val_accuracy: 0.7869\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.3977 - accuracy: 0.8597 - val_loss: 1.3175 - val_accuracy: 0.6721\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.3831 - accuracy: 0.8561 - val_loss: 0.7692 - val_accuracy: 0.7213\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.3404 - accuracy: 0.8759 - val_loss: 0.8728 - val_accuracy: 0.6557\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.3005 - accuracy: 0.8813 - val_loss: 0.7963 - val_accuracy: 0.7213\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.2573 - accuracy: 0.9083 - val_loss: 0.8363 - val_accuracy: 0.7377\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.2119 - accuracy: 0.9263 - val_loss: 0.6755 - val_accuracy: 0.7541\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.2007 - accuracy: 0.9317 - val_loss: 0.6527 - val_accuracy: 0.7049\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1836 - accuracy: 0.9317 - val_loss: 0.8720 - val_accuracy: 0.7869\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1696 - accuracy: 0.9442 - val_loss: 0.7789 - val_accuracy: 0.7705\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1241 - accuracy: 0.9658 - val_loss: 0.7414 - val_accuracy: 0.7541\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0868 - accuracy: 0.9748 - val_loss: 1.0282 - val_accuracy: 0.7213\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1178 - accuracy: 0.9568 - val_loss: 0.8563 - val_accuracy: 0.7869\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1067 - accuracy: 0.9622 - val_loss: 1.4427 - val_accuracy: 0.7213\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.1058 - accuracy: 0.9694 - val_loss: 0.8634 - val_accuracy: 0.7869\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0890 - accuracy: 0.9730 - val_loss: 1.1420 - val_accuracy: 0.7869\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0751 - accuracy: 0.9748 - val_loss: 1.0971 - val_accuracy: 0.7377\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0594 - accuracy: 0.9784 - val_loss: 2.2858 - val_accuracy: 0.3934\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0817 - accuracy: 0.9748 - val_loss: 1.0788 - val_accuracy: 0.7377\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0661 - accuracy: 0.9802 - val_loss: 1.3096 - val_accuracy: 0.7705\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0404 - accuracy: 0.9892 - val_loss: 0.8815 - val_accuracy: 0.6885\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0481 - accuracy: 0.9892 - val_loss: 1.0770 - val_accuracy: 0.7541\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0797 - accuracy: 0.9748 - val_loss: 1.9830 - val_accuracy: 0.7049\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0751 - accuracy: 0.9712 - val_loss: 1.3266 - val_accuracy: 0.7705\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0378 - accuracy: 0.9928 - val_loss: 1.0883 - val_accuracy: 0.7541\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0779 - accuracy: 0.9694 - val_loss: 1.2999 - val_accuracy: 0.8033\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0374 - accuracy: 0.9874 - val_loss: 1.7523 - val_accuracy: 0.7705\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0285 - accuracy: 0.9946 - val_loss: 1.4163 - val_accuracy: 0.7541\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0309 - accuracy: 0.9910 - val_loss: 1.3037 - val_accuracy: 0.6557\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0246 - accuracy: 0.9910 - val_loss: 1.1233 - val_accuracy: 0.7377\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0177 - accuracy: 0.9982 - val_loss: 1.4623 - val_accuracy: 0.8033\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0311 - accuracy: 0.9892 - val_loss: 1.1611 - val_accuracy: 0.7869\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.9839 - val_accuracy: 0.7705\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0161 - accuracy: 0.9928 - val_loss: 1.0889 - val_accuracy: 0.8033\n",
      "Epoch 42/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0294 - accuracy: 0.9874 - val_loss: 0.9577 - val_accuracy: 0.7213\n",
      "Epoch 43/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0112 - accuracy: 0.9982 - val_loss: 0.9144 - val_accuracy: 0.7869\n",
      "Epoch 44/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0223 - accuracy: 0.9910 - val_loss: 1.7936 - val_accuracy: 0.5902\n",
      "Epoch 45/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0217 - accuracy: 0.9946 - val_loss: 1.0764 - val_accuracy: 0.7377\n",
      "Epoch 46/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0332 - accuracy: 0.9874 - val_loss: 1.1731 - val_accuracy: 0.7049\n",
      "Epoch 47/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0190 - accuracy: 0.9964 - val_loss: 1.4660 - val_accuracy: 0.7869\n",
      "Epoch 48/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0333 - accuracy: 0.9856 - val_loss: 1.4886 - val_accuracy: 0.7869\n",
      "Epoch 49/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0272 - accuracy: 0.9910 - val_loss: 1.7466 - val_accuracy: 0.8033\n",
      "Epoch 50/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0138 - accuracy: 0.9964 - val_loss: 1.5095 - val_accuracy: 0.7705\n",
      "Epoch 51/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0103 - accuracy: 0.9964 - val_loss: 1.2652 - val_accuracy: 0.7705\n",
      "Epoch 52/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0188 - accuracy: 0.9946 - val_loss: 1.5149 - val_accuracy: 0.7705\n",
      "Epoch 53/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0216 - accuracy: 0.9946 - val_loss: 1.8277 - val_accuracy: 0.8033\n",
      "Epoch 54/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0084 - accuracy: 0.9982 - val_loss: 1.1857 - val_accuracy: 0.7213\n",
      "Epoch 55/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0198 - accuracy: 0.9946 - val_loss: 1.5571 - val_accuracy: 0.7377\n",
      "Epoch 56/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0096 - accuracy: 0.9946 - val_loss: 1.2868 - val_accuracy: 0.6885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0288 - accuracy: 0.9910 - val_loss: 1.6770 - val_accuracy: 0.6557\n",
      "Epoch 58/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0210 - accuracy: 0.9946 - val_loss: 1.2059 - val_accuracy: 0.8033\n",
      "Epoch 59/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.2236 - val_accuracy: 0.8197\n",
      "Epoch 60/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0089 - accuracy: 0.9964 - val_loss: 1.1649 - val_accuracy: 0.8197\n",
      "Epoch 61/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0201 - accuracy: 0.9946 - val_loss: 1.2407 - val_accuracy: 0.8033\n",
      "Epoch 62/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0102 - accuracy: 0.9964 - val_loss: 0.9833 - val_accuracy: 0.7541\n",
      "Epoch 63/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0161 - accuracy: 0.9964 - val_loss: 2.2549 - val_accuracy: 0.6885\n",
      "Epoch 64/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0266 - accuracy: 0.9874 - val_loss: 1.3601 - val_accuracy: 0.8033\n",
      "Epoch 65/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0217 - accuracy: 0.9892 - val_loss: 1.1466 - val_accuracy: 0.7869\n",
      "Epoch 66/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0437 - accuracy: 0.9838 - val_loss: 1.2248 - val_accuracy: 0.5902\n",
      "Epoch 67/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0443 - accuracy: 0.9856 - val_loss: 1.6115 - val_accuracy: 0.5410\n",
      "Epoch 68/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0356 - accuracy: 0.9874 - val_loss: 0.8570 - val_accuracy: 0.7377\n",
      "Epoch 69/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0105 - accuracy: 0.9982 - val_loss: 1.5679 - val_accuracy: 0.8197\n",
      "Epoch 70/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0128 - accuracy: 0.9982 - val_loss: 0.9413 - val_accuracy: 0.7541\n",
      "Epoch 71/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0177 - accuracy: 0.9946 - val_loss: 2.0475 - val_accuracy: 0.8033\n",
      "Epoch 72/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0326 - accuracy: 0.9892 - val_loss: 1.6577 - val_accuracy: 0.6721\n",
      "Epoch 73/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0481 - accuracy: 0.9856 - val_loss: 1.5018 - val_accuracy: 0.8033\n",
      "Epoch 74/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0304 - accuracy: 0.9928 - val_loss: 1.4806 - val_accuracy: 0.6721\n",
      "Epoch 75/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0158 - accuracy: 0.9946 - val_loss: 1.7484 - val_accuracy: 0.7705\n",
      "Epoch 76/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0279 - accuracy: 0.9856 - val_loss: 2.0542 - val_accuracy: 0.7377\n",
      "Epoch 77/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0478 - accuracy: 0.9820 - val_loss: 1.1417 - val_accuracy: 0.6885\n",
      "Epoch 78/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0235 - accuracy: 0.9910 - val_loss: 1.7139 - val_accuracy: 0.6066\n",
      "Epoch 79/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0210 - accuracy: 0.9892 - val_loss: 1.4069 - val_accuracy: 0.7705\n",
      "Epoch 80/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0213 - accuracy: 0.9928 - val_loss: 2.0535 - val_accuracy: 0.7377\n",
      "Epoch 81/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0089 - accuracy: 0.9982 - val_loss: 1.3179 - val_accuracy: 0.6885\n",
      "Epoch 82/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0197 - accuracy: 0.9910 - val_loss: 2.9938 - val_accuracy: 0.7049\n",
      "Epoch 83/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0209 - accuracy: 0.9910 - val_loss: 1.1611 - val_accuracy: 0.8033\n",
      "Epoch 84/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0263 - accuracy: 0.9928 - val_loss: 1.7958 - val_accuracy: 0.7705\n",
      "Epoch 85/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0205 - accuracy: 0.9946 - val_loss: 1.2195 - val_accuracy: 0.7705\n",
      "Epoch 86/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0227 - accuracy: 0.9910 - val_loss: 1.0908 - val_accuracy: 0.7705\n",
      "Epoch 87/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0274 - accuracy: 0.9892 - val_loss: 1.7810 - val_accuracy: 0.6230\n",
      "Epoch 88/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0619 - accuracy: 0.9748 - val_loss: 2.4002 - val_accuracy: 0.7377\n",
      "Epoch 89/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0222 - accuracy: 0.9910 - val_loss: 1.4203 - val_accuracy: 0.6393\n",
      "Epoch 90/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0320 - accuracy: 0.9892 - val_loss: 1.0296 - val_accuracy: 0.7869\n",
      "Epoch 91/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0243 - accuracy: 0.9910 - val_loss: 2.8842 - val_accuracy: 0.7049\n",
      "Epoch 92/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0188 - accuracy: 0.9928 - val_loss: 0.9883 - val_accuracy: 0.7213\n",
      "Epoch 93/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.8097 - val_accuracy: 0.7377\n",
      "Epoch 94/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.8483 - val_accuracy: 0.8361\n",
      "Epoch 95/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0348 - accuracy: 0.9874 - val_loss: 1.0053 - val_accuracy: 0.7213\n",
      "Epoch 96/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0246 - accuracy: 0.9892 - val_loss: 0.9832 - val_accuracy: 0.8033\n",
      "Epoch 97/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0200 - accuracy: 0.9910 - val_loss: 1.5730 - val_accuracy: 0.7541\n",
      "Epoch 98/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.3813 - val_accuracy: 0.7213\n",
      "Epoch 99/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0069 - accuracy: 0.9964 - val_loss: 1.4821 - val_accuracy: 0.7869\n",
      "Epoch 100/100\n",
      "556/556 [==============================] - 1s 1ms/sample - loss: 0.0117 - accuracy: 0.9982 - val_loss: 1.9956 - val_accuracy: 0.7705\n",
      "accuracy for model 8 is 77.04917788505554\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_88 (Dense)             (None, 192)               32716224  \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 32,751,971\n",
      "Trainable params: 32,751,907\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 557 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      "557/557 [==============================] - 1s 2ms/sample - loss: 0.8966 - accuracy: 0.5601 - val_loss: 7.1547 - val_accuracy: 0.6000\n",
      "Epoch 2/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.7233 - accuracy: 0.6804 - val_loss: 4.5648 - val_accuracy: 0.6000\n",
      "Epoch 3/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.6691 - accuracy: 0.7181 - val_loss: 3.3387 - val_accuracy: 0.6000\n",
      "Epoch 4/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.6409 - accuracy: 0.7074 - val_loss: 2.9819 - val_accuracy: 0.6167\n",
      "Epoch 5/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.6116 - accuracy: 0.7235 - val_loss: 1.5699 - val_accuracy: 0.6167\n",
      "Epoch 6/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.5782 - accuracy: 0.7361 - val_loss: 1.1945 - val_accuracy: 0.6333\n",
      "Epoch 7/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.5300 - accuracy: 0.7540 - val_loss: 1.2418 - val_accuracy: 0.6167\n",
      "Epoch 8/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.5125 - accuracy: 0.7846 - val_loss: 0.9540 - val_accuracy: 0.6833\n",
      "Epoch 9/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.4534 - accuracy: 0.8312 - val_loss: 0.8328 - val_accuracy: 0.6500\n",
      "Epoch 10/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.4066 - accuracy: 0.8330 - val_loss: 1.1530 - val_accuracy: 0.3500\n",
      "Epoch 11/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.3827 - accuracy: 0.8636 - val_loss: 0.9008 - val_accuracy: 0.6667\n",
      "Epoch 12/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.3261 - accuracy: 0.8851 - val_loss: 0.8481 - val_accuracy: 0.6500\n",
      "Epoch 13/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.2898 - accuracy: 0.8941 - val_loss: 0.9724 - val_accuracy: 0.5333\n",
      "Epoch 14/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.2263 - accuracy: 0.9138 - val_loss: 0.9431 - val_accuracy: 0.6167\n",
      "Epoch 15/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.2267 - accuracy: 0.9192 - val_loss: 0.7549 - val_accuracy: 0.7333\n",
      "Epoch 16/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.2019 - accuracy: 0.9192 - val_loss: 0.9177 - val_accuracy: 0.7000\n",
      "Epoch 17/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1533 - accuracy: 0.9408 - val_loss: 0.9405 - val_accuracy: 0.6833\n",
      "Epoch 18/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1627 - accuracy: 0.9425 - val_loss: 1.0967 - val_accuracy: 0.7000\n",
      "Epoch 19/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1154 - accuracy: 0.9623 - val_loss: 0.8454 - val_accuracy: 0.6667\n",
      "Epoch 20/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1203 - accuracy: 0.9677 - val_loss: 0.7977 - val_accuracy: 0.7333\n",
      "Epoch 21/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1067 - accuracy: 0.9605 - val_loss: 2.4316 - val_accuracy: 0.6500\n",
      "Epoch 22/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1050 - accuracy: 0.9605 - val_loss: 1.1963 - val_accuracy: 0.7167\n",
      "Epoch 23/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0767 - accuracy: 0.9731 - val_loss: 1.4226 - val_accuracy: 0.6833\n",
      "Epoch 24/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0626 - accuracy: 0.9856 - val_loss: 1.2790 - val_accuracy: 0.7000\n",
      "Epoch 25/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0669 - accuracy: 0.9785 - val_loss: 1.7308 - val_accuracy: 0.6667\n",
      "Epoch 26/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0566 - accuracy: 0.9820 - val_loss: 1.1540 - val_accuracy: 0.7167\n",
      "Epoch 27/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0614 - accuracy: 0.9803 - val_loss: 1.2510 - val_accuracy: 0.7000\n",
      "Epoch 28/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0402 - accuracy: 0.9856 - val_loss: 1.0998 - val_accuracy: 0.7167\n",
      "Epoch 29/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0591 - accuracy: 0.9820 - val_loss: 1.3344 - val_accuracy: 0.6667\n",
      "Epoch 30/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0466 - accuracy: 0.9785 - val_loss: 1.3111 - val_accuracy: 0.6833\n",
      "Epoch 31/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0377 - accuracy: 0.9856 - val_loss: 1.3731 - val_accuracy: 0.7000\n",
      "Epoch 32/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0232 - accuracy: 0.9964 - val_loss: 1.7374 - val_accuracy: 0.6667\n",
      "Epoch 33/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0377 - accuracy: 0.9892 - val_loss: 1.2744 - val_accuracy: 0.7167\n",
      "Epoch 34/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0213 - accuracy: 0.9928 - val_loss: 1.1460 - val_accuracy: 0.7000\n",
      "Epoch 35/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0301 - accuracy: 0.9892 - val_loss: 2.0511 - val_accuracy: 0.6833\n",
      "Epoch 36/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0278 - accuracy: 0.9946 - val_loss: 1.9965 - val_accuracy: 0.6833\n",
      "Epoch 37/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0728 - accuracy: 0.9695 - val_loss: 3.2307 - val_accuracy: 0.6667\n",
      "Epoch 38/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0286 - accuracy: 0.9928 - val_loss: 1.2358 - val_accuracy: 0.7833\n",
      "Epoch 39/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0162 - accuracy: 0.9964 - val_loss: 2.1686 - val_accuracy: 0.6833\n",
      "Epoch 40/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0369 - accuracy: 0.9928 - val_loss: 1.4573 - val_accuracy: 0.7333\n",
      "Epoch 41/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0165 - accuracy: 0.9964 - val_loss: 1.4703 - val_accuracy: 0.7500\n",
      "Epoch 42/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0177 - accuracy: 0.9946 - val_loss: 1.7685 - val_accuracy: 0.7167\n",
      "Epoch 43/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0209 - accuracy: 0.9928 - val_loss: 1.6698 - val_accuracy: 0.7167\n",
      "Epoch 44/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0121 - accuracy: 0.9982 - val_loss: 1.8891 - val_accuracy: 0.6833\n",
      "Epoch 45/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0151 - accuracy: 0.9982 - val_loss: 1.6128 - val_accuracy: 0.7000\n",
      "Epoch 46/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0161 - accuracy: 0.9964 - val_loss: 1.8824 - val_accuracy: 0.7167\n",
      "Epoch 47/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0202 - accuracy: 0.9946 - val_loss: 1.8057 - val_accuracy: 0.7000\n",
      "Epoch 48/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0125 - accuracy: 0.9982 - val_loss: 1.6434 - val_accuracy: 0.7167\n",
      "Epoch 49/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0409 - accuracy: 0.9803 - val_loss: 1.5963 - val_accuracy: 0.6333\n",
      "Epoch 50/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0111 - accuracy: 0.9982 - val_loss: 1.9846 - val_accuracy: 0.7333\n",
      "Epoch 51/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0094 - accuracy: 0.9946 - val_loss: 1.9817 - val_accuracy: 0.6833\n",
      "Epoch 52/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0085 - accuracy: 0.9964 - val_loss: 1.6468 - val_accuracy: 0.7167\n",
      "Epoch 53/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.7417 - val_accuracy: 0.7333\n",
      "Epoch 54/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.4823 - val_accuracy: 0.7667\n",
      "Epoch 55/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0058 - accuracy: 0.9982 - val_loss: 1.7237 - val_accuracy: 0.7333\n",
      "Epoch 56/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.8238 - val_accuracy: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.8153 - val_accuracy: 0.7167\n",
      "Epoch 58/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0101 - accuracy: 0.9964 - val_loss: 1.6516 - val_accuracy: 0.7167\n",
      "Epoch 59/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0076 - accuracy: 0.9964 - val_loss: 1.7561 - val_accuracy: 0.7333\n",
      "Epoch 60/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.8995 - val_accuracy: 0.7000\n",
      "Epoch 61/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0112 - accuracy: 0.9964 - val_loss: 1.9256 - val_accuracy: 0.6833\n",
      "Epoch 62/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0144 - accuracy: 0.9964 - val_loss: 2.7813 - val_accuracy: 0.6833\n",
      "Epoch 63/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0112 - accuracy: 1.0000 - val_loss: 2.0564 - val_accuracy: 0.6833\n",
      "Epoch 64/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0083 - accuracy: 0.9964 - val_loss: 1.9628 - val_accuracy: 0.7167\n",
      "Epoch 65/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0269 - accuracy: 0.9892 - val_loss: 1.7861 - val_accuracy: 0.5833\n",
      "Epoch 66/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0647 - accuracy: 0.9749 - val_loss: 2.5306 - val_accuracy: 0.5833\n",
      "Epoch 67/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0189 - accuracy: 0.9964 - val_loss: 1.6830 - val_accuracy: 0.7000\n",
      "Epoch 68/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0084 - accuracy: 0.9982 - val_loss: 1.8932 - val_accuracy: 0.7000\n",
      "Epoch 69/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0121 - accuracy: 0.9964 - val_loss: 1.9305 - val_accuracy: 0.7000\n",
      "Epoch 70/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0255 - accuracy: 0.9928 - val_loss: 1.6278 - val_accuracy: 0.7167\n",
      "Epoch 71/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0068 - accuracy: 0.9982 - val_loss: 1.5449 - val_accuracy: 0.7333\n",
      "Epoch 72/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0300 - accuracy: 0.9910 - val_loss: 1.4914 - val_accuracy: 0.7833\n",
      "Epoch 73/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0151 - accuracy: 0.9964 - val_loss: 1.8579 - val_accuracy: 0.7333\n",
      "Epoch 74/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0105 - accuracy: 0.9982 - val_loss: 1.8054 - val_accuracy: 0.7167\n",
      "Epoch 75/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0168 - accuracy: 0.9928 - val_loss: 1.4381 - val_accuracy: 0.7500\n",
      "Epoch 76/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0129 - accuracy: 0.9964 - val_loss: 2.3443 - val_accuracy: 0.7000\n",
      "Epoch 77/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.9247 - val_accuracy: 0.7167\n",
      "Epoch 78/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0097 - accuracy: 0.9964 - val_loss: 1.6136 - val_accuracy: 0.7500\n",
      "Epoch 79/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0050 - accuracy: 0.9982 - val_loss: 1.7479 - val_accuracy: 0.7500\n",
      "Epoch 80/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.4284 - val_accuracy: 0.7333\n",
      "Epoch 81/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0402 - accuracy: 0.9856 - val_loss: 1.5496 - val_accuracy: 0.7167\n",
      "Epoch 82/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0122 - accuracy: 0.9946 - val_loss: 1.8976 - val_accuracy: 0.6833\n",
      "Epoch 83/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0135 - accuracy: 0.9964 - val_loss: 2.1146 - val_accuracy: 0.7167\n",
      "Epoch 84/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.3120 - val_accuracy: 0.7000\n",
      "Epoch 85/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.2735 - val_accuracy: 0.7000\n",
      "Epoch 86/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.8891 - val_accuracy: 0.7333\n",
      "Epoch 87/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0056 - accuracy: 0.9982 - val_loss: 2.3682 - val_accuracy: 0.7000\n",
      "Epoch 88/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0155 - accuracy: 0.9946 - val_loss: 1.5983 - val_accuracy: 0.7500\n",
      "Epoch 89/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0063 - accuracy: 0.9982 - val_loss: 1.7161 - val_accuracy: 0.6500\n",
      "Epoch 90/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0159 - accuracy: 0.9946 - val_loss: 2.0897 - val_accuracy: 0.7000\n",
      "Epoch 91/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0217 - accuracy: 0.9910 - val_loss: 1.2284 - val_accuracy: 0.7500\n",
      "Epoch 92/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0397 - accuracy: 0.9838 - val_loss: 2.0259 - val_accuracy: 0.7000\n",
      "Epoch 93/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0124 - accuracy: 0.9946 - val_loss: 1.8078 - val_accuracy: 0.7000\n",
      "Epoch 94/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0144 - accuracy: 0.9946 - val_loss: 1.3771 - val_accuracy: 0.7333\n",
      "Epoch 95/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 2.4381 - val_accuracy: 0.6667\n",
      "Epoch 96/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.8651 - val_accuracy: 0.7000\n",
      "Epoch 97/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0059 - accuracy: 0.9982 - val_loss: 2.1770 - val_accuracy: 0.6833\n",
      "Epoch 98/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0134 - accuracy: 0.9964 - val_loss: 1.7702 - val_accuracy: 0.7167\n",
      "Epoch 99/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0158 - accuracy: 0.9982 - val_loss: 1.6579 - val_accuracy: 0.7000\n",
      "Epoch 100/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0318 - accuracy: 0.9892 - val_loss: 1.7323 - val_accuracy: 0.6667\n",
      "accuracy for model 9 is 66.66666865348816\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_94 (Dense)             (None, 192)               32716224  \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 32,751,971\n",
      "Trainable params: 32,751,907\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 557 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      "557/557 [==============================] - 1s 2ms/sample - loss: 0.9060 - accuracy: 0.6158 - val_loss: 6.5708 - val_accuracy: 0.6000\n",
      "Epoch 2/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.7322 - accuracy: 0.7056 - val_loss: 4.6329 - val_accuracy: 0.6000\n",
      "Epoch 3/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.6821 - accuracy: 0.7092 - val_loss: 2.8257 - val_accuracy: 0.6167\n",
      "Epoch 4/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.6281 - accuracy: 0.7433 - val_loss: 2.0111 - val_accuracy: 0.6167\n",
      "Epoch 5/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.6110 - accuracy: 0.7415 - val_loss: 1.6095 - val_accuracy: 0.6167\n",
      "Epoch 6/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.5723 - accuracy: 0.7630 - val_loss: 1.3380 - val_accuracy: 0.6333\n",
      "Epoch 7/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.5382 - accuracy: 0.7810 - val_loss: 1.4369 - val_accuracy: 0.6167\n",
      "Epoch 8/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.5019 - accuracy: 0.8061 - val_loss: 1.2847 - val_accuracy: 0.6000\n",
      "Epoch 9/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.4358 - accuracy: 0.8330 - val_loss: 1.1595 - val_accuracy: 0.6167\n",
      "Epoch 10/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.4508 - accuracy: 0.8259 - val_loss: 1.2996 - val_accuracy: 0.6000\n",
      "Epoch 11/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.3834 - accuracy: 0.8474 - val_loss: 1.0916 - val_accuracy: 0.5667\n",
      "Epoch 12/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.3641 - accuracy: 0.8600 - val_loss: 1.1352 - val_accuracy: 0.6333\n",
      "Epoch 13/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.3239 - accuracy: 0.8941 - val_loss: 1.0700 - val_accuracy: 0.6333\n",
      "Epoch 14/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.2692 - accuracy: 0.8977 - val_loss: 1.0651 - val_accuracy: 0.7000\n",
      "Epoch 15/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.2510 - accuracy: 0.8941 - val_loss: 1.0481 - val_accuracy: 0.6500\n",
      "Epoch 16/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.2270 - accuracy: 0.9192 - val_loss: 1.0661 - val_accuracy: 0.5833\n",
      "Epoch 17/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.2071 - accuracy: 0.9354 - val_loss: 1.3092 - val_accuracy: 0.6333\n",
      "Epoch 18/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1768 - accuracy: 0.9372 - val_loss: 1.7257 - val_accuracy: 0.6333\n",
      "Epoch 19/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1999 - accuracy: 0.9264 - val_loss: 1.2638 - val_accuracy: 0.6000\n",
      "Epoch 20/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1445 - accuracy: 0.9533 - val_loss: 1.0356 - val_accuracy: 0.6833\n",
      "Epoch 21/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1242 - accuracy: 0.9587 - val_loss: 1.5471 - val_accuracy: 0.6833\n",
      "Epoch 22/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1216 - accuracy: 0.9623 - val_loss: 1.3466 - val_accuracy: 0.7000\n",
      "Epoch 23/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1062 - accuracy: 0.9677 - val_loss: 1.2891 - val_accuracy: 0.6333\n",
      "Epoch 24/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.1044 - accuracy: 0.9695 - val_loss: 1.3812 - val_accuracy: 0.6667\n",
      "Epoch 25/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0593 - accuracy: 0.9892 - val_loss: 1.1418 - val_accuracy: 0.6833\n",
      "Epoch 26/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0795 - accuracy: 0.9767 - val_loss: 1.4143 - val_accuracy: 0.6167\n",
      "Epoch 27/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0983 - accuracy: 0.9713 - val_loss: 1.7705 - val_accuracy: 0.4833\n",
      "Epoch 28/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0661 - accuracy: 0.9767 - val_loss: 1.2539 - val_accuracy: 0.5833\n",
      "Epoch 29/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0605 - accuracy: 0.9785 - val_loss: 1.3242 - val_accuracy: 0.7500\n",
      "Epoch 30/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0559 - accuracy: 0.9856 - val_loss: 1.5451 - val_accuracy: 0.7500\n",
      "Epoch 31/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0375 - accuracy: 0.9910 - val_loss: 1.8374 - val_accuracy: 0.6833\n",
      "Epoch 32/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0406 - accuracy: 0.9892 - val_loss: 1.4263 - val_accuracy: 0.6333\n",
      "Epoch 33/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0386 - accuracy: 0.9892 - val_loss: 1.9777 - val_accuracy: 0.4833\n",
      "Epoch 34/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0504 - accuracy: 0.9803 - val_loss: 1.6587 - val_accuracy: 0.7000\n",
      "Epoch 35/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0173 - accuracy: 0.9982 - val_loss: 1.5804 - val_accuracy: 0.6833\n",
      "Epoch 36/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0344 - accuracy: 0.9892 - val_loss: 1.8145 - val_accuracy: 0.7000\n",
      "Epoch 37/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0313 - accuracy: 0.9910 - val_loss: 1.5991 - val_accuracy: 0.6500\n",
      "Epoch 38/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0306 - accuracy: 0.9910 - val_loss: 1.5735 - val_accuracy: 0.6500\n",
      "Epoch 39/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0223 - accuracy: 0.9910 - val_loss: 1.5878 - val_accuracy: 0.6500\n",
      "Epoch 40/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0233 - accuracy: 0.9928 - val_loss: 1.6418 - val_accuracy: 0.7000\n",
      "Epoch 41/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0206 - accuracy: 0.9946 - val_loss: 1.7960 - val_accuracy: 0.6333\n",
      "Epoch 42/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0125 - accuracy: 0.9964 - val_loss: 1.4457 - val_accuracy: 0.6833\n",
      "Epoch 43/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0234 - accuracy: 0.9928 - val_loss: 1.5418 - val_accuracy: 0.7333\n",
      "Epoch 44/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0245 - accuracy: 0.9892 - val_loss: 1.4268 - val_accuracy: 0.6667\n",
      "Epoch 45/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0141 - accuracy: 0.9982 - val_loss: 1.9443 - val_accuracy: 0.7333\n",
      "Epoch 46/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0146 - accuracy: 0.9946 - val_loss: 2.1963 - val_accuracy: 0.5167\n",
      "Epoch 47/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0340 - accuracy: 0.9910 - val_loss: 2.4066 - val_accuracy: 0.7167\n",
      "Epoch 48/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0200 - accuracy: 0.9964 - val_loss: 2.5520 - val_accuracy: 0.7167\n",
      "Epoch 49/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0107 - accuracy: 0.9982 - val_loss: 1.5543 - val_accuracy: 0.7167\n",
      "Epoch 50/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0129 - accuracy: 0.9946 - val_loss: 1.7296 - val_accuracy: 0.6000\n",
      "Epoch 51/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0188 - accuracy: 0.9964 - val_loss: 2.1373 - val_accuracy: 0.7333\n",
      "Epoch 52/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0152 - accuracy: 0.9964 - val_loss: 1.8238 - val_accuracy: 0.7333\n",
      "Epoch 53/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0316 - accuracy: 0.9856 - val_loss: 2.2035 - val_accuracy: 0.6667\n",
      "Epoch 54/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0297 - accuracy: 0.9892 - val_loss: 1.7862 - val_accuracy: 0.6000\n",
      "Epoch 55/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0233 - accuracy: 0.9892 - val_loss: 1.6192 - val_accuracy: 0.6167\n",
      "Epoch 56/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0205 - accuracy: 0.9964 - val_loss: 1.6198 - val_accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0180 - accuracy: 0.9964 - val_loss: 2.0739 - val_accuracy: 0.4667\n",
      "Epoch 58/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0153 - accuracy: 0.9946 - val_loss: 2.0166 - val_accuracy: 0.6833\n",
      "Epoch 59/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0221 - accuracy: 0.9946 - val_loss: 1.7937 - val_accuracy: 0.5833\n",
      "Epoch 60/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0111 - accuracy: 0.9982 - val_loss: 1.7799 - val_accuracy: 0.7333\n",
      "Epoch 61/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0063 - accuracy: 0.9982 - val_loss: 1.7867 - val_accuracy: 0.7167\n",
      "Epoch 62/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0138 - accuracy: 0.9982 - val_loss: 1.5283 - val_accuracy: 0.7333\n",
      "Epoch 63/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0120 - accuracy: 0.9964 - val_loss: 1.9146 - val_accuracy: 0.7167\n",
      "Epoch 64/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0180 - accuracy: 0.9946 - val_loss: 2.0647 - val_accuracy: 0.7333\n",
      "Epoch 65/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0135 - accuracy: 0.9964 - val_loss: 1.7781 - val_accuracy: 0.6667\n",
      "Epoch 66/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0152 - accuracy: 0.9928 - val_loss: 1.9477 - val_accuracy: 0.7000\n",
      "Epoch 67/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0144 - accuracy: 0.9946 - val_loss: 1.8055 - val_accuracy: 0.6167\n",
      "Epoch 68/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0154 - accuracy: 0.9928 - val_loss: 2.0037 - val_accuracy: 0.7167\n",
      "Epoch 69/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0168 - accuracy: 0.9946 - val_loss: 2.1373 - val_accuracy: 0.7167\n",
      "Epoch 70/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0170 - accuracy: 0.9964 - val_loss: 2.7164 - val_accuracy: 0.7167\n",
      "Epoch 71/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0204 - accuracy: 0.9946 - val_loss: 1.7857 - val_accuracy: 0.7000\n",
      "Epoch 72/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0211 - accuracy: 0.9946 - val_loss: 1.6060 - val_accuracy: 0.7000\n",
      "Epoch 73/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0159 - accuracy: 0.9946 - val_loss: 1.8189 - val_accuracy: 0.7167\n",
      "Epoch 74/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0438 - accuracy: 0.9910 - val_loss: 2.4794 - val_accuracy: 0.5333\n",
      "Epoch 75/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0203 - accuracy: 0.9946 - val_loss: 2.4317 - val_accuracy: 0.5667\n",
      "Epoch 76/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.2037 - val_accuracy: 0.7167\n",
      "Epoch 77/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0145 - accuracy: 0.9964 - val_loss: 2.1503 - val_accuracy: 0.7167\n",
      "Epoch 78/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0082 - accuracy: 0.9964 - val_loss: 2.1062 - val_accuracy: 0.6833\n",
      "Epoch 79/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0083 - accuracy: 0.9982 - val_loss: 1.9279 - val_accuracy: 0.6333\n",
      "Epoch 80/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0155 - accuracy: 0.9946 - val_loss: 2.6178 - val_accuracy: 0.6833\n",
      "Epoch 81/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0120 - accuracy: 0.9946 - val_loss: 3.3074 - val_accuracy: 0.6833\n",
      "Epoch 82/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0190 - accuracy: 0.9946 - val_loss: 1.7467 - val_accuracy: 0.7333\n",
      "Epoch 83/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0419 - accuracy: 0.9838 - val_loss: 3.8857 - val_accuracy: 0.4000\n",
      "Epoch 84/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0403 - accuracy: 0.9838 - val_loss: 2.2378 - val_accuracy: 0.7167\n",
      "Epoch 85/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0093 - accuracy: 0.9982 - val_loss: 2.0427 - val_accuracy: 0.6167\n",
      "Epoch 86/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0220 - accuracy: 0.9928 - val_loss: 2.2077 - val_accuracy: 0.6500\n",
      "Epoch 87/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0040 - accuracy: 0.9982 - val_loss: 2.1529 - val_accuracy: 0.6167\n",
      "Epoch 88/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 2.4478 - val_accuracy: 0.7167\n",
      "Epoch 89/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0080 - accuracy: 0.9982 - val_loss: 2.0695 - val_accuracy: 0.7000\n",
      "Epoch 90/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0186 - accuracy: 0.9928 - val_loss: 2.9018 - val_accuracy: 0.7333\n",
      "Epoch 91/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0090 - accuracy: 0.9964 - val_loss: 2.1985 - val_accuracy: 0.7000\n",
      "Epoch 92/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.0735 - val_accuracy: 0.7000\n",
      "Epoch 93/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.0704 - val_accuracy: 0.6500\n",
      "Epoch 94/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.9812 - val_accuracy: 0.7167\n",
      "Epoch 95/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.1919 - val_accuracy: 0.7167\n",
      "Epoch 96/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0138 - accuracy: 0.9946 - val_loss: 3.3736 - val_accuracy: 0.4167\n",
      "Epoch 97/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0104 - accuracy: 0.9964 - val_loss: 3.0910 - val_accuracy: 0.7000\n",
      "Epoch 98/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0230 - accuracy: 0.9910 - val_loss: 2.4927 - val_accuracy: 0.6833\n",
      "Epoch 99/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0145 - accuracy: 0.9928 - val_loss: 2.0904 - val_accuracy: 0.5000\n",
      "Epoch 100/100\n",
      "557/557 [==============================] - 1s 1ms/sample - loss: 0.0068 - accuracy: 0.9982 - val_loss: 1.7683 - val_accuracy: 0.6667\n",
      "accuracy for model 10 is 66.66666865348816\n",
      "Training Testing Accuracy: 77.71% (6.92%)\n"
     ]
    }
   ],
   "source": [
    "best_DNN = eval_dnn(tt_vcf, tt_pheno, 10, mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9b43f49bc18e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_DNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PoC_DNN_model.pickle.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(best_DNN, open(\"PoC_DNN_model.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout accuracy is 72.25806713104248\n"
     ]
    }
   ],
   "source": [
    "bs = ((ho_vcf.shape[0])/40)\n",
    "bs = round(bs)\n",
    "ho_pheno = mlb.transform(ho_pheno)\n",
    "_, accuracy = best_DNN.evaluate(ho_vcf, ho_pheno, batch_size=bs, verbose=0)\n",
    "print(\"Holdout accuracy is \" + str(accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
