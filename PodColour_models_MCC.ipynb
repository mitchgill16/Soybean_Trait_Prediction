{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT STATEMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.3)\n",
      "Requirement already satisfied: sklearn in ./.local/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: xgboost in ./.local/lib/python3.6/site-packages (1.3.1)\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.6/site-packages (3.3.3)\n",
      "Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0+nv)\n",
      "Requirement already satisfied: keras==2.2.4 in ./.local/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: fastai in ./.local/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (2.8.1)\n",
      "Requirement already satisfied: scikit-optimize in ./.local/lib/python3.6/site-packages (0.8.1)\n",
      "Requirement already satisfied: scikit-learn==0.21 in ./.local/lib/python3.6/site-packages (0.21.0)\n",
      "Requirement already satisfied: graphviz in ./.local/lib/python3.6/site-packages (0.16)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./.local/lib/python3.6/site-packages (from pandas) (2020.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in ./.local/lib/python3.6/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.local/lib/python3.6/site-packages (from matplotlib) (8.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.local/lib/python3.6/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.34.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.14.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.27.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.11.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.9.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (5.3.1)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from fastai) (20.0.2)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.6/site-packages (from fastai) (20.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.23.0)\n",
      "Requirement already satisfied: fastcore<1.4,>=1.3.8 in ./.local/lib/python3.6/site-packages (from fastai) (1.3.18)\n",
      "Requirement already satisfied: torch<1.8,>=1.7.0 in ./.local/lib/python3.6/site-packages (from fastai) (1.7.1)\n",
      "Requirement already satisfied: spacy in ./.local/lib/python3.6/site-packages (from fastai) (2.3.5)\n",
      "Requirement already satisfied: torchvision<0.9,>=0.8 in ./.local/lib/python3.6/site-packages (from fastai) (0.8.2)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in ./.local/lib/python3.6/site-packages (from fastai) (1.0.0)\n",
      "Requirement already satisfied: pyaml>=16.9 in ./.local/lib/python3.6/site-packages (from scikit-optimize) (20.4.0)\n",
      "Requirement already satisfied: joblib>=0.11 in ./.local/lib/python3.6/site-packages (from scikit-optimize) (1.0.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (46.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.11.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.25.8)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in ./.local/lib/python3.6/site-packages (from torch<1.8,>=1.7.0->fastai) (0.8)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.6/site-packages (from torch<1.8,>=1.7.0->fastai) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (4.43.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (3.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (7.4.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (2.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (0.7.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (0.8.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./.local/lib/python3.6/site-packages (from spacy->fastai) (1.1.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->fastai) (1.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!TMPDIR=/home/mgill/ pip install --cache-dir=/home/mgill/ --build /home/mgill/ pandas numpy sklearn xgboost matplotlib tensorflow==2.1.0 keras==2.2.4 fastai python-dateutil scikit-optimize scikit-learn==0.21 graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/mgill/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import tensorflow as tf; print(tf.__version__)\n",
    "import keras; print(keras.__version__)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from random import randint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Activation\n",
    "from math import sqrt\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import skopt\n",
    "from skopt.searchcv import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import interp\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_prep_data(tt_file, ho_file):\n",
    "    imp = SimpleImputer(missing_values='./.', strategy='most_frequent')\n",
    "    my_list = []\n",
    "    x = 0 \n",
    "    for chunk in pd.read_csv(tt_file, chunksize=10000, index_col=\"Unnamed: 0\"):\n",
    "        x=x+10000\n",
    "        chunk = chunk.T\n",
    "        if 'Value' in chunk.columns:\n",
    "            #does the selecting of pheno array for application ML\n",
    "            chunk[\"Value\"] = pd.to_numeric(chunk[\"Value\"], downcast=\"float\")\n",
    "            tt_pheno = chunk[\"Value\"].to_numpy()\n",
    "            #reshapes it so its not a 1D array\n",
    "            print(tt_pheno.shape)\n",
    "            tt_pheno = np.reshape(tt_pheno,(len(tt_pheno),1))\n",
    "            print(tt_pheno.shape)\n",
    "            chunk = chunk.drop(columns=['Value'])\n",
    "        headers = chunk.columns\n",
    "        row_idx = chunk.index\n",
    "        chunk = imp.fit_transform(chunk) #SHOULD TURN ./. into the most common for each column\n",
    "        #since imputing makes a numpy array have to turn back into PD for label encoding\n",
    "        chunk = pd.DataFrame(data = chunk, index = row_idx, columns = headers)\n",
    "        my_list.append(chunk)\n",
    "        print(x)\n",
    "    tt_vcf = pd.concat(my_list, axis = 1)\n",
    "    my_list = []\n",
    "    x=0\n",
    "    for chunk in pd.read_csv(ho_file, chunksize=10000, index_col=\"Unnamed: 0\"):\n",
    "        x=x+10000\n",
    "        chunk = chunk.T\n",
    "        if 'Value' in chunk.columns:\n",
    "            #does the selecting of pheno array for application ML\n",
    "            chunk[\"Value\"] = pd.to_numeric(chunk[\"Value\"], downcast=\"float\")\n",
    "            ho_pheno = chunk[\"Value\"].to_numpy()\n",
    "            #reshapes it so its not a 1D array\n",
    "            print(ho_pheno.shape)\n",
    "            ho_pheno = np.reshape(ho_pheno,(len(ho_pheno),1))\n",
    "            print(ho_pheno.shape)\n",
    "            chunk = chunk.drop(columns=['Value'])\n",
    "        headers = chunk.columns\n",
    "        row_idx = chunk.index\n",
    "        chunk = imp.fit_transform(chunk) #SHOULD TURN ./. into the most common for each column\n",
    "        #since imputing makes a numpy array have to turn back into PD for label encoding\n",
    "        chunk = pd.DataFrame(data = chunk, index = row_idx, columns = headers)\n",
    "        my_list.append(chunk)\n",
    "        print(x)\n",
    "    ho_vcf = pd.concat(my_list, axis = 1)\n",
    "    print(tt_vcf.shape)\n",
    "    print(ho_vcf.shape)\n",
    "    print(tt_pheno.shape)\n",
    "            \n",
    "    return tt_vcf, ho_vcf, tt_pheno, ho_pheno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "(617,)\n",
      "(617, 1)\n",
      "220000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "(155,)\n",
      "(155, 1)\n",
      "220000\n",
      "(617, 214899)\n",
      "(155, 214899)\n",
      "(617, 1)\n"
     ]
    }
   ],
   "source": [
    "tt_vcf, ho_vcf, tt_pheno, ho_pheno = new_prep_data(\"PoC_Merged_filtered.csv_train_test.csv_5pcnt.csv\", \"PoC_Merged_filtered.csv_holdout.csv_5pcnt.csv\")\n",
    "r_t = tt_pheno.ravel()\n",
    "r_h = ho_pheno.ravel()\n",
    "print(r_t[10])\n",
    "i = 0\n",
    "for x in r_t:\n",
    "    if(x==0.5):\n",
    "        r_t[i]=2.0\n",
    "    i = i+1\n",
    "i = 0\n",
    "for x in r_h:\n",
    "    if(x==0.5):\n",
    "        r_h[i]=2.0\n",
    "    i = i+1\n",
    "r_t = np.reshape(r_t,(len(r_t),1))\n",
    "r_h = np.reshape(r_h,(len(r_h),1))\n",
    "tt_pheno = r_t\n",
    "ho_pheno = r_h\n",
    "print(tt_pheno.shape)\n",
    "print(ho_pheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if it hasn't been run and saved before\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "ohe = ohe.fit(tt_vcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ohe, open(\"PoC_ohe.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if need or have new holdout data etc.\n",
    "ohe = pickle.load(open(\"PoC_ohe.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 214899)\n",
      "(617, 623207)\n",
      "(155, 214899)\n",
      "(155, 623207)\n"
     ]
    }
   ],
   "source": [
    "print(tt_vcf.shape)\n",
    "tt_vcf = ohe.transform(tt_vcf)\n",
    "print(tt_vcf.shape)\n",
    "print(ho_vcf.shape)\n",
    "ho_vcf = ohe.transform(ho_vcf)\n",
    "print(ho_vcf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_snp_from_header(ohe,snp_num):\n",
    "    count = 0\n",
    "    snp = \"Not found\"\n",
    "    found = False\n",
    "    i = 0\n",
    "    while i < len(ohe.categories_) and (found == False):\n",
    "        j = 0\n",
    "        while j < len(ohe.categories_[i]):\n",
    "            if(count == snp_num):\n",
    "                snp = ohe.categories_[i][j]\n",
    "                found = True\n",
    "                break\n",
    "            count = count + 1\n",
    "            j = j + 1\n",
    "        i = i + 1\n",
    "    return snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T/C\n"
     ]
    }
   ],
   "source": [
    "## TESTING IF IT WORKS\n",
    "my_snp = find_snp_from_header(ohe, 462793)\n",
    "print(my_snp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 623207)\n",
      "(617, 1)\n",
      "(124, 623207)\n",
      "seed is 2431\n"
     ]
    }
   ],
   "source": [
    "##ONLY NEED TO DO IF OPTIMISING\n",
    "\n",
    "print(tt_vcf.shape)\n",
    "print(tt_pheno.shape)\n",
    "seed = randint(0,5000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tt_vcf, tt_pheno, test_size=0.2, random_state=seed)\n",
    "print(X_test.shape)\n",
    "print(\"seed is \" + str(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "space ={'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "        'min_child_weight': Integer(0, 10),\n",
    "        'max_depth': Integer(0, 50),\n",
    "        'max_delta_step': Integer(0, 20),\n",
    "        'subsample': Real(0.01, 1.0, 'uniform'),\n",
    "        'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n",
    "        'colsample_bylevel': Real(0.01, 1.0, 'uniform'),\n",
    "        'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n",
    "        'gamma': Real(1e-9, 0.5, 'log-uniform'),\n",
    "        'min_child_weight': Integer(0, 5),\n",
    "        'n_estimators': Integer(50, 200),\n",
    "        'scale_pos_weight': Real(1e-6, 500, 'log-uniform')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_step(optim_result):\n",
    "    \"\"\"\n",
    "    Callback meant to view scores after\n",
    "    each iteration while performing Bayesian\n",
    "    Optimization in Skopt\"\"\"\n",
    "    score = xgb_bayes_search.best_score_\n",
    "    print(\"best score: %s\" % score)\n",
    "    if score >= 0.98:\n",
    "        print('Interrupting!')\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:34:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:34:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216, score=0.770, total= 2.8min\n",
      "[CV] colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:36:58] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:37:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216, score=0.778, total= 2.5min\n",
      "[CV] colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:39:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:39:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216, score=0.778, total= 2.5min\n",
      "[CV] colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216 \n",
      "[14:41:57] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:42:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216, score=0.806, total= 2.4min\n",
      "[CV] colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216 \n",
      "[14:44:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:44:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4160029192647807, colsample_bytree=0.7304484857455519, gamma=0.13031389926541354, learning_rate=0.042815319280763466, max_delta_step=13, max_depth=21, min_child_weight=2, n_estimators=161, reg_alpha=5.497557739289786e-07, reg_lambda=0.05936070635912049, scale_pos_weight=0.060830282487222144, subsample=0.13556548021189216, score=0.804, total= 2.5min\n",
      "[CV] colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859 \n",
      "[14:46:51] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:46:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859, score=0.830, total= 3.9min\n",
      "[CV] colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859 \n",
      "[14:50:45] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:50:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859, score=0.808, total= 4.0min\n",
      "[CV] colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859 \n",
      "[14:54:43] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:54:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859, score=0.788, total= 3.6min\n",
      "[CV] colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859 \n",
      "[14:58:18] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:58:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859, score=0.857, total= 4.0min\n",
      "[CV] colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859 \n",
      "[15:02:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:02:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.18630307921210337, colsample_bytree=0.6357746718688816, gamma=4.759632988007942e-09, learning_rate=0.21560448660617149, max_delta_step=14, max_depth=39, min_child_weight=0, n_estimators=198, reg_alpha=1.307103676304351e-05, reg_lambda=1.1326211354138217e-09, scale_pos_weight=22.356393886375308, subsample=0.7121579996266859, score=0.845, total= 3.6min\n",
      "[CV] colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597 \n",
      "[15:05:56] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:05:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597, score=0.840, total= 1.5min\n",
      "[CV] colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597 \n",
      "[15:07:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:07:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597, score=0.828, total= 1.4min\n",
      "[CV] colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597 \n",
      "[15:08:50] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:08:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597, score=0.859, total= 1.7min\n",
      "[CV] colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597 \n",
      "[15:10:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:10:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597, score=0.898, total= 1.4min\n",
      "[CV] colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597 \n",
      "[15:11:52] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:11:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.25617325301227906, colsample_bytree=0.7083937150495909, gamma=2.41812432168581e-07, learning_rate=0.13965555720269418, max_delta_step=10, max_depth=27, min_child_weight=1, n_estimators=76, reg_alpha=3.178148842971562e-08, reg_lambda=0.005381781269387993, scale_pos_weight=0.23835043249575294, subsample=0.9559763235078597, score=0.856, total= 1.6min\n",
      "[CV] colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:13:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:13:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827, score=0.800, total=  43.1s\n",
      "[CV] colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827 \n",
      "[15:14:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:14:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827, score=0.798, total=  47.4s\n",
      "[CV] colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827 \n",
      "[15:15:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:15:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827, score=0.758, total=  48.9s\n",
      "[CV] colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827 \n",
      "[15:15:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:15:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827, score=0.867, total=  52.2s\n",
      "[CV] colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827 \n",
      "[15:16:41] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:16:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7711308526006485, colsample_bytree=0.07988300914246868, gamma=1.3877597085692663e-08, learning_rate=0.15021004353467043, max_delta_step=6, max_depth=3, min_child_weight=3, n_estimators=77, reg_alpha=0.006097622112520179, reg_lambda=1.9368851099909265e-09, scale_pos_weight=6.97020795804701e-06, subsample=0.46717487628832827, score=0.845, total=  44.3s\n",
      "[CV] colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085 \n",
      "[15:17:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:17:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085, score=0.800, total= 3.9min\n",
      "[CV] colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085 \n",
      "[15:21:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:21:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085, score=0.778, total= 4.0min\n",
      "[CV] colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085 \n",
      "[15:25:19] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:25:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085, score=0.747, total= 4.2min\n",
      "[CV] colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085 \n",
      "[15:29:33] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:29:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085, score=0.837, total= 4.3min\n",
      "[CV] colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085 \n",
      "[15:33:52] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:33:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9132100359267209, colsample_bytree=0.7679711803126804, gamma=2.428808912219129e-07, learning_rate=0.03511167856740689, max_delta_step=3, max_depth=8, min_child_weight=4, n_estimators=197, reg_alpha=0.023358352781682152, reg_lambda=2.1396512525873352e-07, scale_pos_weight=0.001760608876847004, subsample=0.15990388740095085, score=0.814, total= 4.0min\n",
      "[CV] colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985 \n",
      "[15:37:52] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:37:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985, score=0.570, total=  41.7s\n",
      "[CV] colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985 \n",
      "[15:38:34] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:38:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985, score=0.556, total=  39.9s\n",
      "[CV] colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985 \n",
      "[15:39:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:39:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985, score=0.475, total=  40.1s\n",
      "[CV] colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:39:54] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:39:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985, score=0.561, total=  41.4s\n",
      "[CV] colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985 \n",
      "[15:40:36] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:40:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3569079180734289, colsample_bytree=0.1120289995668169, gamma=0.05034432042804318, learning_rate=0.7467518194835729, max_delta_step=7, max_depth=47, min_child_weight=1, n_estimators=67, reg_alpha=0.005500281359785164, reg_lambda=6.3166999939833564e-06, scale_pos_weight=0.4420166988445819, subsample=0.037978567417966985, score=0.515, total=  40.3s\n",
      "[CV] colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774 \n",
      "[15:41:16] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:41:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774, score=0.860, total= 2.1min\n",
      "[CV] colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774 \n",
      "[15:43:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:43:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774, score=0.838, total= 2.2min\n",
      "[CV] colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774 \n",
      "[15:45:36] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:45:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774, score=0.848, total= 2.1min\n",
      "[CV] colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774 \n",
      "[15:47:40] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:47:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774, score=0.878, total= 2.3min\n",
      "[CV] colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774 \n",
      "[15:49:58] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:50:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  colsample_bylevel=0.3173884684879576, colsample_bytree=0.9154083808190769, gamma=0.37957777315906627, learning_rate=0.046564465205051725, max_delta_step=5, max_depth=25, min_child_weight=4, n_estimators=98, reg_alpha=1.8932210348902655e-05, reg_lambda=10.393593598030753, scale_pos_weight=0.019183460268621746, subsample=0.6299477217301774, score=0.845, total= 2.3min\n",
      "[CV] colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868 \n",
      "[15:52:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:52:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868, score=0.850, total= 1.8min\n",
      "[CV] colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868 \n",
      "[15:54:05] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:54:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868, score=0.838, total= 1.8min\n",
      "[CV] colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868 \n",
      "[15:55:54] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:55:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868, score=0.838, total= 1.7min\n",
      "[CV] colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868 \n",
      "[15:57:34] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:57:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868, score=0.878, total= 1.9min\n",
      "[CV] colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868 \n",
      "[15:59:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:59:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.19358622710388942, colsample_bytree=0.9236239290551462, gamma=6.242737149649543e-09, learning_rate=0.5839290998374393, max_delta_step=9, max_depth=2, min_child_weight=1, n_estimators=157, reg_alpha=0.32247446903004606, reg_lambda=5.7439751766594195e-06, scale_pos_weight=1.6287615199535114e-06, subsample=0.9872483677632868, score=0.845, total= 1.8min\n",
      "[CV] colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377 \n",
      "[16:01:18] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:01:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377, score=0.770, total= 1.7min\n",
      "[CV] colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:03:02] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:03:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377, score=0.788, total= 1.7min\n",
      "[CV] colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377 \n",
      "[16:04:45] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:04:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377, score=0.697, total= 1.7min\n",
      "[CV] colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377 \n",
      "[16:06:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:06:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377, score=0.776, total= 1.7min\n",
      "[CV] colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377 \n",
      "[16:08:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:08:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.06936552953088004, colsample_bytree=0.5708703891954323, gamma=1.9956389638103137e-08, learning_rate=0.09010297955022104, max_delta_step=9, max_depth=25, min_child_weight=2, n_estimators=139, reg_alpha=0.9824119669778386, reg_lambda=0.00017577858139476563, scale_pos_weight=261.4450653749918, subsample=0.14242164035537377, score=0.763, total= 1.7min\n",
      "[CV] colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144 \n",
      "[16:09:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:09:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144, score=0.840, total= 1.1min\n",
      "[CV] colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144 \n",
      "[16:10:57] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:11:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144, score=0.808, total= 1.2min\n",
      "[CV] colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144 \n",
      "[16:12:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:12:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144, score=0.768, total= 1.2min\n",
      "[CV] colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144 \n",
      "[16:13:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:13:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144, score=0.847, total= 1.1min\n",
      "[CV] colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144 \n",
      "[16:14:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:14:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.1798648261515349, colsample_bytree=0.1536154650973617, gamma=2.692171045057089e-07, learning_rate=0.031045751168128115, max_delta_step=12, max_depth=24, min_child_weight=3, n_estimators=113, reg_alpha=0.0011518554964803237, reg_lambda=3.9270934206560135e-05, scale_pos_weight=0.036328353682153036, subsample=0.5123744729843144, score=0.814, total= 1.1min\n",
      "[CV] colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086 \n",
      "[16:15:39] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:15:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086, score=0.800, total= 2.1min\n",
      "[CV] colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086 \n",
      "[16:17:45] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:17:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086, score=0.818, total= 2.1min\n",
      "[CV] colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086 \n",
      "[16:19:53] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:19:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086, score=0.838, total= 2.1min\n",
      "[CV] colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086 \n",
      "[16:22:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:22:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086, score=0.888, total= 2.1min\n",
      "[CV] colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:24:06] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:24:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8477171831834894, colsample_bytree=0.9590782028716157, gamma=0.4419705200245942, learning_rate=0.010068534554171981, max_delta_step=18, max_depth=46, min_child_weight=5, n_estimators=66, reg_alpha=1.3396471230171212e-09, reg_lambda=0.1502689667792386, scale_pos_weight=5.68219069233494e-06, subsample=0.3682037565911086, score=0.856, total= 2.1min\n",
      "[CV] colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889 \n",
      "[16:26:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:26:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889, score=0.860, total= 2.2min\n",
      "[CV] colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889 \n",
      "[16:28:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:28:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889, score=0.838, total= 2.2min\n",
      "[CV] colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889 \n",
      "[16:30:39] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:30:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889, score=0.808, total= 2.0min\n",
      "[CV] colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889 \n",
      "[16:32:41] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:32:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889, score=0.857, total= 2.0min\n",
      "[CV] colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889 \n",
      "[16:34:42] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:34:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.88542798001491, colsample_bytree=0.15107725561517718, gamma=0.09274068410384222, learning_rate=0.11437510345794359, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=1.2952579279714691e-08, reg_lambda=0.45707493844512304, scale_pos_weight=10.561837873121673, subsample=0.9105119208713889, score=0.845, total= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 122.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.8559837728194726\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=1e-06, subsample=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:36:54] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:36:56] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=1e-06, subsample=1.0, score=0.570, total=  26.7s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=1e-06, subsample=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:37:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:37:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=1e-06, subsample=1.0, score=0.566, total=  24.8s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=1e-06, subsample=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   51.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:37:45] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:37:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=1e-06, subsample=1.0, score=0.566, total=  26.5s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=1e-06, subsample=1.0 \n",
      "[16:38:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:38:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=1e-06, subsample=1.0, score=0.571, total=  27.2s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=1e-06, subsample=1.0 \n",
      "[16:38:39] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:38:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=1e-06, subsample=1.0, score=0.577, total=  24.1s\n",
      "[CV] colsample_bylevel=0.4768509039736716, colsample_bytree=1.0, gamma=0.0025881548048269907, learning_rate=0.12457587154455696, max_delta_step=7, max_depth=7, min_child_weight=2, n_estimators=50, reg_alpha=4.421859999060103e-08, reg_lambda=199.35959016985052, scale_pos_weight=1e-06, subsample=0.9701311374674002 \n",
      "[16:39:03] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:39:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4768509039736716, colsample_bytree=1.0, gamma=0.0025881548048269907, learning_rate=0.12457587154455696, max_delta_step=7, max_depth=7, min_child_weight=2, n_estimators=50, reg_alpha=4.421859999060103e-08, reg_lambda=199.35959016985052, scale_pos_weight=1e-06, subsample=0.9701311374674002, score=0.830, total= 1.4min\n",
      "[CV] colsample_bylevel=0.4768509039736716, colsample_bytree=1.0, gamma=0.0025881548048269907, learning_rate=0.12457587154455696, max_delta_step=7, max_depth=7, min_child_weight=2, n_estimators=50, reg_alpha=4.421859999060103e-08, reg_lambda=199.35959016985052, scale_pos_weight=1e-06, subsample=0.9701311374674002 \n",
      "[16:40:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:40:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4768509039736716, colsample_bytree=1.0, gamma=0.0025881548048269907, learning_rate=0.12457587154455696, max_delta_step=7, max_depth=7, min_child_weight=2, n_estimators=50, reg_alpha=4.421859999060103e-08, reg_lambda=199.35959016985052, scale_pos_weight=1e-06, subsample=0.9701311374674002, score=0.828, total= 1.4min\n",
      "[CV] colsample_bylevel=0.4768509039736716, colsample_bytree=1.0, gamma=0.0025881548048269907, learning_rate=0.12457587154455696, max_delta_step=7, max_depth=7, min_child_weight=2, n_estimators=50, reg_alpha=4.421859999060103e-08, reg_lambda=199.35959016985052, scale_pos_weight=1e-06, subsample=0.9701311374674002 \n",
      "[16:41:51] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:41:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4768509039736716, colsample_bytree=1.0, gamma=0.0025881548048269907, learning_rate=0.12457587154455696, max_delta_step=7, max_depth=7, min_child_weight=2, n_estimators=50, reg_alpha=4.421859999060103e-08, reg_lambda=199.35959016985052, scale_pos_weight=1e-06, subsample=0.9701311374674002, score=0.848, total= 1.4min\n",
      "[CV] colsample_bylevel=0.4768509039736716, colsample_bytree=1.0, gamma=0.0025881548048269907, learning_rate=0.12457587154455696, max_delta_step=7, max_depth=7, min_child_weight=2, n_estimators=50, reg_alpha=4.421859999060103e-08, reg_lambda=199.35959016985052, scale_pos_weight=1e-06, subsample=0.9701311374674002 \n",
      "[16:43:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:43:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4768509039736716, colsample_bytree=1.0, gamma=0.0025881548048269907, learning_rate=0.12457587154455696, max_delta_step=7, max_depth=7, min_child_weight=2, n_estimators=50, reg_alpha=4.421859999060103e-08, reg_lambda=199.35959016985052, scale_pos_weight=1e-06, subsample=0.9701311374674002, score=0.878, total= 1.5min\n",
      "[CV] colsample_bylevel=0.4768509039736716, colsample_bytree=1.0, gamma=0.0025881548048269907, learning_rate=0.12457587154455696, max_delta_step=7, max_depth=7, min_child_weight=2, n_estimators=50, reg_alpha=4.421859999060103e-08, reg_lambda=199.35959016985052, scale_pos_weight=1e-06, subsample=0.9701311374674002 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:44:41] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:44:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.4768509039736716, colsample_bytree=1.0, gamma=0.0025881548048269907, learning_rate=0.12457587154455696, max_delta_step=7, max_depth=7, min_child_weight=2, n_estimators=50, reg_alpha=4.421859999060103e-08, reg_lambda=199.35959016985052, scale_pos_weight=1e-06, subsample=0.9701311374674002, score=0.876, total= 1.4min\n",
      "[CV] colsample_bylevel=0.8443052635102495, colsample_bytree=1.0, gamma=1.9847490557354955e-08, learning_rate=0.03208011039958541, max_delta_step=20, max_depth=50, min_child_weight=1, n_estimators=50, reg_alpha=0.36371764326745093, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[16:46:07] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:46:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8443052635102495, colsample_bytree=1.0, gamma=1.9847490557354955e-08, learning_rate=0.03208011039958541, max_delta_step=20, max_depth=50, min_child_weight=1, n_estimators=50, reg_alpha=0.36371764326745093, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.820, total= 2.9min\n",
      "[CV] colsample_bylevel=0.8443052635102495, colsample_bytree=1.0, gamma=1.9847490557354955e-08, learning_rate=0.03208011039958541, max_delta_step=20, max_depth=50, min_child_weight=1, n_estimators=50, reg_alpha=0.36371764326745093, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[16:49:02] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:49:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8443052635102495, colsample_bytree=1.0, gamma=1.9847490557354955e-08, learning_rate=0.03208011039958541, max_delta_step=20, max_depth=50, min_child_weight=1, n_estimators=50, reg_alpha=0.36371764326745093, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.859, total= 2.9min\n",
      "[CV] colsample_bylevel=0.8443052635102495, colsample_bytree=1.0, gamma=1.9847490557354955e-08, learning_rate=0.03208011039958541, max_delta_step=20, max_depth=50, min_child_weight=1, n_estimators=50, reg_alpha=0.36371764326745093, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[16:51:58] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:52:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8443052635102495, colsample_bytree=1.0, gamma=1.9847490557354955e-08, learning_rate=0.03208011039958541, max_delta_step=20, max_depth=50, min_child_weight=1, n_estimators=50, reg_alpha=0.36371764326745093, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.828, total= 2.9min\n",
      "[CV] colsample_bylevel=0.8443052635102495, colsample_bytree=1.0, gamma=1.9847490557354955e-08, learning_rate=0.03208011039958541, max_delta_step=20, max_depth=50, min_child_weight=1, n_estimators=50, reg_alpha=0.36371764326745093, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[16:54:52] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:54:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8443052635102495, colsample_bytree=1.0, gamma=1.9847490557354955e-08, learning_rate=0.03208011039958541, max_delta_step=20, max_depth=50, min_child_weight=1, n_estimators=50, reg_alpha=0.36371764326745093, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.878, total= 2.9min\n",
      "[CV] colsample_bylevel=0.8443052635102495, colsample_bytree=1.0, gamma=1.9847490557354955e-08, learning_rate=0.03208011039958541, max_delta_step=20, max_depth=50, min_child_weight=1, n_estimators=50, reg_alpha=0.36371764326745093, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[16:57:46] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:57:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.8443052635102495, colsample_bytree=1.0, gamma=1.9847490557354955e-08, learning_rate=0.03208011039958541, max_delta_step=20, max_depth=50, min_child_weight=1, n_estimators=50, reg_alpha=0.36371764326745093, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.887, total= 2.7min\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=1.0, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:00:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:00:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=1.0, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.750, total=  31.1s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=1.0, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:58] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:01:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=1.0, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.788, total=  28.4s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=1.0, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:01:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:01:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=1.0, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.727, total=  29.7s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=1.0, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:01:57] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:01:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=1.0, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.796, total=  28.6s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=1.0, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:02:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:02:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=0.01, gamma=1e-09, learning_rate=1.0, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1e-09, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.753, total=  29.1s\n",
      "[CV] colsample_bylevel=0.5992809585319264, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.37772584733452497, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.2504173134846927e-07, reg_lambda=1000.0, scale_pos_weight=0.8285422508663005, subsample=1.0 \n",
      "[17:02:55] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:02:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5992809585319264, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.37772584733452497, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.2504173134846927e-07, reg_lambda=1000.0, scale_pos_weight=0.8285422508663005, subsample=1.0, score=0.570, total=  27.0s\n",
      "[CV] colsample_bylevel=0.5992809585319264, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.37772584733452497, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.2504173134846927e-07, reg_lambda=1000.0, scale_pos_weight=0.8285422508663005, subsample=1.0 \n",
      "[17:03:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:03:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5992809585319264, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.37772584733452497, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.2504173134846927e-07, reg_lambda=1000.0, scale_pos_weight=0.8285422508663005, subsample=1.0, score=0.566, total=  28.6s\n",
      "[CV] colsample_bylevel=0.5992809585319264, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.37772584733452497, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.2504173134846927e-07, reg_lambda=1000.0, scale_pos_weight=0.8285422508663005, subsample=1.0 \n",
      "[17:03:51] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:03:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5992809585319264, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.37772584733452497, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.2504173134846927e-07, reg_lambda=1000.0, scale_pos_weight=0.8285422508663005, subsample=1.0, score=0.566, total=  28.9s\n",
      "[CV] colsample_bylevel=0.5992809585319264, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.37772584733452497, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.2504173134846927e-07, reg_lambda=1000.0, scale_pos_weight=0.8285422508663005, subsample=1.0 \n",
      "[17:04:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:04:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5992809585319264, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.37772584733452497, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.2504173134846927e-07, reg_lambda=1000.0, scale_pos_weight=0.8285422508663005, subsample=1.0, score=0.571, total=  28.5s\n",
      "[CV] colsample_bylevel=0.5992809585319264, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.37772584733452497, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.2504173134846927e-07, reg_lambda=1000.0, scale_pos_weight=0.8285422508663005, subsample=1.0 \n",
      "[17:04:48] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:04:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5992809585319264, colsample_bytree=0.01, gamma=1e-09, learning_rate=0.37772584733452497, max_delta_step=20, max_depth=0, min_child_weight=5, n_estimators=50, reg_alpha=1.2504173134846927e-07, reg_lambda=1000.0, scale_pos_weight=0.8285422508663005, subsample=1.0, score=0.577, total=  26.4s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1.706391877722961e-09, learning_rate=0.01, max_delta_step=0, max_depth=50, min_child_weight=4, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.7443205462537579 \n",
      "[17:05:15] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:05:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1.706391877722961e-09, learning_rate=0.01, max_delta_step=0, max_depth=50, min_child_weight=4, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.7443205462537579, score=0.770, total=  56.5s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1.706391877722961e-09, learning_rate=0.01, max_delta_step=0, max_depth=50, min_child_weight=4, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.7443205462537579 \n",
      "[17:06:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:06:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1.706391877722961e-09, learning_rate=0.01, max_delta_step=0, max_depth=50, min_child_weight=4, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.7443205462537579, score=0.788, total= 1.5min\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1.706391877722961e-09, learning_rate=0.01, max_delta_step=0, max_depth=50, min_child_weight=4, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.7443205462537579 \n",
      "[17:07:44] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:07:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1.706391877722961e-09, learning_rate=0.01, max_delta_step=0, max_depth=50, min_child_weight=4, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.7443205462537579, score=0.717, total= 1.6min\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1.706391877722961e-09, learning_rate=0.01, max_delta_step=0, max_depth=50, min_child_weight=4, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.7443205462537579 \n",
      "[17:09:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:09:20] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1.706391877722961e-09, learning_rate=0.01, max_delta_step=0, max_depth=50, min_child_weight=4, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.7443205462537579, score=0.827, total= 1.4min\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1.706391877722961e-09, learning_rate=0.01, max_delta_step=0, max_depth=50, min_child_weight=4, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.7443205462537579 \n",
      "[17:10:44] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:10:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=1.706391877722961e-09, learning_rate=0.01, max_delta_step=0, max_depth=50, min_child_weight=4, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=0.7443205462537579, score=0.825, total= 1.3min\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.06083328540655388, max_delta_step=0, max_depth=50, min_child_weight=2, n_estimators=50, reg_alpha=1e-09, reg_lambda=0.003070765266077698, scale_pos_weight=1e-06, subsample=0.7786806739694166 \n",
      "[17:12:01] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:12:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.06083328540655388, max_delta_step=0, max_depth=50, min_child_weight=2, n_estimators=50, reg_alpha=1e-09, reg_lambda=0.003070765266077698, scale_pos_weight=1e-06, subsample=0.7786806739694166, score=0.850, total= 1.9min\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.06083328540655388, max_delta_step=0, max_depth=50, min_child_weight=2, n_estimators=50, reg_alpha=1e-09, reg_lambda=0.003070765266077698, scale_pos_weight=1e-06, subsample=0.7786806739694166 \n",
      "[17:13:57] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:14:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.06083328540655388, max_delta_step=0, max_depth=50, min_child_weight=2, n_estimators=50, reg_alpha=1e-09, reg_lambda=0.003070765266077698, scale_pos_weight=1e-06, subsample=0.7786806739694166, score=0.838, total= 1.9min\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.06083328540655388, max_delta_step=0, max_depth=50, min_child_weight=2, n_estimators=50, reg_alpha=1e-09, reg_lambda=0.003070765266077698, scale_pos_weight=1e-06, subsample=0.7786806739694166 \n",
      "[17:15:51] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:15:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.06083328540655388, max_delta_step=0, max_depth=50, min_child_weight=2, n_estimators=50, reg_alpha=1e-09, reg_lambda=0.003070765266077698, scale_pos_weight=1e-06, subsample=0.7786806739694166, score=0.838, total= 1.8min\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.06083328540655388, max_delta_step=0, max_depth=50, min_child_weight=2, n_estimators=50, reg_alpha=1e-09, reg_lambda=0.003070765266077698, scale_pos_weight=1e-06, subsample=0.7786806739694166 \n",
      "[17:17:41] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:17:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.06083328540655388, max_delta_step=0, max_depth=50, min_child_weight=2, n_estimators=50, reg_alpha=1e-09, reg_lambda=0.003070765266077698, scale_pos_weight=1e-06, subsample=0.7786806739694166, score=0.918, total= 1.8min\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.06083328540655388, max_delta_step=0, max_depth=50, min_child_weight=2, n_estimators=50, reg_alpha=1e-09, reg_lambda=0.003070765266077698, scale_pos_weight=1e-06, subsample=0.7786806739694166 \n",
      "[17:19:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:19:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.06083328540655388, max_delta_step=0, max_depth=50, min_child_weight=2, n_estimators=50, reg_alpha=1e-09, reg_lambda=0.003070765266077698, scale_pos_weight=1e-06, subsample=0.7786806739694166, score=0.866, total= 1.9min\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[17:21:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:21:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.570, total=  28.2s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[17:21:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:21:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.566, total=  24.7s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[17:22:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:22:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.566, total=  24.1s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.01 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:22:39] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:22:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.571, total=  26.5s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.01 \n",
      "[17:23:05] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:23:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=0, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.01, score=0.577, total=  25.6s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.021377853358445702, max_delta_step=1, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.8361094528141384 \n",
      "[17:23:31] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:23:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.021377853358445702, max_delta_step=1, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.8361094528141384, score=0.570, total=  26.0s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.021377853358445702, max_delta_step=1, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.8361094528141384 \n",
      "[17:23:57] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:24:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.021377853358445702, max_delta_step=1, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.8361094528141384, score=0.566, total=  26.5s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.021377853358445702, max_delta_step=1, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.8361094528141384 \n",
      "[17:24:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:24:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.021377853358445702, max_delta_step=1, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.8361094528141384, score=0.566, total=  27.4s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.021377853358445702, max_delta_step=1, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.8361094528141384 \n",
      "[17:24:51] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:24:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.021377853358445702, max_delta_step=1, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.8361094528141384, score=0.571, total=  26.7s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.021377853358445702, max_delta_step=1, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.8361094528141384 \n",
      "[17:25:18] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:25:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=0.021377853358445702, max_delta_step=1, max_depth=0, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1000.0, scale_pos_weight=499.99999999999994, subsample=0.8361094528141384, score=0.577, total=  27.2s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:25:46] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:25:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.690, total=  34.0s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:26:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:26:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.758, total=  31.3s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:26:51] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:26:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.687, total=  33.3s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:27:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:27:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.776, total=  29.7s\n",
      "[CV] colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:27:55] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:27:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.01, colsample_bytree=0.01, gamma=0.49999999999999994, learning_rate=0.01, max_delta_step=20, max_depth=50, min_child_weight=0, n_estimators=50, reg_alpha=1.0, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.753, total=  31.0s\n",
      "[CV] colsample_bylevel=0.02033988572040353, colsample_bytree=0.6867075332192113, gamma=0.00010085792273827948, learning_rate=0.09909420592676477, max_delta_step=11, max_depth=28, min_child_weight=5, n_estimators=59, reg_alpha=5.19048059662466e-09, reg_lambda=1.895793252681232e-09, scale_pos_weight=0.7286137596066893, subsample=0.8422629944483357 \n",
      "[17:28:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:28:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.02033988572040353, colsample_bytree=0.6867075332192113, gamma=0.00010085792273827948, learning_rate=0.09909420592676477, max_delta_step=11, max_depth=28, min_child_weight=5, n_estimators=59, reg_alpha=5.19048059662466e-09, reg_lambda=1.895793252681232e-09, scale_pos_weight=0.7286137596066893, subsample=0.8422629944483357, score=0.800, total= 1.1min\n",
      "[CV] colsample_bylevel=0.02033988572040353, colsample_bytree=0.6867075332192113, gamma=0.00010085792273827948, learning_rate=0.09909420592676477, max_delta_step=11, max_depth=28, min_child_weight=5, n_estimators=59, reg_alpha=5.19048059662466e-09, reg_lambda=1.895793252681232e-09, scale_pos_weight=0.7286137596066893, subsample=0.8422629944483357 \n",
      "[17:29:33] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:29:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.02033988572040353, colsample_bytree=0.6867075332192113, gamma=0.00010085792273827948, learning_rate=0.09909420592676477, max_delta_step=11, max_depth=28, min_child_weight=5, n_estimators=59, reg_alpha=5.19048059662466e-09, reg_lambda=1.895793252681232e-09, scale_pos_weight=0.7286137596066893, subsample=0.8422629944483357, score=0.808, total= 1.1min\n",
      "[CV] colsample_bylevel=0.02033988572040353, colsample_bytree=0.6867075332192113, gamma=0.00010085792273827948, learning_rate=0.09909420592676477, max_delta_step=11, max_depth=28, min_child_weight=5, n_estimators=59, reg_alpha=5.19048059662466e-09, reg_lambda=1.895793252681232e-09, scale_pos_weight=0.7286137596066893, subsample=0.8422629944483357 \n",
      "[17:30:42] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:30:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.02033988572040353, colsample_bytree=0.6867075332192113, gamma=0.00010085792273827948, learning_rate=0.09909420592676477, max_delta_step=11, max_depth=28, min_child_weight=5, n_estimators=59, reg_alpha=5.19048059662466e-09, reg_lambda=1.895793252681232e-09, scale_pos_weight=0.7286137596066893, subsample=0.8422629944483357, score=0.768, total= 1.1min\n",
      "[CV] colsample_bylevel=0.02033988572040353, colsample_bytree=0.6867075332192113, gamma=0.00010085792273827948, learning_rate=0.09909420592676477, max_delta_step=11, max_depth=28, min_child_weight=5, n_estimators=59, reg_alpha=5.19048059662466e-09, reg_lambda=1.895793252681232e-09, scale_pos_weight=0.7286137596066893, subsample=0.8422629944483357 \n",
      "[17:31:48] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:31:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.02033988572040353, colsample_bytree=0.6867075332192113, gamma=0.00010085792273827948, learning_rate=0.09909420592676477, max_delta_step=11, max_depth=28, min_child_weight=5, n_estimators=59, reg_alpha=5.19048059662466e-09, reg_lambda=1.895793252681232e-09, scale_pos_weight=0.7286137596066893, subsample=0.8422629944483357, score=0.847, total= 1.2min\n",
      "[CV] colsample_bylevel=0.02033988572040353, colsample_bytree=0.6867075332192113, gamma=0.00010085792273827948, learning_rate=0.09909420592676477, max_delta_step=11, max_depth=28, min_child_weight=5, n_estimators=59, reg_alpha=5.19048059662466e-09, reg_lambda=1.895793252681232e-09, scale_pos_weight=0.7286137596066893, subsample=0.8422629944483357 \n",
      "[17:32:57] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:33:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.02033988572040353, colsample_bytree=0.6867075332192113, gamma=0.00010085792273827948, learning_rate=0.09909420592676477, max_delta_step=11, max_depth=28, min_child_weight=5, n_estimators=59, reg_alpha=5.19048059662466e-09, reg_lambda=1.895793252681232e-09, scale_pos_weight=0.7286137596066893, subsample=0.8422629944483357, score=0.856, total=  52.1s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=1.0, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:33:50] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:33:54] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=1.0, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.840, total=  45.1s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=1.0, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:34:35] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:34:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=1.0, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.828, total=  48.9s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=1.0, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:35:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:35:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=1.0, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.848, total=  44.6s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=1.0, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:36:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:36:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=1.0, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.867, total=  48.1s\n",
      "[CV] colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=1.0, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0 \n",
      "[17:36:57] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:37:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=1.0, colsample_bytree=1.0, gamma=0.49999999999999994, learning_rate=1.0, max_delta_step=20, max_depth=50, min_child_weight=5, n_estimators=50, reg_alpha=1e-09, reg_lambda=1e-09, scale_pos_weight=499.99999999999994, subsample=1.0, score=0.835, total=  53.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 61.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.8620689655172413\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] colsample_bylevel=0.48822684112670245, colsample_bytree=0.7605772669100791, gamma=3.102622104751858e-08, learning_rate=0.20386878491795582, max_delta_step=9, max_depth=26, min_child_weight=0, n_estimators=105, reg_alpha=2.9226281370072575e-06, reg_lambda=1000.0, scale_pos_weight=0.0028734416587893508, subsample=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:37:58] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:38:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.48822684112670245, colsample_bytree=0.7605772669100791, gamma=3.102622104751858e-08, learning_rate=0.20386878491795582, max_delta_step=9, max_depth=26, min_child_weight=0, n_estimators=105, reg_alpha=2.9226281370072575e-06, reg_lambda=1000.0, scale_pos_weight=0.0028734416587893508, subsample=1.0, score=0.850, total= 4.3min\n",
      "[CV] colsample_bylevel=0.48822684112670245, colsample_bytree=0.7605772669100791, gamma=3.102622104751858e-08, learning_rate=0.20386878491795582, max_delta_step=9, max_depth=26, min_child_weight=0, n_estimators=105, reg_alpha=2.9226281370072575e-06, reg_lambda=1000.0, scale_pos_weight=0.0028734416587893508, subsample=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:42:18] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:42:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.48822684112670245, colsample_bytree=0.7605772669100791, gamma=3.102622104751858e-08, learning_rate=0.20386878491795582, max_delta_step=9, max_depth=26, min_child_weight=0, n_estimators=105, reg_alpha=2.9226281370072575e-06, reg_lambda=1000.0, scale_pos_weight=0.0028734416587893508, subsample=1.0, score=0.828, total= 4.3min\n",
      "[CV] colsample_bylevel=0.48822684112670245, colsample_bytree=0.7605772669100791, gamma=3.102622104751858e-08, learning_rate=0.20386878491795582, max_delta_step=9, max_depth=26, min_child_weight=0, n_estimators=105, reg_alpha=2.9226281370072575e-06, reg_lambda=1000.0, scale_pos_weight=0.0028734416587893508, subsample=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  8.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:46:37] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:46:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.48822684112670245, colsample_bytree=0.7605772669100791, gamma=3.102622104751858e-08, learning_rate=0.20386878491795582, max_delta_step=9, max_depth=26, min_child_weight=0, n_estimators=105, reg_alpha=2.9226281370072575e-06, reg_lambda=1000.0, scale_pos_weight=0.0028734416587893508, subsample=1.0, score=0.848, total= 4.0min\n",
      "[CV] colsample_bylevel=0.48822684112670245, colsample_bytree=0.7605772669100791, gamma=3.102622104751858e-08, learning_rate=0.20386878491795582, max_delta_step=9, max_depth=26, min_child_weight=0, n_estimators=105, reg_alpha=2.9226281370072575e-06, reg_lambda=1000.0, scale_pos_weight=0.0028734416587893508, subsample=1.0 \n",
      "[17:50:40] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:50:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.48822684112670245, colsample_bytree=0.7605772669100791, gamma=3.102622104751858e-08, learning_rate=0.20386878491795582, max_delta_step=9, max_depth=26, min_child_weight=0, n_estimators=105, reg_alpha=2.9226281370072575e-06, reg_lambda=1000.0, scale_pos_weight=0.0028734416587893508, subsample=1.0, score=0.878, total= 4.0min\n",
      "[CV] colsample_bylevel=0.48822684112670245, colsample_bytree=0.7605772669100791, gamma=3.102622104751858e-08, learning_rate=0.20386878491795582, max_delta_step=9, max_depth=26, min_child_weight=0, n_estimators=105, reg_alpha=2.9226281370072575e-06, reg_lambda=1000.0, scale_pos_weight=0.0028734416587893508, subsample=1.0 \n",
      "[17:54:40] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:54:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.48822684112670245, colsample_bytree=0.7605772669100791, gamma=3.102622104751858e-08, learning_rate=0.20386878491795582, max_delta_step=9, max_depth=26, min_child_weight=0, n_estimators=105, reg_alpha=2.9226281370072575e-06, reg_lambda=1000.0, scale_pos_weight=0.0028734416587893508, subsample=1.0, score=0.856, total= 3.9min\n",
      "[CV] colsample_bylevel=0.14683089443311095, colsample_bytree=0.9994449561597772, gamma=4.593920824030072e-09, learning_rate=0.14885404747775002, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=83, reg_alpha=7.985517464495955e-09, reg_lambda=9.480081417491835, scale_pos_weight=1.0707843615848027e-06, subsample=0.053470408016864805 \n",
      "[17:58:34] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:58:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.14683089443311095, colsample_bytree=0.9994449561597772, gamma=4.593920824030072e-09, learning_rate=0.14885404747775002, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=83, reg_alpha=7.985517464495955e-09, reg_lambda=9.480081417491835, scale_pos_weight=1.0707843615848027e-06, subsample=0.053470408016864805, score=0.690, total= 1.2min\n",
      "[CV] colsample_bylevel=0.14683089443311095, colsample_bytree=0.9994449561597772, gamma=4.593920824030072e-09, learning_rate=0.14885404747775002, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=83, reg_alpha=7.985517464495955e-09, reg_lambda=9.480081417491835, scale_pos_weight=1.0707843615848027e-06, subsample=0.053470408016864805 \n",
      "[17:59:44] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:59:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.14683089443311095, colsample_bytree=0.9994449561597772, gamma=4.593920824030072e-09, learning_rate=0.14885404747775002, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=83, reg_alpha=7.985517464495955e-09, reg_lambda=9.480081417491835, scale_pos_weight=1.0707843615848027e-06, subsample=0.053470408016864805, score=0.727, total= 1.3min\n",
      "[CV] colsample_bylevel=0.14683089443311095, colsample_bytree=0.9994449561597772, gamma=4.593920824030072e-09, learning_rate=0.14885404747775002, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=83, reg_alpha=7.985517464495955e-09, reg_lambda=9.480081417491835, scale_pos_weight=1.0707843615848027e-06, subsample=0.053470408016864805 \n",
      "[18:01:01] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:01:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.14683089443311095, colsample_bytree=0.9994449561597772, gamma=4.593920824030072e-09, learning_rate=0.14885404747775002, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=83, reg_alpha=7.985517464495955e-09, reg_lambda=9.480081417491835, scale_pos_weight=1.0707843615848027e-06, subsample=0.053470408016864805, score=0.677, total= 1.3min\n",
      "[CV] colsample_bylevel=0.14683089443311095, colsample_bytree=0.9994449561597772, gamma=4.593920824030072e-09, learning_rate=0.14885404747775002, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=83, reg_alpha=7.985517464495955e-09, reg_lambda=9.480081417491835, scale_pos_weight=1.0707843615848027e-06, subsample=0.053470408016864805 \n",
      "[18:02:16] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:02:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  colsample_bylevel=0.14683089443311095, colsample_bytree=0.9994449561597772, gamma=4.593920824030072e-09, learning_rate=0.14885404747775002, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=83, reg_alpha=7.985517464495955e-09, reg_lambda=9.480081417491835, scale_pos_weight=1.0707843615848027e-06, subsample=0.053470408016864805, score=0.724, total= 1.2min\n",
      "[CV] colsample_bylevel=0.14683089443311095, colsample_bytree=0.9994449561597772, gamma=4.593920824030072e-09, learning_rate=0.14885404747775002, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=83, reg_alpha=7.985517464495955e-09, reg_lambda=9.480081417491835, scale_pos_weight=1.0707843615848027e-06, subsample=0.053470408016864805 \n",
      "[18:03:28] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:03:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.14683089443311095, colsample_bytree=0.9994449561597772, gamma=4.593920824030072e-09, learning_rate=0.14885404747775002, max_delta_step=0, max_depth=50, min_child_weight=0, n_estimators=83, reg_alpha=7.985517464495955e-09, reg_lambda=9.480081417491835, scale_pos_weight=1.0707843615848027e-06, subsample=0.053470408016864805, score=0.711, total= 1.2min\n",
      "[CV] colsample_bylevel=0.9076259858834325, colsample_bytree=1.0, gamma=1.768275322039507e-07, learning_rate=0.43407729977950316, max_delta_step=10, max_depth=29, min_child_weight=1, n_estimators=146, reg_alpha=1.3863159863174651e-06, reg_lambda=1e-09, scale_pos_weight=6.315651324365968e-06, subsample=0.6983659610730831 \n",
      "[18:04:42] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:04:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9076259858834325, colsample_bytree=1.0, gamma=1.768275322039507e-07, learning_rate=0.43407729977950316, max_delta_step=10, max_depth=29, min_child_weight=1, n_estimators=146, reg_alpha=1.3863159863174651e-06, reg_lambda=1e-09, scale_pos_weight=6.315651324365968e-06, subsample=0.6983659610730831, score=0.830, total= 2.8min\n",
      "[CV] colsample_bylevel=0.9076259858834325, colsample_bytree=1.0, gamma=1.768275322039507e-07, learning_rate=0.43407729977950316, max_delta_step=10, max_depth=29, min_child_weight=1, n_estimators=146, reg_alpha=1.3863159863174651e-06, reg_lambda=1e-09, scale_pos_weight=6.315651324365968e-06, subsample=0.6983659610730831 \n",
      "[18:07:31] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:07:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9076259858834325, colsample_bytree=1.0, gamma=1.768275322039507e-07, learning_rate=0.43407729977950316, max_delta_step=10, max_depth=29, min_child_weight=1, n_estimators=146, reg_alpha=1.3863159863174651e-06, reg_lambda=1e-09, scale_pos_weight=6.315651324365968e-06, subsample=0.6983659610730831, score=0.828, total= 2.9min\n",
      "[CV] colsample_bylevel=0.9076259858834325, colsample_bytree=1.0, gamma=1.768275322039507e-07, learning_rate=0.43407729977950316, max_delta_step=10, max_depth=29, min_child_weight=1, n_estimators=146, reg_alpha=1.3863159863174651e-06, reg_lambda=1e-09, scale_pos_weight=6.315651324365968e-06, subsample=0.6983659610730831 \n",
      "[18:10:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:10:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9076259858834325, colsample_bytree=1.0, gamma=1.768275322039507e-07, learning_rate=0.43407729977950316, max_delta_step=10, max_depth=29, min_child_weight=1, n_estimators=146, reg_alpha=1.3863159863174651e-06, reg_lambda=1e-09, scale_pos_weight=6.315651324365968e-06, subsample=0.6983659610730831, score=0.828, total= 2.9min\n",
      "[CV] colsample_bylevel=0.9076259858834325, colsample_bytree=1.0, gamma=1.768275322039507e-07, learning_rate=0.43407729977950316, max_delta_step=10, max_depth=29, min_child_weight=1, n_estimators=146, reg_alpha=1.3863159863174651e-06, reg_lambda=1e-09, scale_pos_weight=6.315651324365968e-06, subsample=0.6983659610730831 \n",
      "[18:13:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:13:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9076259858834325, colsample_bytree=1.0, gamma=1.768275322039507e-07, learning_rate=0.43407729977950316, max_delta_step=10, max_depth=29, min_child_weight=1, n_estimators=146, reg_alpha=1.3863159863174651e-06, reg_lambda=1e-09, scale_pos_weight=6.315651324365968e-06, subsample=0.6983659610730831, score=0.888, total= 2.9min\n",
      "[CV] colsample_bylevel=0.9076259858834325, colsample_bytree=1.0, gamma=1.768275322039507e-07, learning_rate=0.43407729977950316, max_delta_step=10, max_depth=29, min_child_weight=1, n_estimators=146, reg_alpha=1.3863159863174651e-06, reg_lambda=1e-09, scale_pos_weight=6.315651324365968e-06, subsample=0.6983659610730831 \n",
      "[18:16:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:16:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.9076259858834325, colsample_bytree=1.0, gamma=1.768275322039507e-07, learning_rate=0.43407729977950316, max_delta_step=10, max_depth=29, min_child_weight=1, n_estimators=146, reg_alpha=1.3863159863174651e-06, reg_lambda=1e-09, scale_pos_weight=6.315651324365968e-06, subsample=0.6983659610730831, score=0.835, total= 3.1min\n",
      "[CV] colsample_bylevel=0.7846176622243527, colsample_bytree=0.972829065423114, gamma=1.3483631756165142e-05, learning_rate=0.0397336756042424, max_delta_step=8, max_depth=16, min_child_weight=0, n_estimators=147, reg_alpha=0.01558887411264243, reg_lambda=6.474799133946946, scale_pos_weight=2.746113892093292e-06, subsample=0.6707614366879879 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:19:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:19:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7846176622243527, colsample_bytree=0.972829065423114, gamma=1.3483631756165142e-05, learning_rate=0.0397336756042424, max_delta_step=8, max_depth=16, min_child_weight=0, n_estimators=147, reg_alpha=0.01558887411264243, reg_lambda=6.474799133946946, scale_pos_weight=2.746113892093292e-06, subsample=0.6707614366879879, score=0.850, total= 6.9min\n",
      "[CV] colsample_bylevel=0.7846176622243527, colsample_bytree=0.972829065423114, gamma=1.3483631756165142e-05, learning_rate=0.0397336756042424, max_delta_step=8, max_depth=16, min_child_weight=0, n_estimators=147, reg_alpha=0.01558887411264243, reg_lambda=6.474799133946946, scale_pos_weight=2.746113892093292e-06, subsample=0.6707614366879879 \n",
      "[18:26:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:26:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7846176622243527, colsample_bytree=0.972829065423114, gamma=1.3483631756165142e-05, learning_rate=0.0397336756042424, max_delta_step=8, max_depth=16, min_child_weight=0, n_estimators=147, reg_alpha=0.01558887411264243, reg_lambda=6.474799133946946, scale_pos_weight=2.746113892093292e-06, subsample=0.6707614366879879, score=0.838, total= 6.9min\n",
      "[CV] colsample_bylevel=0.7846176622243527, colsample_bytree=0.972829065423114, gamma=1.3483631756165142e-05, learning_rate=0.0397336756042424, max_delta_step=8, max_depth=16, min_child_weight=0, n_estimators=147, reg_alpha=0.01558887411264243, reg_lambda=6.474799133946946, scale_pos_weight=2.746113892093292e-06, subsample=0.6707614366879879 \n",
      "[18:33:05] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:33:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7846176622243527, colsample_bytree=0.972829065423114, gamma=1.3483631756165142e-05, learning_rate=0.0397336756042424, max_delta_step=8, max_depth=16, min_child_weight=0, n_estimators=147, reg_alpha=0.01558887411264243, reg_lambda=6.474799133946946, scale_pos_weight=2.746113892093292e-06, subsample=0.6707614366879879, score=0.859, total= 6.9min\n",
      "[CV] colsample_bylevel=0.7846176622243527, colsample_bytree=0.972829065423114, gamma=1.3483631756165142e-05, learning_rate=0.0397336756042424, max_delta_step=8, max_depth=16, min_child_weight=0, n_estimators=147, reg_alpha=0.01558887411264243, reg_lambda=6.474799133946946, scale_pos_weight=2.746113892093292e-06, subsample=0.6707614366879879 \n",
      "[18:39:57] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:40:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7846176622243527, colsample_bytree=0.972829065423114, gamma=1.3483631756165142e-05, learning_rate=0.0397336756042424, max_delta_step=8, max_depth=16, min_child_weight=0, n_estimators=147, reg_alpha=0.01558887411264243, reg_lambda=6.474799133946946, scale_pos_weight=2.746113892093292e-06, subsample=0.6707614366879879, score=0.878, total= 6.8min\n",
      "[CV] colsample_bylevel=0.7846176622243527, colsample_bytree=0.972829065423114, gamma=1.3483631756165142e-05, learning_rate=0.0397336756042424, max_delta_step=8, max_depth=16, min_child_weight=0, n_estimators=147, reg_alpha=0.01558887411264243, reg_lambda=6.474799133946946, scale_pos_weight=2.746113892093292e-06, subsample=0.6707614366879879 \n",
      "[18:46:45] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:46:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.7846176622243527, colsample_bytree=0.972829065423114, gamma=1.3483631756165142e-05, learning_rate=0.0397336756042424, max_delta_step=8, max_depth=16, min_child_weight=0, n_estimators=147, reg_alpha=0.01558887411264243, reg_lambda=6.474799133946946, scale_pos_weight=2.746113892093292e-06, subsample=0.6707614366879879, score=0.866, total= 7.0min\n",
      "[CV] colsample_bylevel=0.07210635895242454, colsample_bytree=0.7983847077302229, gamma=1e-09, learning_rate=0.025184339136998492, max_delta_step=10, max_depth=29, min_child_weight=2, n_estimators=180, reg_alpha=0.0374170096509949, reg_lambda=1.1873995590128428, scale_pos_weight=0.8379344744529977, subsample=0.8987752907001196 \n",
      "[18:53:43] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:53:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.07210635895242454, colsample_bytree=0.7983847077302229, gamma=1e-09, learning_rate=0.025184339136998492, max_delta_step=10, max_depth=29, min_child_weight=2, n_estimators=180, reg_alpha=0.0374170096509949, reg_lambda=1.1873995590128428, scale_pos_weight=0.8379344744529977, subsample=0.8987752907001196, score=0.860, total= 3.6min\n",
      "[CV] colsample_bylevel=0.07210635895242454, colsample_bytree=0.7983847077302229, gamma=1e-09, learning_rate=0.025184339136998492, max_delta_step=10, max_depth=29, min_child_weight=2, n_estimators=180, reg_alpha=0.0374170096509949, reg_lambda=1.1873995590128428, scale_pos_weight=0.8379344744529977, subsample=0.8987752907001196 \n",
      "[18:57:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:57:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  colsample_bylevel=0.07210635895242454, colsample_bytree=0.7983847077302229, gamma=1e-09, learning_rate=0.025184339136998492, max_delta_step=10, max_depth=29, min_child_weight=2, n_estimators=180, reg_alpha=0.0374170096509949, reg_lambda=1.1873995590128428, scale_pos_weight=0.8379344744529977, subsample=0.8987752907001196, score=0.838, total= 2.9min\n",
      "[CV] colsample_bylevel=0.07210635895242454, colsample_bytree=0.7983847077302229, gamma=1e-09, learning_rate=0.025184339136998492, max_delta_step=10, max_depth=29, min_child_weight=2, n_estimators=180, reg_alpha=0.0374170096509949, reg_lambda=1.1873995590128428, scale_pos_weight=0.8379344744529977, subsample=0.8987752907001196 \n",
      "[19:00:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:00:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.07210635895242454, colsample_bytree=0.7983847077302229, gamma=1e-09, learning_rate=0.025184339136998492, max_delta_step=10, max_depth=29, min_child_weight=2, n_estimators=180, reg_alpha=0.0374170096509949, reg_lambda=1.1873995590128428, scale_pos_weight=0.8379344744529977, subsample=0.8987752907001196, score=0.838, total= 3.4min\n",
      "[CV] colsample_bylevel=0.07210635895242454, colsample_bytree=0.7983847077302229, gamma=1e-09, learning_rate=0.025184339136998492, max_delta_step=10, max_depth=29, min_child_weight=2, n_estimators=180, reg_alpha=0.0374170096509949, reg_lambda=1.1873995590128428, scale_pos_weight=0.8379344744529977, subsample=0.8987752907001196 \n",
      "[19:03:39] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:03:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.07210635895242454, colsample_bytree=0.7983847077302229, gamma=1e-09, learning_rate=0.025184339136998492, max_delta_step=10, max_depth=29, min_child_weight=2, n_estimators=180, reg_alpha=0.0374170096509949, reg_lambda=1.1873995590128428, scale_pos_weight=0.8379344744529977, subsample=0.8987752907001196, score=0.888, total= 3.3min\n",
      "[CV] colsample_bylevel=0.07210635895242454, colsample_bytree=0.7983847077302229, gamma=1e-09, learning_rate=0.025184339136998492, max_delta_step=10, max_depth=29, min_child_weight=2, n_estimators=180, reg_alpha=0.0374170096509949, reg_lambda=1.1873995590128428, scale_pos_weight=0.8379344744529977, subsample=0.8987752907001196 \n",
      "[19:07:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:07:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.07210635895242454, colsample_bytree=0.7983847077302229, gamma=1e-09, learning_rate=0.025184339136998492, max_delta_step=10, max_depth=29, min_child_weight=2, n_estimators=180, reg_alpha=0.0374170096509949, reg_lambda=1.1873995590128428, scale_pos_weight=0.8379344744529977, subsample=0.8987752907001196, score=0.856, total= 3.2min\n",
      "[CV] colsample_bylevel=0.3649766029594872, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.27743611891638503, max_delta_step=4, max_depth=50, min_child_weight=1, n_estimators=198, reg_alpha=1.3468386879342492e-09, reg_lambda=7.00425315341386e-05, scale_pos_weight=1e-06, subsample=0.7675329374838437 \n",
      "[19:10:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:10:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3649766029594872, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.27743611891638503, max_delta_step=4, max_depth=50, min_child_weight=1, n_estimators=198, reg_alpha=1.3468386879342492e-09, reg_lambda=7.00425315341386e-05, scale_pos_weight=1e-06, subsample=0.7675329374838437, score=0.860, total= 2.9min\n",
      "[CV] colsample_bylevel=0.3649766029594872, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.27743611891638503, max_delta_step=4, max_depth=50, min_child_weight=1, n_estimators=198, reg_alpha=1.3468386879342492e-09, reg_lambda=7.00425315341386e-05, scale_pos_weight=1e-06, subsample=0.7675329374838437 \n",
      "[19:13:05] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:13:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3649766029594872, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.27743611891638503, max_delta_step=4, max_depth=50, min_child_weight=1, n_estimators=198, reg_alpha=1.3468386879342492e-09, reg_lambda=7.00425315341386e-05, scale_pos_weight=1e-06, subsample=0.7675329374838437, score=0.848, total= 3.0min\n",
      "[CV] colsample_bylevel=0.3649766029594872, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.27743611891638503, max_delta_step=4, max_depth=50, min_child_weight=1, n_estimators=198, reg_alpha=1.3468386879342492e-09, reg_lambda=7.00425315341386e-05, scale_pos_weight=1e-06, subsample=0.7675329374838437 \n",
      "[19:16:05] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:16:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3649766029594872, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.27743611891638503, max_delta_step=4, max_depth=50, min_child_weight=1, n_estimators=198, reg_alpha=1.3468386879342492e-09, reg_lambda=7.00425315341386e-05, scale_pos_weight=1e-06, subsample=0.7675329374838437, score=0.818, total= 2.8min\n",
      "[CV] colsample_bylevel=0.3649766029594872, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.27743611891638503, max_delta_step=4, max_depth=50, min_child_weight=1, n_estimators=198, reg_alpha=1.3468386879342492e-09, reg_lambda=7.00425315341386e-05, scale_pos_weight=1e-06, subsample=0.7675329374838437 \n",
      "[19:18:56] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:19:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3649766029594872, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.27743611891638503, max_delta_step=4, max_depth=50, min_child_weight=1, n_estimators=198, reg_alpha=1.3468386879342492e-09, reg_lambda=7.00425315341386e-05, scale_pos_weight=1e-06, subsample=0.7675329374838437, score=0.898, total= 2.9min\n",
      "[CV] colsample_bylevel=0.3649766029594872, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.27743611891638503, max_delta_step=4, max_depth=50, min_child_weight=1, n_estimators=198, reg_alpha=1.3468386879342492e-09, reg_lambda=7.00425315341386e-05, scale_pos_weight=1e-06, subsample=0.7675329374838437 \n",
      "[19:21:51] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:21:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.3649766029594872, colsample_bytree=1.0, gamma=1e-09, learning_rate=0.27743611891638503, max_delta_step=4, max_depth=50, min_child_weight=1, n_estimators=198, reg_alpha=1.3468386879342492e-09, reg_lambda=7.00425315341386e-05, scale_pos_weight=1e-06, subsample=0.7675329374838437, score=0.835, total= 3.0min\n",
      "[CV] colsample_bylevel=0.22343051982072445, colsample_bytree=0.8025761504269208, gamma=3.037264596539688e-09, learning_rate=0.012011158382787894, max_delta_step=10, max_depth=25, min_child_weight=1, n_estimators=60, reg_alpha=1.766824433712305e-07, reg_lambda=2.8998014117380543e-09, scale_pos_weight=1.1254925064441485e-05, subsample=0.6445091307056292 \n",
      "[19:24:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:24:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.22343051982072445, colsample_bytree=0.8025761504269208, gamma=3.037264596539688e-09, learning_rate=0.012011158382787894, max_delta_step=10, max_depth=25, min_child_weight=1, n_estimators=60, reg_alpha=1.766824433712305e-07, reg_lambda=2.8998014117380543e-09, scale_pos_weight=1.1254925064441485e-05, subsample=0.6445091307056292, score=0.840, total= 3.0min\n",
      "[CV] colsample_bylevel=0.22343051982072445, colsample_bytree=0.8025761504269208, gamma=3.037264596539688e-09, learning_rate=0.012011158382787894, max_delta_step=10, max_depth=25, min_child_weight=1, n_estimators=60, reg_alpha=1.766824433712305e-07, reg_lambda=2.8998014117380543e-09, scale_pos_weight=1.1254925064441485e-05, subsample=0.6445091307056292 \n",
      "[19:27:48] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:27:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.22343051982072445, colsample_bytree=0.8025761504269208, gamma=3.037264596539688e-09, learning_rate=0.012011158382787894, max_delta_step=10, max_depth=25, min_child_weight=1, n_estimators=60, reg_alpha=1.766824433712305e-07, reg_lambda=2.8998014117380543e-09, scale_pos_weight=1.1254925064441485e-05, subsample=0.6445091307056292, score=0.859, total= 2.8min\n",
      "[CV] colsample_bylevel=0.22343051982072445, colsample_bytree=0.8025761504269208, gamma=3.037264596539688e-09, learning_rate=0.012011158382787894, max_delta_step=10, max_depth=25, min_child_weight=1, n_estimators=60, reg_alpha=1.766824433712305e-07, reg_lambda=2.8998014117380543e-09, scale_pos_weight=1.1254925064441485e-05, subsample=0.6445091307056292 \n",
      "[19:30:33] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:30:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.22343051982072445, colsample_bytree=0.8025761504269208, gamma=3.037264596539688e-09, learning_rate=0.012011158382787894, max_delta_step=10, max_depth=25, min_child_weight=1, n_estimators=60, reg_alpha=1.766824433712305e-07, reg_lambda=2.8998014117380543e-09, scale_pos_weight=1.1254925064441485e-05, subsample=0.6445091307056292, score=0.848, total= 2.4min\n",
      "[CV] colsample_bylevel=0.22343051982072445, colsample_bytree=0.8025761504269208, gamma=3.037264596539688e-09, learning_rate=0.012011158382787894, max_delta_step=10, max_depth=25, min_child_weight=1, n_estimators=60, reg_alpha=1.766824433712305e-07, reg_lambda=2.8998014117380543e-09, scale_pos_weight=1.1254925064441485e-05, subsample=0.6445091307056292 \n",
      "[19:33:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:33:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.22343051982072445, colsample_bytree=0.8025761504269208, gamma=3.037264596539688e-09, learning_rate=0.012011158382787894, max_delta_step=10, max_depth=25, min_child_weight=1, n_estimators=60, reg_alpha=1.766824433712305e-07, reg_lambda=2.8998014117380543e-09, scale_pos_weight=1.1254925064441485e-05, subsample=0.6445091307056292, score=0.898, total= 2.4min\n",
      "[CV] colsample_bylevel=0.22343051982072445, colsample_bytree=0.8025761504269208, gamma=3.037264596539688e-09, learning_rate=0.012011158382787894, max_delta_step=10, max_depth=25, min_child_weight=1, n_estimators=60, reg_alpha=1.766824433712305e-07, reg_lambda=2.8998014117380543e-09, scale_pos_weight=1.1254925064441485e-05, subsample=0.6445091307056292 \n",
      "[19:35:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:35:29] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.22343051982072445, colsample_bytree=0.8025761504269208, gamma=3.037264596539688e-09, learning_rate=0.012011158382787894, max_delta_step=10, max_depth=25, min_child_weight=1, n_estimators=60, reg_alpha=1.766824433712305e-07, reg_lambda=2.8998014117380543e-09, scale_pos_weight=1.1254925064441485e-05, subsample=0.6445091307056292, score=0.866, total= 3.1min\n",
      "[CV] colsample_bylevel=0.5156713964316169, colsample_bytree=0.9393361153664864, gamma=8.192997405245567e-05, learning_rate=0.6319965814108763, max_delta_step=6, max_depth=34, min_child_weight=1, n_estimators=178, reg_alpha=4.425087335578294e-09, reg_lambda=0.024629886028452785, scale_pos_weight=1e-06, subsample=0.7578492056623087 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:38:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:38:34] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5156713964316169, colsample_bytree=0.9393361153664864, gamma=8.192997405245567e-05, learning_rate=0.6319965814108763, max_delta_step=6, max_depth=34, min_child_weight=1, n_estimators=178, reg_alpha=4.425087335578294e-09, reg_lambda=0.024629886028452785, scale_pos_weight=1e-06, subsample=0.7578492056623087, score=0.840, total= 2.6min\n",
      "[CV] colsample_bylevel=0.5156713964316169, colsample_bytree=0.9393361153664864, gamma=8.192997405245567e-05, learning_rate=0.6319965814108763, max_delta_step=6, max_depth=34, min_child_weight=1, n_estimators=178, reg_alpha=4.425087335578294e-09, reg_lambda=0.024629886028452785, scale_pos_weight=1e-06, subsample=0.7578492056623087 \n",
      "[19:41:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:41:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5156713964316169, colsample_bytree=0.9393361153664864, gamma=8.192997405245567e-05, learning_rate=0.6319965814108763, max_delta_step=6, max_depth=34, min_child_weight=1, n_estimators=178, reg_alpha=4.425087335578294e-09, reg_lambda=0.024629886028452785, scale_pos_weight=1e-06, subsample=0.7578492056623087, score=0.828, total= 2.5min\n",
      "[CV] colsample_bylevel=0.5156713964316169, colsample_bytree=0.9393361153664864, gamma=8.192997405245567e-05, learning_rate=0.6319965814108763, max_delta_step=6, max_depth=34, min_child_weight=1, n_estimators=178, reg_alpha=4.425087335578294e-09, reg_lambda=0.024629886028452785, scale_pos_weight=1e-06, subsample=0.7578492056623087 \n",
      "[19:43:40] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:43:44] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5156713964316169, colsample_bytree=0.9393361153664864, gamma=8.192997405245567e-05, learning_rate=0.6319965814108763, max_delta_step=6, max_depth=34, min_child_weight=1, n_estimators=178, reg_alpha=4.425087335578294e-09, reg_lambda=0.024629886028452785, scale_pos_weight=1e-06, subsample=0.7578492056623087, score=0.848, total= 2.5min\n",
      "[CV] colsample_bylevel=0.5156713964316169, colsample_bytree=0.9393361153664864, gamma=8.192997405245567e-05, learning_rate=0.6319965814108763, max_delta_step=6, max_depth=34, min_child_weight=1, n_estimators=178, reg_alpha=4.425087335578294e-09, reg_lambda=0.024629886028452785, scale_pos_weight=1e-06, subsample=0.7578492056623087 \n",
      "[19:46:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:46:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5156713964316169, colsample_bytree=0.9393361153664864, gamma=8.192997405245567e-05, learning_rate=0.6319965814108763, max_delta_step=6, max_depth=34, min_child_weight=1, n_estimators=178, reg_alpha=4.425087335578294e-09, reg_lambda=0.024629886028452785, scale_pos_weight=1e-06, subsample=0.7578492056623087, score=0.847, total= 2.5min\n",
      "[CV] colsample_bylevel=0.5156713964316169, colsample_bytree=0.9393361153664864, gamma=8.192997405245567e-05, learning_rate=0.6319965814108763, max_delta_step=6, max_depth=34, min_child_weight=1, n_estimators=178, reg_alpha=4.425087335578294e-09, reg_lambda=0.024629886028452785, scale_pos_weight=1e-06, subsample=0.7578492056623087 \n",
      "[19:48:40] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:48:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  colsample_bylevel=0.5156713964316169, colsample_bytree=0.9393361153664864, gamma=8.192997405245567e-05, learning_rate=0.6319965814108763, max_delta_step=6, max_depth=34, min_child_weight=1, n_estimators=178, reg_alpha=4.425087335578294e-09, reg_lambda=0.024629886028452785, scale_pos_weight=1e-06, subsample=0.7578492056623087, score=0.825, total= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 133.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.8620689655172413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:51:05] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:51:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "OrderedDict([('colsample_bylevel', 1.0), ('colsample_bytree', 1.0), ('gamma', 1e-09), ('learning_rate', 0.06083328540655388), ('max_delta_step', 0), ('max_depth', 50), ('min_child_weight', 2), ('n_estimators', 50), ('reg_alpha', 1e-09), ('reg_lambda', 0.003070765266077698), ('scale_pos_weight', 1e-06), ('subsample', 0.7786806739694166)])\n"
     ]
    }
   ],
   "source": [
    "xgbcl = xgb.XGBClassifier(objective='multi:softmax', num_class=3)\n",
    "xgb_bayes_search = BayesSearchCV(xgbcl, space, n_iter=32, # specify how many iterations\n",
    "                                    scoring=None, n_jobs=1, cv=5, verbose=3, random_state=42, n_points=12,\n",
    "                                 refit=True)\n",
    "xgb_bayes_search.fit(X_train, y_train.ravel(), callback = on_step)\n",
    "print(xgb_bayes_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('colsample_bylevel', 1.0), ('colsample_bytree', 1.0), ('gamma', 1e-09), ('learning_rate', 0.06083328540655388), ('max_delta_step', 0), ('max_depth', 50), ('min_child_weight', 2), ('n_estimators', 50), ('reg_alpha', 1e-09), ('reg_lambda', 0.003070765266077698), ('scale_pos_weight', 1e-06), ('subsample', 0.7786806739694166)])\n"
     ]
    }
   ],
   "source": [
    "print(xgb_bayes_search.best_params_)\n",
    "model = xgb.XGBClassifier(**xgb_bayes_search.best_params_, objective='multi:softmax', num_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##if not in same run as optimisation\n",
    "best_params = OrderedDict([('colsample_bylevel', 1.0), ('colsample_bytree', 1.0), ('gamma', 1e-09), ('learning_rate', 0.06083328540655388), ('max_delta_step', 0), ('max_depth', 50), ('min_child_weight', 2), ('n_estimators', 50), ('reg_alpha', 1e-09), ('reg_lambda', 0.003070765266077698), ('scale_pos_weight', 1e-06), ('subsample', 0.7786806739694166)])\n",
    "model = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO MORE STUFF HERE when OPTIMISED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_k_fold(m, x, y, k, hx, hy):\n",
    "    #model: xgboost model, should be with the best params available\n",
    "    #x: input data (eg. all samples and SNPS)\n",
    "    #y: labels\n",
    "    #k: number of folds for cross validation\n",
    "    cv = StratifiedKFold(n_splits=k,shuffle=False)\n",
    "    fig1 = plt.figure(figsize=[12,12])\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    results = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    high = 0\n",
    "    best = m\n",
    "    i = 1\n",
    "    for train,test in cv.split(x,y):\n",
    "        prediction = m.fit(x[train],y[train].ravel()).predict_proba(x[test])\n",
    "        print(\"variables for auroc curve done. Processing fold accuracy + checking best model\")\n",
    "        y_pred = m.predict(x[test])\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        #sees how accurate the model was when testing the test set\n",
    "        accuracy = accuracy_score(y[test], predictions)\n",
    "        pcent = accuracy * 100.0\n",
    "        print(\"The accuracy of this model is\" + str(pcent))\n",
    "        if(pcent > high):\n",
    "            high = pcent\n",
    "            best = m\n",
    "       # fpr, tpr, t = roc_curve(y[test], prediction[:, 1])\n",
    "       # tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "       # roc_auc = auc(fpr, tpr)\n",
    "       # aucs.append(roc_auc)\n",
    "        results.append(pcent)\n",
    "       # plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "        i= i+1\n",
    "\n",
    "   # plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "   # mean_tpr = np.mean(tprs, axis=0)\n",
    "   # mean_auc = auc(mean_fpr, mean_tpr)\n",
    "   # plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "    #         label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "    \n",
    "    holdout_pred = best.predict(hx)\n",
    "    predictions = [round(value) for value in holdout_pred]\n",
    "    #sees how accurate the model was when testing the test set\n",
    "    accuracy = accuracy_score(hy, predictions)\n",
    "    pcent = accuracy * 100.0\n",
    "    print(pcent)\n",
    "    xgb_predictions = best.predict(hx)\n",
    "    xgb_probs = best.predict_proba(hx)[:, 1]\n",
    "    #model_fpr, model_tpr, my_roccy = evaluate_model(xgb_predictions, xgb_probs, hy)\n",
    "    #plt.plot(model_fpr, model_tpr, 'r', label = 'Holdout Data'+my_roccy, lw=2)\n",
    "    \n",
    "    #plt.xlabel('False Positive Rate')\n",
    "    #plt.ylabel('True Positive Rate')\n",
    "    #plt.title('ROC Flower Colour Training Model & Holdout Data')\n",
    "    #plt.legend(loc=\"lower right\")\n",
    "    #plt.show()\n",
    "\n",
    "    print(\"Training Testing Accuracy: %.2f%% (%.2f%%)\" % (np.mean(results), np.std(results)))\n",
    "    print(\"Holdout Accuracy: %.2f%%\" % (pcent))\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 623207)\n",
      "(617, 1)\n",
      "(155, 623207)\n",
      "(155, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:48:00] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:48:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is85.71428571428571\n",
      "[22:51:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:51:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is92.06349206349206\n",
      "[22:54:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:54:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is93.65079365079364\n",
      "[22:57:45] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[22:57:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is92.06349206349206\n",
      "[23:00:53] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:00:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is87.09677419354838\n",
      "[23:04:01] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:04:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is86.88524590163934\n",
      "[23:07:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:07:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is95.08196721311475\n",
      "[23:10:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:10:26] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is77.04918032786885\n",
      "[23:13:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:13:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is75.0\n",
      "[23:16:32] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:16:38] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "variables for auroc curve done. Processing fold accuracy + checking best model\n",
      "The accuracy of this model is73.33333333333333\n",
      "84.51612903225806\n",
      "Training Testing Accuracy: 85.79% (7.60%)\n",
      "Holdout Accuracy: 84.52%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(tt_vcf.shape)\n",
    "print(tt_pheno.shape)\n",
    "print(ho_vcf.shape)\n",
    "print(ho_pheno.shape)\n",
    " #if optimised in same session, other enter manually below\n",
    "#this function should average out 10 folds and training, with inital params optimised\n",
    "#average accuracy and std should be calculated along with a nice AUROC graph of train/test models\n",
    "#best model should be extracted for use on holdout set\n",
    "best_model = eval_k_fold(model, tt_vcf, tt_pheno, 10, ho_vcf, ho_pheno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(best_model, open(\"PoC_kfold_10_tt_from_all.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only load if not generated in same session\n",
    "best_model = pickle.load(open(\"PoC_kfold_10_tt_from_all.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO SNPS of importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "#best_model = pickle.load(open(\"FC_kfold_10_tt_from_all.pickle.dat\", \"rb\"))\n",
    "plt.figure(figsize = (20, 20))\n",
    "plot_importance(best_model, max_num_features=15, importance_type='gain', height=0.3)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my X value is: 28537\n",
      "(617, 28537)\n",
      "my header list is: 28537\n",
      "my X value is: 57374\n",
      "(617, 28837)\n",
      "my header list is: 57374\n",
      "my X value is: 86375\n",
      "(617, 29001)\n",
      "my header list is: 86375\n",
      "my X value is: 115537\n",
      "(617, 29162)\n",
      "my header list is: 115537\n",
      "my X value is: 144307\n",
      "(617, 28770)\n",
      "my header list is: 144307\n",
      "my X value is: 173535\n",
      "(617, 29228)\n",
      "my header list is: 173535\n",
      "my X value is: 202727\n",
      "(617, 29192)\n",
      "my header list is: 202727\n",
      "my X value is: 231683\n",
      "(617, 28956)\n",
      "my header list is: 231683\n",
      "my X value is: 260481\n",
      "(617, 28798)\n",
      "my header list is: 260481\n",
      "my X value is: 289251\n",
      "(617, 28770)\n",
      "my header list is: 289251\n",
      "my X value is: 318304\n",
      "(617, 29053)\n",
      "my header list is: 318304\n",
      "my X value is: 347245\n",
      "(617, 28941)\n",
      "my header list is: 347245\n",
      "my X value is: 376423\n",
      "(617, 29178)\n",
      "my header list is: 376423\n",
      "my X value is: 405294\n",
      "(617, 28871)\n",
      "my header list is: 405294\n",
      "my X value is: 434640\n",
      "(617, 29346)\n",
      "my header list is: 434640\n",
      "my X value is: 463726\n",
      "(617, 29086)\n",
      "my header list is: 463726\n",
      "my X value is: 492911\n",
      "(617, 29185)\n",
      "my header list is: 492911\n",
      "my X value is: 522029\n",
      "(617, 29118)\n",
      "my header list is: 522029\n",
      "my X value is: 551090\n",
      "(617, 29061)\n",
      "my header list is: 551090\n",
      "my X value is: 579966\n",
      "(617, 28876)\n",
      "my header list is: 579966\n",
      "my X value is: 608869\n",
      "(617, 28903)\n",
      "my header list is: 608869\n",
      "dropping value so it doesn't include that in headers\n",
      "my X value is: 623207\n",
      "(617, 14338)\n",
      "my header list is: 623207\n",
      "623207\n"
     ]
    }
   ],
   "source": [
    "#This function essentially returns an array of dataframe headers the length of OHE'd input SNPs for training data\n",
    "#EG. It will be able to determine that feature 357310 is Gm13_17683957 but not what allele it is\n",
    "#eg. feature 357309 357310 and 357311 may all be one hot encoded versions of all possible values of Gm13_17683957\n",
    "#iterating through the saved OHE will by able to determine what specific allele the feature is but cannot determine\n",
    "#what SNP header it belongs to. Therefore combining these two methods you can determine both allele and SNP\n",
    "snp = []\n",
    "imp = SimpleImputer(missing_values='./.', strategy='most_frequent')\n",
    "fs_ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "x = 0\n",
    "n_headers = []\n",
    "le = LabelEncoder()\n",
    "#while (i < 10):\n",
    "for chunk in pd.read_csv(\"PoC_Merged_filtered.csv_train_test.csv_5pcnt.csv\", chunksize=10000, index_col=\"Unnamed: 0\"):\n",
    "    chunk = chunk.T\n",
    "    if 'Value' in chunk.columns:\n",
    "        print(\"dropping value so it doesn't include that in headers\")\n",
    "        chunk = chunk.drop(columns=['Value'])\n",
    "    headers = chunk.columns\n",
    "    row_idx = chunk.index\n",
    "    chunk = imp.fit_transform(chunk) #SHOULD TURN ./. into the most common for each column\n",
    "    #since imputing makes a numpy array have to turn back into PD for label encoding\n",
    "    chunk = pd.DataFrame(data = chunk, index = row_idx, columns = headers)\n",
    "    chunk = chunk.apply(lambda col: le.fit_transform(col))\n",
    "    c_headers = chunk.columns\n",
    "    y = 0\n",
    "    for column in chunk:\n",
    "        d = (chunk[column].nunique())\n",
    "        n_headers.extend([c_headers[y] for i in range(d)])\n",
    "        #print(n_headers)\n",
    "        #print(l)\n",
    "        #n_headers.append(c_headers[y] * d)\n",
    "        #print(n_headers)\n",
    "        y = y + 1\n",
    "    #to double check that it would indeed be one hot encoded with this amount of columns\n",
    "    chunk = fs_ohe.fit_transform(chunk)\n",
    "    x = x + chunk.shape[1]\n",
    "    print(\"my X value is: \" + str(x))\n",
    "    print(chunk.shape)\n",
    "    print(\"my header list is: \" + str(len(n_headers)))\n",
    "print(len(n_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gm05_11968079 (A/A)\n",
      "Gm01_55053811 (T/T)\n",
      "Gm02_41961414 (A/A)\n",
      "Gm17_6746570 (C/C)\n",
      "Gm08_183702 (G/G)\n",
      "Gm19_6434610 (A/A)\n",
      "Gm07_17356456 (A/A)\n",
      "Gm02_49207571 (C/C)\n",
      "Gm12_18950685 (A/A)\n",
      "Gm13_16076593 (G/G)\n",
      "Gm07_45624847 (C/C)\n",
      "Gm07_42559917 (T/T)\n",
      "Gm05_6303805 (A/A)\n",
      "Gm19_4824644 (A/A)\n",
      "Gm03_642218 (C/C)\n",
      "15\n",
      "15\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(10, 10), dpi=100, facecolor='w', edgecolor='k')\n",
    "fs = [136052,30299,54586,479986,211027,561565,198737,59473,336396,356471,210689,209071,133882,560181,61728]\n",
    "scores = [39.5,38.1,34.4,32.3,32.2,29.2,26.2,23.7,23.5,22.113,22.107,21.58,21.56,19.2,18.6]\n",
    "snp_label = []\n",
    "for jj in fs:\n",
    "    jj_allele = find_snp_from_header(ohe, jj)\n",
    "    this_snp = (n_headers[jj] + ' ('+str(jj_allele)+')')\n",
    "    print(this_snp)\n",
    "    snp_label.append(this_snp)\n",
    "snp_label.reverse()\n",
    "scores.reverse()\n",
    "print(len(scores))\n",
    "print(len(snp_label))\n",
    "plt.barh(snp_label,scores)\n",
    "plt.title('SNP Importance XGBoost Pod Colour')\n",
    "plt.ylabel('SNP Label')\n",
    "plt.xlabel('Relative F_Score (GAIN)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = best_model.get_booster().get_score(importance_type=\"gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_f_header(fn,n_headers,ohe):\n",
    "    fn = fn[1:]\n",
    "    fn = int(fn)\n",
    "    allele = find_snp_from_header(ohe, fn)\n",
    "    this_snp = (n_headers[fn] + ' ('+str(allele)+')')\n",
    "    return this_snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n"
     ]
    }
   ],
   "source": [
    "#convert feature to actual SNP name\n",
    "i = 0\n",
    "new_dict = {}\n",
    "for key in my_dict:\n",
    "    new_key = rename_f_header(key, n_headers, ohe)\n",
    "    new_dict[new_key] = my_dict[key]\n",
    "    i = i + 1\n",
    "    print(str(i))\n",
    "    if(my_dict):\n",
    "        continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gm03_642218 (C/C)      18.576057\n",
      "Gm01_4670080 (G/G)      0.414410\n",
      "Gm08_183702 (G/G)      32.243211\n",
      "Gm03_481617 (C/C)       9.107324\n",
      "Gm17_28441916 (C/T)     6.203692\n",
      "                         ...    \n",
      "Gm05_42185208 (G/G)     0.691419\n",
      "Gm16_4456665 (A/A)      0.497086\n",
      "Gm01_52694957 (A/A)     0.086651\n",
      "Gm01_44369368 (A/A)     0.002682\n",
      "Gm15_39127919 (T/T)     0.352772\n",
      "Length: 2093, dtype: float64\n",
      "                     F_Score(GAIN)\n",
      "Gm03_642218 (C/C)        18.576057\n",
      "Gm01_4670080 (G/G)        0.414410\n",
      "Gm08_183702 (G/G)        32.243211\n",
      "Gm03_481617 (C/C)         9.107324\n",
      "Gm17_28441916 (C/T)       6.203692\n",
      "...                            ...\n",
      "Gm05_42185208 (G/G)       0.691419\n",
      "Gm16_4456665 (A/A)        0.497086\n",
      "Gm01_52694957 (A/A)       0.086651\n",
      "Gm01_44369368 (A/A)       0.002682\n",
      "Gm15_39127919 (T/T)       0.352772\n",
      "\n",
      "[2093 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "new_fi = pd.Series(new_dict)\n",
    "print(new_fi)\n",
    "df = new_fi.to_frame()\n",
    "df = df.rename(columns = {0:'F_Score(GAIN)'})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(10, 10), dpi=100, facecolor='w', edgecolor='k')\n",
    "indexes = df.nlargest(20, \"F_Score(GAIN)\").index\n",
    "values = df.nlargest(20, \"F_Score(GAIN)\").values.ravel()\n",
    "indexes = indexes[::-1]\n",
    "values = values[::-1]\n",
    "plt.barh(indexes, values)\n",
    "plt.title('SNP Importance XGBoost Pod Colour')\n",
    "plt.ylabel('SNP Label')\n",
    "plt.xlabel('Relative F_Score (GAIN)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#generate figure object\n",
    "figure(num=None, figsize=(10, 10), dpi=100, facecolor='w', edgecolor='k')\n",
    "#load in the 20 lardest values and their SNP label\n",
    "indexes = df.nlargest(20, \"F_Score(GAIN)\").index\n",
    "values = df.nlargest(20, \"F_Score(GAIN)\").values.ravel()\n",
    "#reverse to make the largest be at the front\n",
    "indexes = indexes[::-1]\n",
    "values = values[::-1]\n",
    "#for each different chromosome you want to colour add a index(*_i) and value (*_v) array\n",
    "#black would be colour for singular/notinteresting chromosomes\n",
    "r_i = []\n",
    "r_v = []\n",
    "b_i = []\n",
    "b_v = []\n",
    "g_i = []\n",
    "g_v = []\n",
    "y_i = []\n",
    "y_v = []\n",
    "bl_i = []\n",
    "bl_v = []\n",
    "p_i = []\n",
    "p_v = []\n",
    "br_i = []\n",
    "br_v = []\n",
    "pu_i = []\n",
    "pu_v = []\n",
    "#for each value in the top n (default 20) check which chromosome it belongs to and add it to the colour array\n",
    "i = 0\n",
    "while i < len(indexes):\n",
    "    if('Gm03' in indexes[i]):\n",
    "        r_i.append(indexes[i])\n",
    "        r_v.append(values[i])\n",
    "    elif('Gm19' in indexes[i]):\n",
    "        b_i.append(indexes[i])\n",
    "        b_v.append(values[i])\n",
    "    elif('Gm05' in indexes[i]):\n",
    "        g_i.append(indexes[i])\n",
    "        g_v.append(values[i])\n",
    "    elif('Gm02' in indexes[i]):\n",
    "        y_i.append(indexes[i])\n",
    "        y_v.append(values[i])\n",
    "    elif('Gm07' in indexes[i]):\n",
    "        p_i.append(indexes[i])\n",
    "        p_v.append(values[i])\n",
    "   # elif('Gm04' in indexes[i]):\n",
    "   #     br_i.append(indexes[i])\n",
    "   #     br_v.append(values[i])\n",
    "   # elif('Gm13' in indexes[i]):\n",
    "   #     pu_i.append(indexes[i])\n",
    "   #     pu_v.append(values[i])\n",
    "    else:\n",
    "        bl_i.append(indexes[i])\n",
    "        bl_v.append(values[i])\n",
    "    i = i + 1\n",
    "#plot each of the arrays with appropriate colour and label graph\n",
    "plt.barh(bl_i, bl_v, color=\"black\")\n",
    "plt.barh(br_i, br_v, color=\"brown\")\n",
    "plt.barh(pu_i, pu_v, color=\"purple\")\n",
    "plt.barh(y_i, y_v, color=\"yellow\")\n",
    "plt.barh(p_i, p_v, color=\"orange\")\n",
    "plt.barh(g_i, g_v, color=\"green\")\n",
    "plt.barh(r_i, r_v, color=\"red\")\n",
    "plt.barh(b_i, b_v, color=\"blue\")\n",
    "plt.title('SNP Importance XGBoost Pod Colour')\n",
    "plt.ylabel('SNP Label')\n",
    "plt.xlabel('Relative F_Score (GAIN)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "(617,)\n",
      "(617, 1)\n",
      "220000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "(155,)\n",
      "(155, 1)\n",
      "220000\n",
      "(617, 214899)\n",
      "(155, 214899)\n",
      "(617, 1)\n",
      "0.0\n",
      "(617, 1)\n",
      "(155, 1)\n"
     ]
    }
   ],
   "source": [
    "tt_vcf, ho_vcf, tt_pheno, ho_pheno = new_prep_data(\"PoC_Merged_filtered.csv_train_test.csv_5pcnt.csv\", \"PoC_Merged_filtered.csv_holdout.csv_5pcnt.csv\")\n",
    "r_t = tt_pheno.ravel()\n",
    "r_h = ho_pheno.ravel()\n",
    "print(r_t[10])\n",
    "i = 0\n",
    "for x in r_t:\n",
    "    if(x==0.5):\n",
    "        r_t[i]=2.0\n",
    "    i = i+1\n",
    "i = 0\n",
    "for x in r_h:\n",
    "    if(x==0.5):\n",
    "        r_h[i]=2.0\n",
    "    i = i+1\n",
    "r_t = np.reshape(r_t,(len(r_t),1))\n",
    "r_h = np.reshape(r_h,(len(r_h),1))\n",
    "tt_pheno = r_t\n",
    "ho_pheno = r_h\n",
    "print(tt_pheno.shape)\n",
    "print(ho_pheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if need or have new holdout data etc.\n",
    "ohe = pickle.load(open(\"PoC_ohe.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 214899)\n",
      "(617, 623207)\n",
      "(155, 214899)\n",
      "(155, 623207)\n"
     ]
    }
   ],
   "source": [
    "print(tt_vcf.shape)\n",
    "tt_vcf = ohe.transform(tt_vcf)\n",
    "print(tt_vcf.shape)\n",
    "print(ho_vcf.shape)\n",
    "ho_vcf = ohe.transform(ho_vcf)\n",
    "print(ho_vcf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 623207)\n",
      "(617, 1)\n",
      "(155, 623207)\n",
      "(155, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of this model is79.36507936507937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of this model is79.36507936507937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of this model is90.47619047619048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of this model is80.95238095238095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of this model is75.80645161290323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of this model is73.77049180327869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of this model is80.32786885245902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of this model is63.934426229508205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of this model is65.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables for auroc curve done. Processing fold accuracy + checking best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of this model is63.33333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.12903225806451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Testing Accuracy: 75.23% (8.38%)\n",
      "Holdout Accuracy: 76.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(tt_vcf.shape)\n",
    "print(tt_pheno.shape)\n",
    "print(ho_vcf.shape)\n",
    "print(ho_pheno.shape)\n",
    "seed = randint(0,5000)\n",
    " #if optimised in same session, other enter manually below\n",
    "#this function should average out 10 folds and training, with inital params optimised\n",
    "#average accuracy and std should be calculated along with a nice AUROC graph of train/test models\n",
    "#best model should be extracted for use on holdout set\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=seed, max_features = 'sqrt',n_jobs=1, verbose = 1)\n",
    "best_model = eval_k_fold(model, tt_vcf, tt_pheno, 10, ho_vcf, ho_pheno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (based off primer paper and Philipp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "(617,)\n",
      "(617, 1)\n",
      "220000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "(155,)\n",
      "(155, 1)\n",
      "220000\n",
      "(617, 214899)\n",
      "(155, 214899)\n",
      "(617, 1)\n",
      "0.0\n",
      "(617, 1)\n",
      "(155, 1)\n"
     ]
    }
   ],
   "source": [
    "tt_vcf, ho_vcf, tt_pheno, ho_pheno = new_prep_data(\"PoC_Merged_filtered.csv_train_test.csv_5pcnt.csv\", \"PoC_Merged_filtered.csv_holdout.csv_5pcnt.csv\")\n",
    "r_t = tt_pheno.ravel()\n",
    "r_h = ho_pheno.ravel()\n",
    "print(r_t[10])\n",
    "i = 0\n",
    "for x in r_t:\n",
    "    if(x==0.5):\n",
    "        r_t[i]=2.0\n",
    "    i = i+1\n",
    "i = 0\n",
    "for x in r_h:\n",
    "    if(x==0.5):\n",
    "        r_h[i]=2.0\n",
    "    i = i+1\n",
    "r_t = np.reshape(r_t,(len(r_t),1))\n",
    "r_h = np.reshape(r_h,(len(r_h),1))\n",
    "tt_pheno = r_t\n",
    "ho_pheno = r_h\n",
    "print(tt_pheno.shape)\n",
    "print(ho_pheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 623207)\n",
      "(155, 214899)\n",
      "(155, 623207)\n"
     ]
    }
   ],
   "source": [
    "ohe = pickle.load(open(\"PoC_ohe.dat\", \"rb\"))\n",
    "tt_vcf = ohe.transform(tt_vcf)\n",
    "print(tt_vcf.shape)\n",
    "print(ho_vcf.shape)\n",
    "ho_vcf = ohe.transform(ho_vcf)\n",
    "print(ho_vcf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##how to mlb both tt and ho for same scheme? do i even need to?\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb = mlb.fit(tt_pheno)\n",
    "##print(tt_pheno.shape)\n",
    "#print(ho_pheno.shape)\n",
    "#tt_pheno = mlb.transform(tt_pheno)\n",
    "#print(tt_pheno.shape)\n",
    "#ho_pheno = mlb.transform(ho_pheno)\n",
    "#print(ho_pheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN_model(x_len):    \n",
    "    #del model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=10, kernel_size=10, \n",
    "                     input_shape=(x_len, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(filters=8, kernel_size=8, \n",
    "                     input_shape=(10, 1)))\n",
    "    model.add(Activation('linear'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv1D(filters=6, kernel_size=6, \n",
    "                     input_shape=(8, 1)))\n",
    "    model.add(Activation('linear'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(24, activation='linear'))\n",
    "    model.add(Dense(16, activation='linear'))\n",
    "    model.add(Dense(8, activation='linear'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    opt = tf.keras.optimizers.Adamax(learning_rate=0.003)#, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cnn(x,y,k,mlb):\n",
    "    cv = StratifiedKFold(n_splits=k,shuffle=False)\n",
    "    best_model = []\n",
    "    results = []\n",
    "    highest = 0\n",
    "    i = 1\n",
    "    for train,test in cv.split(x,y):\n",
    "        print(y.shape)\n",
    "        print(y[train])\n",
    "        if(i==1):\n",
    "            y = mlb.transform(y)\n",
    "            print(y.shape)\n",
    "            print(y[train])\n",
    "        x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "        model = build_CNN_model(x[train].shape[1])\n",
    "        bs = ((x[train].shape[0])/20)\n",
    "        bs = round(bs)\n",
    "        history = model.fit(x[train], y[train], validation_data=(x[test], y[test]), epochs=100, batch_size=bs)\n",
    "        _, accuracy = model.evaluate(x[test], y[test], batch_size=bs, verbose=0)\n",
    "        accuracy = accuracy *100\n",
    "        print(\"accuracy for model \" + str(i) + \" is \" + str(accuracy))\n",
    "        if(accuracy > highest):\n",
    "            highest = accuracy\n",
    "            best_model = model\n",
    "        results.append(accuracy)\n",
    "        del model\n",
    "        i = i + 1\n",
    "    print(\"Training Testing Accuracy: %.2f%% (%.2f%%)\" % (np.mean(results), np.std(results))) \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 1)\n",
      "[[1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]]\n",
      "(617, 3)\n",
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 623198, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 623191, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 623186, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 623186, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 311593, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 311593, 6)         24        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1869558)           0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                44869416  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 44,871,087\n",
      "Trainable params: 44,871,059\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 53s 96ms/sample - loss: 1.1197 - accuracy: 0.4819 - val_loss: 0.8825 - val_accuracy: 0.5079\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 6s 11ms/sample - loss: 0.8531 - accuracy: 0.6065 - val_loss: 1.0936 - val_accuracy: 0.4286\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 6s 10ms/sample - loss: 0.8069 - accuracy: 0.6264 - val_loss: 1.2801 - val_accuracy: 0.1746\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 6s 10ms/sample - loss: 0.8001 - accuracy: 0.6625 - val_loss: 1.3984 - val_accuracy: 0.1746\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 6s 10ms/sample - loss: 0.7513 - accuracy: 0.6661 - val_loss: 1.4864 - val_accuracy: 0.1746\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 6s 10ms/sample - loss: 0.7336 - accuracy: 0.6931 - val_loss: 1.5937 - val_accuracy: 0.1746\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.6857 - accuracy: 0.7148 - val_loss: 1.8620 - val_accuracy: 0.1746\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.5583 - accuracy: 0.7816 - val_loss: 1.9916 - val_accuracy: 0.1746\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.3744 - accuracy: 0.9097 - val_loss: 2.0246 - val_accuracy: 0.1746\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.2223 - accuracy: 0.9783 - val_loss: 1.8479 - val_accuracy: 0.1746\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.1509 - accuracy: 0.9892 - val_loss: 1.7496 - val_accuracy: 0.1746\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.1056 - accuracy: 0.9982 - val_loss: 1.6000 - val_accuracy: 0.1746\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0844 - accuracy: 1.0000 - val_loss: 1.4603 - val_accuracy: 0.1746\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0644 - accuracy: 1.0000 - val_loss: 1.3020 - val_accuracy: 0.1746\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0527 - accuracy: 1.0000 - val_loss: 1.2285 - val_accuracy: 0.1746\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0479 - accuracy: 1.0000 - val_loss: 1.0517 - val_accuracy: 0.2381\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0445 - accuracy: 1.0000 - val_loss: 0.9307 - val_accuracy: 0.5397\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0427 - accuracy: 1.0000 - val_loss: 0.8576 - val_accuracy: 0.6825\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.7418 - val_accuracy: 0.8413\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0343 - accuracy: 1.0000 - val_loss: 0.6518 - val_accuracy: 0.8571\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.6162 - val_accuracy: 0.8730\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.5534 - val_accuracy: 0.8571\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.5403 - val_accuracy: 0.8413\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.5161 - val_accuracy: 0.8254\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.5103 - val_accuracy: 0.8254\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.5092 - val_accuracy: 0.8254\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0252 - accuracy: 0.9982 - val_loss: 0.5345 - val_accuracy: 0.7778\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0252 - accuracy: 0.9982 - val_loss: 0.5023 - val_accuracy: 0.8254\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.5089 - val_accuracy: 0.7937\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.5051 - val_accuracy: 0.8095\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.5302 - val_accuracy: 0.8254\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.5155 - val_accuracy: 0.8254\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.5537 - val_accuracy: 0.8254\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.4989 - val_accuracy: 0.8254\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.4872 - val_accuracy: 0.8095\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.5568 - val_accuracy: 0.8095\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.5578 - val_accuracy: 0.8254\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.5487 - val_accuracy: 0.8254\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.5886 - val_accuracy: 0.7937\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.5419 - val_accuracy: 0.8254\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.5717 - val_accuracy: 0.8254\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.5513 - val_accuracy: 0.8095\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.5944 - val_accuracy: 0.8254\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.5509 - val_accuracy: 0.8254\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.5467 - val_accuracy: 0.8254\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.5835 - val_accuracy: 0.8254\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.5561 - val_accuracy: 0.8254\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.6041 - val_accuracy: 0.8254\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.6012 - val_accuracy: 0.8254\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.6175 - val_accuracy: 0.8254\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.5541 - val_accuracy: 0.8254\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.8775 - val_accuracy: 0.7619\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0131 - accuracy: 0.9982 - val_loss: 0.7725 - val_accuracy: 0.8254\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.6806 - val_accuracy: 0.7619\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.8118 - val_accuracy: 0.8571\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.5818 - val_accuracy: 0.8254\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5986 - val_accuracy: 0.8254\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.5969 - val_accuracy: 0.8254\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.6238 - val_accuracy: 0.8254\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.5754 - val_accuracy: 0.8254\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.7085 - val_accuracy: 0.8254\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.6424 - val_accuracy: 0.8095\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.6893 - val_accuracy: 0.8254\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.6439 - val_accuracy: 0.8254\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.6431 - val_accuracy: 0.8254\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.6298 - val_accuracy: 0.8254\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.6524 - val_accuracy: 0.8254\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.6509 - val_accuracy: 0.8254\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 0.8254\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.6273 - val_accuracy: 0.8254\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.6314 - val_accuracy: 0.8254\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.6316 - val_accuracy: 0.8254\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 0.8254\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 0.8254\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6589 - val_accuracy: 0.8254\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.6815 - val_accuracy: 0.8413\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5646 - val_accuracy: 0.8254\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.6423 - val_accuracy: 0.8254\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 0.8254\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6429 - val_accuracy: 0.8254\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.6287 - val_accuracy: 0.8254\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6700 - val_accuracy: 0.8254\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.6471 - val_accuracy: 0.8254\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.7006 - val_accuracy: 0.8254\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7199 - val_accuracy: 0.8254\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6709 - val_accuracy: 0.8254\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6564 - val_accuracy: 0.8254\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 0.8254\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6477 - val_accuracy: 0.8254\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6992 - val_accuracy: 0.8254\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6504 - val_accuracy: 0.8254\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 0.8254\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 0.7937\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.9567 - val_accuracy: 0.8413\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.7502 - val_accuracy: 0.7937\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7437 - val_accuracy: 0.8095\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8014 - val_accuracy: 0.8254\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.7143 - val_accuracy: 0.8254\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.7619 - val_accuracy: 0.7937\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.9036 - val_accuracy: 0.7937\n",
      "accuracy for model 1 is 79.36508059501648\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 623198, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 623191, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 623186, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 623186, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 311593, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 311593, 6)         24        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1869558)           0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 24)                44869416  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 44,871,087\n",
      "Trainable params: 44,871,059\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 17s 31ms/sample - loss: 1.0453 - accuracy: 0.5740 - val_loss: 0.7038 - val_accuracy: 0.7302\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.7533 - accuracy: 0.7094 - val_loss: 0.8775 - val_accuracy: 0.7143\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.7412 - accuracy: 0.7148 - val_loss: 1.0669 - val_accuracy: 0.2222\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.7046 - accuracy: 0.7274 - val_loss: 1.2230 - val_accuracy: 0.1746\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.6915 - accuracy: 0.7148 - val_loss: 1.3592 - val_accuracy: 0.1746\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.6701 - accuracy: 0.7437 - val_loss: 1.4774 - val_accuracy: 0.1746\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.6034 - accuracy: 0.7671 - val_loss: 1.6428 - val_accuracy: 0.1746\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.4966 - accuracy: 0.8321 - val_loss: 1.9597 - val_accuracy: 0.1746\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.3341 - accuracy: 0.9097 - val_loss: 2.0457 - val_accuracy: 0.1746\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.2039 - accuracy: 0.9603 - val_loss: 1.9029 - val_accuracy: 0.1746\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.1300 - accuracy: 0.9838 - val_loss: 1.9502 - val_accuracy: 0.1746\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0992 - accuracy: 0.9964 - val_loss: 1.7219 - val_accuracy: 0.1746\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0703 - accuracy: 1.0000 - val_loss: 1.5752 - val_accuracy: 0.1746\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0497 - accuracy: 1.0000 - val_loss: 1.4556 - val_accuracy: 0.1746\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0451 - accuracy: 1.0000 - val_loss: 1.2017 - val_accuracy: 0.1746\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0344 - accuracy: 1.0000 - val_loss: 1.0501 - val_accuracy: 0.3016\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0341 - accuracy: 1.0000 - val_loss: 1.0154 - val_accuracy: 0.4127\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.9281 - val_accuracy: 0.5079\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.8120 - val_accuracy: 0.6508\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.7194 - val_accuracy: 0.7302\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.8667 - val_accuracy: 0.6190\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.8495 - val_accuracy: 0.6667\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.6493 - val_accuracy: 0.7619\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.5940 - val_accuracy: 0.7778\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.6426 - val_accuracy: 0.7778\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.5789 - val_accuracy: 0.7937\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.6123 - val_accuracy: 0.7937\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 0.7937\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.6426 - val_accuracy: 0.7619\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.6249 - val_accuracy: 0.7937\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.6343 - val_accuracy: 0.7937\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.6633 - val_accuracy: 0.7619\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.6520 - val_accuracy: 0.7778\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.6492 - val_accuracy: 0.8095\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.6398 - val_accuracy: 0.7937\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.6474 - val_accuracy: 0.7937\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.6425 - val_accuracy: 0.7937\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.6737 - val_accuracy: 0.7937\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.6725 - val_accuracy: 0.7937\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.6734 - val_accuracy: 0.7937\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6707 - val_accuracy: 0.7937\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.6858 - val_accuracy: 0.7937\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.6793 - val_accuracy: 0.7937\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.6864 - val_accuracy: 0.7937\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.7038 - val_accuracy: 0.7937\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.7696 - val_accuracy: 0.7619\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.6955 - val_accuracy: 0.8095\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.7104 - val_accuracy: 0.7937\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.7328 - val_accuracy: 0.7937\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.6850 - val_accuracy: 0.7937\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.7176 - val_accuracy: 0.7778\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.7117 - val_accuracy: 0.7937\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.7257 - val_accuracy: 0.7937\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.7164 - val_accuracy: 0.7937\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.7141 - val_accuracy: 0.7778\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.7477 - val_accuracy: 0.7937\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.7157 - val_accuracy: 0.7937\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.7200 - val_accuracy: 0.7937\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.7394 - val_accuracy: 0.7937\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.7452 - val_accuracy: 0.7937\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.7470 - val_accuracy: 0.7937\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7650 - val_accuracy: 0.7937\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.7405 - val_accuracy: 0.7937\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.7671 - val_accuracy: 0.7937\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7454 - val_accuracy: 0.7937\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.7860 - val_accuracy: 0.7937\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7670 - val_accuracy: 0.7937\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7729 - val_accuracy: 0.7937\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.8265 - val_accuracy: 0.7937\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.8901 - val_accuracy: 0.8095\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.7833 - val_accuracy: 0.7937\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.8209 - val_accuracy: 0.7937\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.7933 - val_accuracy: 0.7937\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.8578 - val_accuracy: 0.7937\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.7767 - val_accuracy: 0.7937\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.8046 - val_accuracy: 0.7778\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.7952 - val_accuracy: 0.7937\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.7652 - val_accuracy: 0.7937\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.8249 - val_accuracy: 0.7937\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7505 - val_accuracy: 0.7778\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.7878 - val_accuracy: 0.7937\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.9209 - val_accuracy: 0.7778\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.8100 - val_accuracy: 0.7778\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.8028 - val_accuracy: 0.7778\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.8508 - val_accuracy: 0.7937\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.3473 - val_accuracy: 0.7619\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.8460 - val_accuracy: 0.7619\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.8542 - val_accuracy: 0.7937\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8935 - val_accuracy: 0.7937\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9236 - val_accuracy: 0.7937\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8648 - val_accuracy: 0.7937\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.9021 - val_accuracy: 0.7778\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.8434 - val_accuracy: 0.7937\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.8875 - val_accuracy: 0.7778\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0132 - accuracy: 0.9964 - val_loss: 1.1911 - val_accuracy: 0.7302\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0133 - accuracy: 0.9982 - val_loss: 1.6381 - val_accuracy: 0.7619\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0357 - accuracy: 0.9910 - val_loss: 1.9754 - val_accuracy: 0.7302\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0709 - accuracy: 0.9747 - val_loss: 1.0620 - val_accuracy: 0.8095\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0948 - accuracy: 0.9729 - val_loss: 1.0130 - val_accuracy: 0.7937\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0514 - accuracy: 0.9874 - val_loss: 1.1119 - val_accuracy: 0.7937\n",
      "accuracy for model 2 is 79.36508059501648\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 623198, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 623191, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 623186, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 623186, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 311593, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 311593, 6)         24        \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1869558)           0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 24)                44869416  \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 44,871,087\n",
      "Trainable params: 44,871,059\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 18s 32ms/sample - loss: 0.9249 - accuracy: 0.5776 - val_loss: 0.5900 - val_accuracy: 0.7619\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.8010 - accuracy: 0.6534 - val_loss: 0.8068 - val_accuracy: 0.6825\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.7637 - accuracy: 0.6805 - val_loss: 0.9904 - val_accuracy: 0.3175\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.7958 - accuracy: 0.6751 - val_loss: 1.1443 - val_accuracy: 0.1746\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.7335 - accuracy: 0.6931 - val_loss: 1.2148 - val_accuracy: 0.1746\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.7185 - accuracy: 0.6986 - val_loss: 1.2569 - val_accuracy: 0.1746\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.7304 - accuracy: 0.7022 - val_loss: 1.3047 - val_accuracy: 0.1746\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.6951 - accuracy: 0.7130 - val_loss: 1.3413 - val_accuracy: 0.1746\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.6590 - accuracy: 0.7202 - val_loss: 1.4397 - val_accuracy: 0.1746\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.6056 - accuracy: 0.7563 - val_loss: 1.8516 - val_accuracy: 0.1746\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.4871 - accuracy: 0.8448 - val_loss: 2.3210 - val_accuracy: 0.1746\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.3243 - accuracy: 0.9097 - val_loss: 2.2462 - val_accuracy: 0.1746\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.1611 - accuracy: 0.9783 - val_loss: 2.2109 - val_accuracy: 0.1746\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0972 - accuracy: 0.9946 - val_loss: 1.9583 - val_accuracy: 0.1746\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0753 - accuracy: 0.9928 - val_loss: 1.8072 - val_accuracy: 0.1746\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0540 - accuracy: 0.9982 - val_loss: 1.5770 - val_accuracy: 0.1746\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0505 - accuracy: 1.0000 - val_loss: 1.4367 - val_accuracy: 0.1746\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0415 - accuracy: 0.9982 - val_loss: 1.2226 - val_accuracy: 0.1746\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0346 - accuracy: 1.0000 - val_loss: 1.0404 - val_accuracy: 0.2063\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.8950 - val_accuracy: 0.4127\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.7665 - val_accuracy: 0.6190\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0225 - accuracy: 1.0000 - val_loss: 0.5563 - val_accuracy: 0.8730\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 0.8095\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.5106 - val_accuracy: 0.9048\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.5368 - val_accuracy: 0.8571\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.4770 - val_accuracy: 0.8889\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.4283 - val_accuracy: 0.8889\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.4407 - val_accuracy: 0.8889\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.3957 - val_accuracy: 0.9048\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.4078 - val_accuracy: 0.8889\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3765 - val_accuracy: 0.8889\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.3995 - val_accuracy: 0.8889\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.3937 - val_accuracy: 0.9048\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.3711 - val_accuracy: 0.8889\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.3741 - val_accuracy: 0.8889\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.3908 - val_accuracy: 0.8730\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.3820 - val_accuracy: 0.8730\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.3997 - val_accuracy: 0.8730\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.3944 - val_accuracy: 0.8889\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.3797 - val_accuracy: 0.8730\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.4600 - val_accuracy: 0.8413\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.4529 - val_accuracy: 0.8571\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.4240 - val_accuracy: 0.8730\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.4205 - val_accuracy: 0.8571\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.4161 - val_accuracy: 0.8730\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.4672 - val_accuracy: 0.8413\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.4641 - val_accuracy: 0.8571\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.4688 - val_accuracy: 0.8254\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.4540 - val_accuracy: 0.8571\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.4870 - val_accuracy: 0.8413\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4782 - val_accuracy: 0.8889\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.4099 - val_accuracy: 0.9048\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.4593 - val_accuracy: 0.8889\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0076 - accuracy: 0.9982 - val_loss: 0.4747 - val_accuracy: 0.8889\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.5162 - val_accuracy: 0.8730\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.4885 - val_accuracy: 0.8730\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.6396 - val_accuracy: 0.8095\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.5435 - val_accuracy: 0.8254\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.5002 - val_accuracy: 0.8889\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.4720 - val_accuracy: 0.8730\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0110 - accuracy: 0.9982 - val_loss: 0.7380 - val_accuracy: 0.7619\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.5357 - val_accuracy: 0.8254\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4954 - val_accuracy: 0.8889\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.5681 - val_accuracy: 0.8254\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4916 - val_accuracy: 0.8413\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.5062 - val_accuracy: 0.8413\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5137 - val_accuracy: 0.8571\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.5036 - val_accuracy: 0.8413\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.4749 - val_accuracy: 0.8571\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5053 - val_accuracy: 0.8413\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5348 - val_accuracy: 0.8254\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5194 - val_accuracy: 0.8571\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5436 - val_accuracy: 0.8254\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4879 - val_accuracy: 0.8413\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4912 - val_accuracy: 0.8889\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4843 - val_accuracy: 0.8571\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5018 - val_accuracy: 0.8571\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4913 - val_accuracy: 0.8571\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.5015 - val_accuracy: 0.8571\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5142 - val_accuracy: 0.8413\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4998 - val_accuracy: 0.8730\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5238 - val_accuracy: 0.8413\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4967 - val_accuracy: 0.8571\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4907 - val_accuracy: 0.8889\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5114 - val_accuracy: 0.8571\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5015 - val_accuracy: 0.8571\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5051 - val_accuracy: 0.8571\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4917 - val_accuracy: 0.8571\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 9.8920e-04 - accuracy: 1.0000 - val_loss: 0.4835 - val_accuracy: 0.8571\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4978 - val_accuracy: 0.8571\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5088 - val_accuracy: 0.8413\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6520 - val_accuracy: 0.8095\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5227 - val_accuracy: 0.8730\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4911 - val_accuracy: 0.8730\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4977 - val_accuracy: 0.8571\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5269 - val_accuracy: 0.8413\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4796 - val_accuracy: 0.8730\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4869 - val_accuracy: 0.8730\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4918 - val_accuracy: 0.8889\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 8.9859e-04 - accuracy: 1.0000 - val_loss: 0.5011 - val_accuracy: 0.8889\n",
      "accuracy for model 3 is 88.88888955116272\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 623198, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 623191, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 623186, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 623186, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 311593, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 311593, 6)         24        \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1869558)           0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 24)                44869416  \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 44,871,087\n",
      "Trainable params: 44,871,059\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 35s 64ms/sample - loss: 1.0445 - accuracy: 0.5686 - val_loss: 0.6759 - val_accuracy: 0.7302\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.7499 - accuracy: 0.6697 - val_loss: 0.9286 - val_accuracy: 0.6032\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.7364 - accuracy: 0.6986 - val_loss: 1.2138 - val_accuracy: 0.2540\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.6913 - accuracy: 0.7076 - val_loss: 1.4230 - val_accuracy: 0.2063\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.6755 - accuracy: 0.7148 - val_loss: 1.5814 - val_accuracy: 0.2063\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.6768 - accuracy: 0.7220 - val_loss: 1.8328 - val_accuracy: 0.1746\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.5725 - accuracy: 0.7798 - val_loss: 2.2609 - val_accuracy: 0.1746\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.4450 - accuracy: 0.8466 - val_loss: 2.3891 - val_accuracy: 0.1746\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.3080 - accuracy: 0.9170 - val_loss: 2.1611 - val_accuracy: 0.1746\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.1830 - accuracy: 0.9729 - val_loss: 2.2008 - val_accuracy: 0.1746\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.1139 - accuracy: 0.9964 - val_loss: 1.8071 - val_accuracy: 0.1746\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0854 - accuracy: 0.9964 - val_loss: 1.6724 - val_accuracy: 0.1746\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0649 - accuracy: 1.0000 - val_loss: 1.6269 - val_accuracy: 0.1746\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0576 - accuracy: 0.9964 - val_loss: 1.3372 - val_accuracy: 0.2540\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0483 - accuracy: 1.0000 - val_loss: 1.0614 - val_accuracy: 0.3810\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0395 - accuracy: 1.0000 - val_loss: 1.0857 - val_accuracy: 0.3968\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0388 - accuracy: 1.0000 - val_loss: 0.9297 - val_accuracy: 0.4921\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0430 - accuracy: 1.0000 - val_loss: 0.8110 - val_accuracy: 0.6667\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0376 - accuracy: 0.9982 - val_loss: 0.7447 - val_accuracy: 0.6667\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.7504 - val_accuracy: 0.6349\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.5997 - val_accuracy: 0.7302\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.5495 - val_accuracy: 0.7619\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.5820 - val_accuracy: 0.7460\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.4791 - val_accuracy: 0.8095\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.5433 - val_accuracy: 0.7460\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.4723 - val_accuracy: 0.7778\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.5044 - val_accuracy: 0.7460\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.5374 - val_accuracy: 0.7937\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.5269 - val_accuracy: 0.7619\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.5055 - val_accuracy: 0.7778\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.5342 - val_accuracy: 0.7619\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.5234 - val_accuracy: 0.7778\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.5230 - val_accuracy: 0.7778\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.5028 - val_accuracy: 0.7619\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.5588 - val_accuracy: 0.8095\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.4931 - val_accuracy: 0.7778\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.5184 - val_accuracy: 0.7460\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.5470 - val_accuracy: 0.7937\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.5153 - val_accuracy: 0.7619\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.5231 - val_accuracy: 0.7937\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.5141 - val_accuracy: 0.7937\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.5358 - val_accuracy: 0.7619\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.5423 - val_accuracy: 0.7619\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.5249 - val_accuracy: 0.7778\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.5096 - val_accuracy: 0.8095\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5634 - val_accuracy: 0.7619\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5840 - val_accuracy: 0.7937\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.5613 - val_accuracy: 0.7619\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.5545 - val_accuracy: 0.7619\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.5866 - val_accuracy: 0.7937\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5430 - val_accuracy: 0.7778\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.5575 - val_accuracy: 0.7778\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.5232 - val_accuracy: 0.7619\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.5860 - val_accuracy: 0.7302\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.6238 - val_accuracy: 0.7619\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5810 - val_accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.5512 - val_accuracy: 0.7937\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 0.7937\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.7056 - val_accuracy: 0.8254\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.5953 - val_accuracy: 0.7937\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.5785 - val_accuracy: 0.7937\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5359 - val_accuracy: 0.8095\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5919 - val_accuracy: 0.7937\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.5250 - val_accuracy: 0.8095\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5986 - val_accuracy: 0.7937\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.5681 - val_accuracy: 0.8095\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.5534 - val_accuracy: 0.7778\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.5697 - val_accuracy: 0.8095\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5726 - val_accuracy: 0.8095\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.5661 - val_accuracy: 0.7619\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5712 - val_accuracy: 0.7619\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.6639 - val_accuracy: 0.8095\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.6410 - val_accuracy: 0.7937\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.6542 - val_accuracy: 0.8095\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5874 - val_accuracy: 0.7778\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.6936 - val_accuracy: 0.8095\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.5584 - val_accuracy: 0.7937\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.6908 - val_accuracy: 0.7778\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.6185 - val_accuracy: 0.7778\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.5826 - val_accuracy: 0.7937\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.5782 - val_accuracy: 0.7937\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5551 - val_accuracy: 0.7778\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6695 - val_accuracy: 0.7937\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6204 - val_accuracy: 0.7937\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6063 - val_accuracy: 0.7778\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 5s 10ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6273 - val_accuracy: 0.7937\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.5723 - val_accuracy: 0.7937\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6379 - val_accuracy: 0.7937\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.8127 - val_accuracy: 0.5397\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0130 - accuracy: 0.9982 - val_loss: 1.0117 - val_accuracy: 0.8254\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0149 - accuracy: 0.9982 - val_loss: 0.6495 - val_accuracy: 0.7937\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0149 - accuracy: 0.9964 - val_loss: 0.6685 - val_accuracy: 0.7619\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0179 - accuracy: 0.9982 - val_loss: 0.9200 - val_accuracy: 0.8254\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0533 - accuracy: 0.9892 - val_loss: 0.7622 - val_accuracy: 0.7937\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.1118 - accuracy: 0.9675 - val_loss: 2.0025 - val_accuracy: 0.7460\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.1185 - accuracy: 0.9603 - val_loss: 0.6539 - val_accuracy: 0.7937\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0397 - accuracy: 0.9928 - val_loss: 0.9598 - val_accuracy: 0.8095\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0199 - accuracy: 0.9946 - val_loss: 0.7222 - val_accuracy: 0.8254\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0195 - accuracy: 0.9946 - val_loss: 0.5771 - val_accuracy: 0.7937\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 5s 9ms/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.5718 - val_accuracy: 0.7937\n",
      "accuracy for model 4 is 79.36508059501648\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 623198, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 623191, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 623186, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 623186, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 311593, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 311593, 6)         24        \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1869558)           0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 24)                44869416  \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 44,871,087\n",
      "Trainable params: 44,871,059\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 555 samples, validate on 62 samples\n",
      "Epoch 1/100\n",
      "555/555 [==============================] - 31s 56ms/sample - loss: 0.9560 - accuracy: 0.6126 - val_loss: 0.7096 - val_accuracy: 0.7097\n",
      "Epoch 2/100\n",
      "555/555 [==============================] - 6s 10ms/sample - loss: 0.7951 - accuracy: 0.6595 - val_loss: 0.8844 - val_accuracy: 0.6290\n",
      "Epoch 3/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.7175 - accuracy: 0.6919 - val_loss: 1.0045 - val_accuracy: 0.4032\n",
      "Epoch 4/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.6916 - accuracy: 0.7171 - val_loss: 1.1431 - val_accuracy: 0.2419\n",
      "Epoch 5/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.6473 - accuracy: 0.7423 - val_loss: 1.3441 - val_accuracy: 0.1774\n",
      "Epoch 6/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.5571 - accuracy: 0.7892 - val_loss: 1.6839 - val_accuracy: 0.1774\n",
      "Epoch 7/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.4154 - accuracy: 0.8829 - val_loss: 1.6699 - val_accuracy: 0.1774\n",
      "Epoch 8/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.2477 - accuracy: 0.9550 - val_loss: 1.5916 - val_accuracy: 0.1774\n",
      "Epoch 9/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.1771 - accuracy: 0.9640 - val_loss: 1.4967 - val_accuracy: 0.1774\n",
      "Epoch 10/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.1013 - accuracy: 1.0000 - val_loss: 1.3515 - val_accuracy: 0.1774\n",
      "Epoch 11/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0738 - accuracy: 0.9964 - val_loss: 1.3386 - val_accuracy: 0.1774\n",
      "Epoch 12/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0610 - accuracy: 0.9982 - val_loss: 1.2080 - val_accuracy: 0.1935\n",
      "Epoch 13/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0478 - accuracy: 0.9964 - val_loss: 1.1773 - val_accuracy: 0.2258\n",
      "Epoch 14/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0443 - accuracy: 1.0000 - val_loss: 0.9652 - val_accuracy: 0.4355\n",
      "Epoch 15/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0411 - accuracy: 0.9982 - val_loss: 0.8522 - val_accuracy: 0.5645\n",
      "Epoch 16/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0298 - accuracy: 1.0000 - val_loss: 0.7643 - val_accuracy: 0.6774\n",
      "Epoch 17/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0385 - accuracy: 1.0000 - val_loss: 0.8308 - val_accuracy: 0.5806\n",
      "Epoch 18/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.8193 - val_accuracy: 0.6290\n",
      "Epoch 19/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.7293 - val_accuracy: 0.6935\n",
      "Epoch 20/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.6755 - val_accuracy: 0.7419\n",
      "Epoch 21/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.6862 - val_accuracy: 0.7258\n",
      "Epoch 22/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.7061 - val_accuracy: 0.7581\n",
      "Epoch 23/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.6473 - val_accuracy: 0.7903\n",
      "Epoch 24/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.6613 - val_accuracy: 0.8065\n",
      "Epoch 25/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.6543 - val_accuracy: 0.7903\n",
      "Epoch 26/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.6575 - val_accuracy: 0.7742\n",
      "Epoch 27/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.6446 - val_accuracy: 0.8065\n",
      "Epoch 28/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.6395 - val_accuracy: 0.8387\n",
      "Epoch 29/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.6984 - val_accuracy: 0.8065\n",
      "Epoch 30/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.6602 - val_accuracy: 0.8065\n",
      "Epoch 31/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.6902 - val_accuracy: 0.8226\n",
      "Epoch 32/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.6743 - val_accuracy: 0.8065\n",
      "Epoch 33/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.7036 - val_accuracy: 0.7903\n",
      "Epoch 34/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.7142 - val_accuracy: 0.7581\n",
      "Epoch 35/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.7118 - val_accuracy: 0.8226\n",
      "Epoch 36/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.7400 - val_accuracy: 0.8226\n",
      "Epoch 37/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.7398 - val_accuracy: 0.8065\n",
      "Epoch 38/100\n",
      "555/555 [==============================] - 5s 10ms/sample - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.7342 - val_accuracy: 0.7903\n",
      "Epoch 39/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.6894 - val_accuracy: 0.8065\n",
      "Epoch 40/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.9332 - val_accuracy: 0.8226\n",
      "Epoch 41/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.7520 - val_accuracy: 0.8065\n",
      "Epoch 42/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.8472 - val_accuracy: 0.7419\n",
      "Epoch 43/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.7282 - val_accuracy: 0.7903\n",
      "Epoch 44/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.7743 - val_accuracy: 0.7742\n",
      "Epoch 45/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.7810 - val_accuracy: 0.7581\n",
      "Epoch 46/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.7496 - val_accuracy: 0.7742\n",
      "Epoch 47/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.7368 - val_accuracy: 0.7903\n",
      "Epoch 48/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.7679 - val_accuracy: 0.8065\n",
      "Epoch 49/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.7766 - val_accuracy: 0.7903\n",
      "Epoch 50/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.7451 - val_accuracy: 0.7903\n",
      "Epoch 51/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.7876 - val_accuracy: 0.7903\n",
      "Epoch 52/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.8081 - val_accuracy: 0.7903\n",
      "Epoch 53/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.7373 - val_accuracy: 0.8387\n",
      "Epoch 54/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.7327 - val_accuracy: 0.7903\n",
      "Epoch 55/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.7769 - val_accuracy: 0.7903\n",
      "Epoch 56/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.7598 - val_accuracy: 0.7742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.7501 - val_accuracy: 0.8065\n",
      "Epoch 58/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.7906 - val_accuracy: 0.7903\n",
      "Epoch 59/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.8562 - val_accuracy: 0.8226\n",
      "Epoch 60/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.0217 - val_accuracy: 0.8226\n",
      "Epoch 61/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.8010 - val_accuracy: 0.8065\n",
      "Epoch 62/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.8313 - val_accuracy: 0.7742\n",
      "Epoch 63/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.8990 - val_accuracy: 0.7903\n",
      "Epoch 64/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.8446 - val_accuracy: 0.7742\n",
      "Epoch 65/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.7970 - val_accuracy: 0.7903\n",
      "Epoch 66/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.7927 - val_accuracy: 0.7903\n",
      "Epoch 67/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.8146 - val_accuracy: 0.8226\n",
      "Epoch 68/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.8354 - val_accuracy: 0.8226\n",
      "Epoch 69/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.8273 - val_accuracy: 0.8065\n",
      "Epoch 70/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.8331 - val_accuracy: 0.8065\n",
      "Epoch 71/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.8141 - val_accuracy: 0.8065\n",
      "Epoch 72/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.8465 - val_accuracy: 0.8065\n",
      "Epoch 73/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.8313 - val_accuracy: 0.8065\n",
      "Epoch 74/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.8554 - val_accuracy: 0.8065\n",
      "Epoch 75/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.8549 - val_accuracy: 0.7903\n",
      "Epoch 76/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.8148 - val_accuracy: 0.8226\n",
      "Epoch 77/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8635 - val_accuracy: 0.7903\n",
      "Epoch 78/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.9507 - val_accuracy: 0.8065\n",
      "Epoch 79/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.8432 - val_accuracy: 0.7903\n",
      "Epoch 80/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8285 - val_accuracy: 0.7903\n",
      "Epoch 81/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.8595 - val_accuracy: 0.7903\n",
      "Epoch 82/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.8412 - val_accuracy: 0.7903\n",
      "Epoch 83/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.0173 - val_accuracy: 0.7581\n",
      "Epoch 84/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0098 - accuracy: 0.9982 - val_loss: 1.3140 - val_accuracy: 0.7903\n",
      "Epoch 85/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0598 - accuracy: 0.9856 - val_loss: 2.0333 - val_accuracy: 0.7742\n",
      "Epoch 86/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0409 - accuracy: 0.9874 - val_loss: 1.1896 - val_accuracy: 0.8065\n",
      "Epoch 87/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0341 - accuracy: 0.9910 - val_loss: 1.0545 - val_accuracy: 0.7419\n",
      "Epoch 88/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0246 - accuracy: 0.9928 - val_loss: 0.9663 - val_accuracy: 0.8065\n",
      "Epoch 89/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0310 - accuracy: 0.9946 - val_loss: 0.9208 - val_accuracy: 0.8226\n",
      "Epoch 90/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0120 - accuracy: 0.9982 - val_loss: 1.1171 - val_accuracy: 0.8226\n",
      "Epoch 91/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0091 - accuracy: 0.9982 - val_loss: 0.9373 - val_accuracy: 0.8065\n",
      "Epoch 92/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0099 - accuracy: 0.9982 - val_loss: 0.7994 - val_accuracy: 0.8226\n",
      "Epoch 93/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.8715 - val_accuracy: 0.7903\n",
      "Epoch 94/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.9916 - val_accuracy: 0.7742\n",
      "Epoch 95/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.8970 - val_accuracy: 0.7903\n",
      "Epoch 96/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.8641 - val_accuracy: 0.8065\n",
      "Epoch 97/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.8674 - val_accuracy: 0.8065\n",
      "Epoch 98/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.9530 - val_accuracy: 0.8226\n",
      "Epoch 99/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.9265 - val_accuracy: 0.7903\n",
      "Epoch 100/100\n",
      "555/555 [==============================] - 5s 9ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.8905 - val_accuracy: 0.8065\n",
      "accuracy for model 5 is 80.64516186714172\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 623198, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 623191, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 623186, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 623186, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 311593, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 311593, 6)         24        \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1869558)           0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 24)                44869416  \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 44,871,087\n",
      "Trainable params: 44,871,059\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "556/556 [==============================] - 24s 44ms/sample - loss: 0.9400 - accuracy: 0.5755 - val_loss: 0.7281 - val_accuracy: 0.7213\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 6s 12ms/sample - loss: 0.8031 - accuracy: 0.6511 - val_loss: 0.9039 - val_accuracy: 0.5574\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 5s 10ms/sample - loss: 0.7485 - accuracy: 0.6673 - val_loss: 1.0172 - val_accuracy: 0.2951\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 5s 10ms/sample - loss: 0.6787 - accuracy: 0.7158 - val_loss: 1.1625 - val_accuracy: 0.1803\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.5573 - accuracy: 0.7950 - val_loss: 1.2719 - val_accuracy: 0.1803\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.3876 - accuracy: 0.8885 - val_loss: 1.3266 - val_accuracy: 0.1803\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.2306 - accuracy: 0.9766 - val_loss: 1.2727 - val_accuracy: 0.1803\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.1448 - accuracy: 0.9928 - val_loss: 1.1893 - val_accuracy: 0.1803\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.1080 - accuracy: 0.9946 - val_loss: 1.1603 - val_accuracy: 0.2131\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0920 - accuracy: 0.9964 - val_loss: 1.1163 - val_accuracy: 0.2623\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0721 - accuracy: 0.9982 - val_loss: 1.0702 - val_accuracy: 0.3279\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0532 - accuracy: 1.0000 - val_loss: 0.9528 - val_accuracy: 0.4098\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0485 - accuracy: 1.0000 - val_loss: 0.9806 - val_accuracy: 0.4426\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.9004 - val_accuracy: 0.5902\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.9020 - val_accuracy: 0.5574\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.9211 - val_accuracy: 0.5574\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.8844 - val_accuracy: 0.5738\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.8483 - val_accuracy: 0.5738\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.8040 - val_accuracy: 0.6066\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.8228 - val_accuracy: 0.5574\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.7653 - val_accuracy: 0.6230\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.7389 - val_accuracy: 0.6557\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.6493 - val_accuracy: 0.7377\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.7657 - val_accuracy: 0.6885\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.9758 - val_accuracy: 0.6066\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0199 - accuracy: 0.9982 - val_loss: 0.8287 - val_accuracy: 0.6721\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.6602 - val_accuracy: 0.7377\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.7651 - val_accuracy: 0.6393\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.7890 - val_accuracy: 0.6721\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.7346 - val_accuracy: 0.7049\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.7773 - val_accuracy: 0.6557\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.7627 - val_accuracy: 0.7049\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.9075 - val_accuracy: 0.6393\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.7929 - val_accuracy: 0.6557\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.7652 - val_accuracy: 0.7049\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.7808 - val_accuracy: 0.6885\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.7769 - val_accuracy: 0.6721\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.9228 - val_accuracy: 0.6393\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.7996 - val_accuracy: 0.6721\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.7822 - val_accuracy: 0.6885\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.7847 - val_accuracy: 0.7049\n",
      "Epoch 42/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.8231 - val_accuracy: 0.6885\n",
      "Epoch 43/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.8511 - val_accuracy: 0.6393\n",
      "Epoch 44/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.8715 - val_accuracy: 0.6393\n",
      "Epoch 45/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.8682 - val_accuracy: 0.6393\n",
      "Epoch 46/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.7599 - val_accuracy: 0.7049\n",
      "Epoch 47/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.7733 - val_accuracy: 0.7213\n",
      "Epoch 48/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.8689 - val_accuracy: 0.6557\n",
      "Epoch 49/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.8572 - val_accuracy: 0.6885\n",
      "Epoch 50/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.7936 - val_accuracy: 0.7377\n",
      "Epoch 51/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.7917 - val_accuracy: 0.6721\n",
      "Epoch 52/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.8349 - val_accuracy: 0.6557\n",
      "Epoch 53/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.7393 - val_accuracy: 0.7049\n",
      "Epoch 54/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.7309 - val_accuracy: 0.7213\n",
      "Epoch 55/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.7612 - val_accuracy: 0.7049\n",
      "Epoch 56/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.7790 - val_accuracy: 0.7541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.7390 - val_accuracy: 0.7377\n",
      "Epoch 58/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.7561 - val_accuracy: 0.7377\n",
      "Epoch 59/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.7970 - val_accuracy: 0.7213\n",
      "Epoch 60/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.8652 - val_accuracy: 0.6557\n",
      "Epoch 61/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.8862 - val_accuracy: 0.6721\n",
      "Epoch 62/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.8508 - val_accuracy: 0.7377\n",
      "Epoch 63/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.7819 - val_accuracy: 0.7213\n",
      "Epoch 64/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.7837 - val_accuracy: 0.7541\n",
      "Epoch 65/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.8415 - val_accuracy: 0.6885\n",
      "Epoch 66/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.8025 - val_accuracy: 0.6721\n",
      "Epoch 67/100\n",
      "556/556 [==============================] - 5s 10ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.8394 - val_accuracy: 0.6885\n",
      "Epoch 68/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.8227 - val_accuracy: 0.7213\n",
      "Epoch 69/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.9312 - val_accuracy: 0.6557\n",
      "Epoch 70/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.9017 - val_accuracy: 0.7049\n",
      "Epoch 71/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.8804 - val_accuracy: 0.7049\n",
      "Epoch 72/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8505 - val_accuracy: 0.6885\n",
      "Epoch 73/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.8692 - val_accuracy: 0.7049\n",
      "Epoch 74/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.8073 - val_accuracy: 0.7869\n",
      "Epoch 75/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.8249 - val_accuracy: 0.8033\n",
      "Epoch 76/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.7495 - val_accuracy: 0.8033\n",
      "Epoch 77/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.7591 - val_accuracy: 0.7705\n",
      "Epoch 78/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.7620 - val_accuracy: 0.7377\n",
      "Epoch 79/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.7983 - val_accuracy: 0.7377\n",
      "Epoch 80/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8232 - val_accuracy: 0.7213\n",
      "Epoch 81/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.8293 - val_accuracy: 0.7213\n",
      "Epoch 82/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.8212 - val_accuracy: 0.7541\n",
      "Epoch 83/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.8026 - val_accuracy: 0.7541\n",
      "Epoch 84/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.7558 - val_accuracy: 0.7705\n",
      "Epoch 85/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.7884 - val_accuracy: 0.7213\n",
      "Epoch 86/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8191 - val_accuracy: 0.7377\n",
      "Epoch 87/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.8053 - val_accuracy: 0.7377\n",
      "Epoch 88/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.8015 - val_accuracy: 0.7377\n",
      "Epoch 89/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8231 - val_accuracy: 0.7705\n",
      "Epoch 90/100\n",
      "556/556 [==============================] - 5s 10ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.8178 - val_accuracy: 0.7541\n",
      "Epoch 91/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8776 - val_accuracy: 0.7213\n",
      "Epoch 92/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8368 - val_accuracy: 0.6885\n",
      "Epoch 93/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.8382 - val_accuracy: 0.7049\n",
      "Epoch 94/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 8.7655e-04 - accuracy: 1.0000 - val_loss: 0.8108 - val_accuracy: 0.7049\n",
      "Epoch 95/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 9.9897e-04 - accuracy: 1.0000 - val_loss: 0.8345 - val_accuracy: 0.7049\n",
      "Epoch 96/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.8783 - val_accuracy: 0.7049\n",
      "Epoch 97/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 8.3125e-04 - accuracy: 1.0000 - val_loss: 0.8720 - val_accuracy: 0.7049\n",
      "Epoch 98/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8754 - val_accuracy: 0.7049\n",
      "Epoch 99/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8870 - val_accuracy: 0.7213\n",
      "Epoch 100/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 7.5690e-04 - accuracy: 1.0000 - val_loss: 0.8526 - val_accuracy: 0.7377\n",
      "accuracy for model 6 is 73.77049326896667\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 623198, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 623191, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 623186, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 623186, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 311593, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 311593, 6)         24        \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1869558)           0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 24)                44869416  \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 44,871,087\n",
      "Trainable params: 44,871,059\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "556/556 [==============================] - 19s 35ms/sample - loss: 0.9067 - accuracy: 0.5881 - val_loss: 0.7099 - val_accuracy: 0.7705\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 5s 10ms/sample - loss: 0.8208 - accuracy: 0.6565 - val_loss: 0.8420 - val_accuracy: 0.7213\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.7641 - accuracy: 0.7014 - val_loss: 0.9386 - val_accuracy: 0.4754\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.7371 - accuracy: 0.7086 - val_loss: 0.9991 - val_accuracy: 0.3443\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 5s 10ms/sample - loss: 0.6770 - accuracy: 0.7374 - val_loss: 1.1292 - val_accuracy: 0.1803\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.6165 - accuracy: 0.7968 - val_loss: 1.3208 - val_accuracy: 0.1803\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.4947 - accuracy: 0.8471 - val_loss: 1.4884 - val_accuracy: 0.1803\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.3866 - accuracy: 0.8957 - val_loss: 1.6806 - val_accuracy: 0.1803\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.2540 - accuracy: 0.9532 - val_loss: 1.4955 - val_accuracy: 0.1803\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.1736 - accuracy: 0.9784 - val_loss: 1.5718 - val_accuracy: 0.1803\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.1192 - accuracy: 0.9946 - val_loss: 1.3606 - val_accuracy: 0.1803\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0927 - accuracy: 0.9946 - val_loss: 1.0736 - val_accuracy: 0.3443\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0767 - accuracy: 0.9964 - val_loss: 1.2571 - val_accuracy: 0.1967\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0570 - accuracy: 1.0000 - val_loss: 1.0522 - val_accuracy: 0.3443\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0437 - accuracy: 1.0000 - val_loss: 1.0005 - val_accuracy: 0.4590\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0432 - accuracy: 1.0000 - val_loss: 0.9288 - val_accuracy: 0.5246\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.9051 - val_accuracy: 0.6066\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.7995 - val_accuracy: 0.7213\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.7635 - val_accuracy: 0.7705\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.7730 - val_accuracy: 0.7377\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.7281 - val_accuracy: 0.7869\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.7190 - val_accuracy: 0.7869\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.6953 - val_accuracy: 0.8033\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.7166 - val_accuracy: 0.8033\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.7213 - val_accuracy: 0.8197\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.7348 - val_accuracy: 0.8197\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.7371 - val_accuracy: 0.7705\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.7145 - val_accuracy: 0.8033\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.7045 - val_accuracy: 0.7869\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.7714 - val_accuracy: 0.8361\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.7473 - val_accuracy: 0.7705\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.7924 - val_accuracy: 0.7869\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.8168 - val_accuracy: 0.8033\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 5s 10ms/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.8260 - val_accuracy: 0.7705\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.7438 - val_accuracy: 0.7705\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.7878 - val_accuracy: 0.7705\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.8297 - val_accuracy: 0.8197\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.8132 - val_accuracy: 0.8033\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.8575 - val_accuracy: 0.8197\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.8429 - val_accuracy: 0.8197\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.8719 - val_accuracy: 0.7869\n",
      "Epoch 42/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.8731 - val_accuracy: 0.8197\n",
      "Epoch 43/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.8498 - val_accuracy: 0.8033\n",
      "Epoch 44/100\n",
      "556/556 [==============================] - 5s 10ms/sample - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.8671 - val_accuracy: 0.8197\n",
      "Epoch 45/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.8482 - val_accuracy: 0.7869\n",
      "Epoch 46/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.8720 - val_accuracy: 0.8197\n",
      "Epoch 47/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.8541 - val_accuracy: 0.7541\n",
      "Epoch 48/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.8748 - val_accuracy: 0.7705\n",
      "Epoch 49/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.8565 - val_accuracy: 0.7869\n",
      "Epoch 50/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.8898 - val_accuracy: 0.7541\n",
      "Epoch 51/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.9072 - val_accuracy: 0.7705\n",
      "Epoch 52/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.9036 - val_accuracy: 0.7705\n",
      "Epoch 53/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.8712 - val_accuracy: 0.7541\n",
      "Epoch 54/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.9232 - val_accuracy: 0.8033\n",
      "Epoch 55/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.8868 - val_accuracy: 0.7869\n",
      "Epoch 56/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.8823 - val_accuracy: 0.7869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.8807 - val_accuracy: 0.7541\n",
      "Epoch 58/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.8925 - val_accuracy: 0.7705\n",
      "Epoch 59/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.9030 - val_accuracy: 0.7869\n",
      "Epoch 60/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.9110 - val_accuracy: 0.7869\n",
      "Epoch 61/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.9116 - val_accuracy: 0.7869\n",
      "Epoch 62/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.9496 - val_accuracy: 0.8033\n",
      "Epoch 63/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.9551 - val_accuracy: 0.8033\n",
      "Epoch 64/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.9373 - val_accuracy: 0.7869\n",
      "Epoch 65/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.9363 - val_accuracy: 0.7869\n",
      "Epoch 66/100\n",
      "556/556 [==============================] - 5s 10ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.9292 - val_accuracy: 0.7705\n",
      "Epoch 67/100\n",
      "556/556 [==============================] - 5s 10ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.9105 - val_accuracy: 0.7705\n",
      "Epoch 68/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.9582 - val_accuracy: 0.7705\n",
      "Epoch 69/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.9241 - val_accuracy: 0.7541\n",
      "Epoch 70/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.9645 - val_accuracy: 0.8033\n",
      "Epoch 71/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.9726 - val_accuracy: 0.8033\n",
      "Epoch 72/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.0197 - val_accuracy: 0.8033\n",
      "Epoch 73/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.9393 - val_accuracy: 0.8033\n",
      "Epoch 74/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.9208 - val_accuracy: 0.7869\n",
      "Epoch 75/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.9361 - val_accuracy: 0.7705\n",
      "Epoch 76/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.0106 - val_accuracy: 0.8197\n",
      "Epoch 77/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.9791 - val_accuracy: 0.7869\n",
      "Epoch 78/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.9874 - val_accuracy: 0.8033\n",
      "Epoch 79/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.0187 - val_accuracy: 0.8361\n",
      "Epoch 80/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.0003 - val_accuracy: 0.8033\n",
      "Epoch 81/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.9740 - val_accuracy: 0.7869\n",
      "Epoch 82/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9921 - val_accuracy: 0.7869\n",
      "Epoch 83/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9926 - val_accuracy: 0.7869\n",
      "Epoch 84/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.0041 - val_accuracy: 0.7705\n",
      "Epoch 85/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9999 - val_accuracy: 0.7705\n",
      "Epoch 86/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.9852 - val_accuracy: 0.7705\n",
      "Epoch 87/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.0078 - val_accuracy: 0.7705\n",
      "Epoch 88/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9421 - val_accuracy: 0.7377\n",
      "Epoch 89/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.9831 - val_accuracy: 0.7705\n",
      "Epoch 90/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.1063 - val_accuracy: 0.8197\n",
      "Epoch 91/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.0596 - val_accuracy: 0.8197\n",
      "Epoch 92/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9996 - val_accuracy: 0.7705\n",
      "Epoch 93/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.1455 - val_accuracy: 0.8361\n",
      "Epoch 94/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.0417 - val_accuracy: 0.7869\n",
      "Epoch 95/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.0482 - val_accuracy: 0.7869\n",
      "Epoch 96/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.0908 - val_accuracy: 0.8197\n",
      "Epoch 97/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.1157 - val_accuracy: 0.8197\n",
      "Epoch 98/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 9.0207e-04 - accuracy: 1.0000 - val_loss: 1.0111 - val_accuracy: 0.7377\n",
      "Epoch 99/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 9.0297e-04 - accuracy: 1.0000 - val_loss: 1.0406 - val_accuracy: 0.8033\n",
      "Epoch 100/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.0455 - val_accuracy: 0.7869\n",
      "accuracy for model 7 is 78.68852615356445\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_24 (Conv1D)           (None, 623198, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 623191, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 623186, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 623186, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 311593, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 311593, 6)         24        \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1869558)           0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 24)                44869416  \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 44,871,087\n",
      "Trainable params: 44,871,059\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "556/556 [==============================] - 30s 54ms/sample - loss: 0.8448 - accuracy: 0.6277 - val_loss: 0.8687 - val_accuracy: 0.6066\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.7299 - accuracy: 0.7158 - val_loss: 1.0102 - val_accuracy: 0.5082\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.7272 - accuracy: 0.6924 - val_loss: 1.2078 - val_accuracy: 0.1803\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.6965 - accuracy: 0.7050 - val_loss: 1.2767 - val_accuracy: 0.1803\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.6502 - accuracy: 0.7338 - val_loss: 1.4109 - val_accuracy: 0.1803\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.6091 - accuracy: 0.7518 - val_loss: 1.5488 - val_accuracy: 0.1803\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.5354 - accuracy: 0.8022 - val_loss: 1.9635 - val_accuracy: 0.1803\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.4048 - accuracy: 0.8669 - val_loss: 1.8712 - val_accuracy: 0.1803\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.2300 - accuracy: 0.9568 - val_loss: 1.8447 - val_accuracy: 0.1803\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.1540 - accuracy: 0.9910 - val_loss: 1.4293 - val_accuracy: 0.1803\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.1034 - accuracy: 0.9964 - val_loss: 1.3568 - val_accuracy: 0.1803\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0870 - accuracy: 0.9946 - val_loss: 1.2207 - val_accuracy: 0.2459\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0603 - accuracy: 1.0000 - val_loss: 1.1628 - val_accuracy: 0.2459\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0445 - accuracy: 1.0000 - val_loss: 1.1457 - val_accuracy: 0.2787\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0410 - accuracy: 0.9982 - val_loss: 1.1274 - val_accuracy: 0.3279\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0379 - accuracy: 1.0000 - val_loss: 1.0576 - val_accuracy: 0.4098\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0353 - accuracy: 1.0000 - val_loss: 0.9385 - val_accuracy: 0.4918\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.8656 - val_accuracy: 0.6393\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.8974 - val_accuracy: 0.5410\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0213 - accuracy: 1.0000 - val_loss: 0.8611 - val_accuracy: 0.6230\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 5s 10ms/sample - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.8609 - val_accuracy: 0.6557\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.8528 - val_accuracy: 0.7377\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0218 - accuracy: 1.0000 - val_loss: 0.9466 - val_accuracy: 0.5902\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.8940 - val_accuracy: 0.6557\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.8828 - val_accuracy: 0.6721\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.9130 - val_accuracy: 0.7213\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.9394 - val_accuracy: 0.6721\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.9417 - val_accuracy: 0.6557\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.9830 - val_accuracy: 0.6721\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0116 - accuracy: 1.0000 - val_loss: 1.0212 - val_accuracy: 0.6721\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0112 - accuracy: 1.0000 - val_loss: 1.0162 - val_accuracy: 0.6721\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0114 - accuracy: 1.0000 - val_loss: 1.0493 - val_accuracy: 0.6557\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.1554 - val_accuracy: 0.6721\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.1051 - val_accuracy: 0.6721\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 1.1008 - val_accuracy: 0.6557\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0102 - accuracy: 1.0000 - val_loss: 1.2358 - val_accuracy: 0.6721\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0093 - accuracy: 1.0000 - val_loss: 1.1745 - val_accuracy: 0.6393\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0094 - accuracy: 0.9982 - val_loss: 1.2254 - val_accuracy: 0.6230\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0380 - accuracy: 0.9982 - val_loss: 1.4373 - val_accuracy: 0.6721\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0180 - accuracy: 1.0000 - val_loss: 1.5288 - val_accuracy: 0.6230\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0141 - accuracy: 1.0000 - val_loss: 1.3783 - val_accuracy: 0.5738\n",
      "Epoch 42/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0092 - accuracy: 1.0000 - val_loss: 1.3318 - val_accuracy: 0.6230\n",
      "Epoch 43/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.3673 - val_accuracy: 0.5902\n",
      "Epoch 44/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.3158 - val_accuracy: 0.6230\n",
      "Epoch 45/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.3284 - val_accuracy: 0.6066\n",
      "Epoch 46/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.3267 - val_accuracy: 0.6066\n",
      "Epoch 47/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.3768 - val_accuracy: 0.6393\n",
      "Epoch 48/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.3051 - val_accuracy: 0.6230\n",
      "Epoch 49/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.4317 - val_accuracy: 0.7049\n",
      "Epoch 50/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.3186 - val_accuracy: 0.6393\n",
      "Epoch 51/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.4307 - val_accuracy: 0.6721\n",
      "Epoch 52/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.6338 - val_accuracy: 0.7213\n",
      "Epoch 53/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.3696 - val_accuracy: 0.6721\n",
      "Epoch 54/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4126 - val_accuracy: 0.6557\n",
      "Epoch 55/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3789 - val_accuracy: 0.6393\n",
      "Epoch 56/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.4411 - val_accuracy: 0.6393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.3913 - val_accuracy: 0.6230\n",
      "Epoch 58/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3395 - val_accuracy: 0.6393\n",
      "Epoch 59/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.3772 - val_accuracy: 0.6393\n",
      "Epoch 60/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.4318 - val_accuracy: 0.6230\n",
      "Epoch 61/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.4009 - val_accuracy: 0.6393\n",
      "Epoch 62/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.4157 - val_accuracy: 0.6557\n",
      "Epoch 63/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.4056 - val_accuracy: 0.6393\n",
      "Epoch 64/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.3958 - val_accuracy: 0.6557\n",
      "Epoch 65/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.3852 - val_accuracy: 0.7213\n",
      "Epoch 66/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.5620 - val_accuracy: 0.7049\n",
      "Epoch 67/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.4785 - val_accuracy: 0.6557\n",
      "Epoch 68/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.4613 - val_accuracy: 0.6393\n",
      "Epoch 69/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3406 - val_accuracy: 0.6066\n",
      "Epoch 70/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.3879 - val_accuracy: 0.6230\n",
      "Epoch 71/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.4037 - val_accuracy: 0.6393\n",
      "Epoch 72/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.3541 - val_accuracy: 0.6230\n",
      "Epoch 73/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.4170 - val_accuracy: 0.6393\n",
      "Epoch 74/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.4174 - val_accuracy: 0.6393\n",
      "Epoch 75/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.4263 - val_accuracy: 0.6230\n",
      "Epoch 76/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.4207 - val_accuracy: 0.6393\n",
      "Epoch 77/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.4410 - val_accuracy: 0.6557\n",
      "Epoch 78/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.4798 - val_accuracy: 0.6721\n",
      "Epoch 79/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.5018 - val_accuracy: 0.6230\n",
      "Epoch 80/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.4857 - val_accuracy: 0.6230\n",
      "Epoch 81/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.5961 - val_accuracy: 0.6721\n",
      "Epoch 82/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.5032 - val_accuracy: 0.6230\n",
      "Epoch 83/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.4954 - val_accuracy: 0.6230\n",
      "Epoch 84/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.5065 - val_accuracy: 0.6557\n",
      "Epoch 85/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.5358 - val_accuracy: 0.6557\n",
      "Epoch 86/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.4902 - val_accuracy: 0.6393\n",
      "Epoch 87/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.5287 - val_accuracy: 0.6393\n",
      "Epoch 88/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.5713 - val_accuracy: 0.6393\n",
      "Epoch 89/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.5165 - val_accuracy: 0.6230\n",
      "Epoch 90/100\n",
      "556/556 [==============================] - 5s 10ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.5444 - val_accuracy: 0.6393\n",
      "Epoch 91/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.5235 - val_accuracy: 0.6557\n",
      "Epoch 92/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 9.6032e-04 - accuracy: 1.0000 - val_loss: 1.5953 - val_accuracy: 0.6393\n",
      "Epoch 93/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 8.5955e-04 - accuracy: 1.0000 - val_loss: 1.5394 - val_accuracy: 0.6393\n",
      "Epoch 94/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 8.8433e-04 - accuracy: 1.0000 - val_loss: 1.5360 - val_accuracy: 0.6393\n",
      "Epoch 95/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 9.1205e-04 - accuracy: 1.0000 - val_loss: 1.5533 - val_accuracy: 0.6393\n",
      "Epoch 96/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 8.5309e-04 - accuracy: 1.0000 - val_loss: 1.5590 - val_accuracy: 0.6557\n",
      "Epoch 97/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 7.3640e-04 - accuracy: 1.0000 - val_loss: 1.5641 - val_accuracy: 0.6230\n",
      "Epoch 98/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 7.9859e-04 - accuracy: 1.0000 - val_loss: 1.5783 - val_accuracy: 0.6721\n",
      "Epoch 99/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 7.6933e-04 - accuracy: 1.0000 - val_loss: 1.5629 - val_accuracy: 0.6557\n",
      "Epoch 100/100\n",
      "556/556 [==============================] - 5s 9ms/sample - loss: 6.2726e-04 - accuracy: 1.0000 - val_loss: 1.5238 - val_accuracy: 0.6557\n",
      "accuracy for model 8 is 65.57376980781555\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_27 (Conv1D)           (None, 623198, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 623191, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 623186, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 623186, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 311593, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 311593, 6)         24        \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1869558)           0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 24)                44869416  \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 44,871,087\n",
      "Trainable params: 44,871,059\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 557 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      "557/557 [==============================] - 42s 75ms/sample - loss: 0.9983 - accuracy: 0.5691 - val_loss: 0.9559 - val_accuracy: 0.4833\n",
      "Epoch 2/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.7187 - accuracy: 0.6840 - val_loss: 1.0574 - val_accuracy: 0.4000\n",
      "Epoch 3/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.6782 - accuracy: 0.7181 - val_loss: 1.1393 - val_accuracy: 0.2500\n",
      "Epoch 4/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.6510 - accuracy: 0.7235 - val_loss: 1.1930 - val_accuracy: 0.2000\n",
      "Epoch 5/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.6317 - accuracy: 0.7397 - val_loss: 1.2815 - val_accuracy: 0.1667\n",
      "Epoch 6/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.5657 - accuracy: 0.7774 - val_loss: 1.3379 - val_accuracy: 0.1667\n",
      "Epoch 7/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.5162 - accuracy: 0.8151 - val_loss: 1.4298 - val_accuracy: 0.1667\n",
      "Epoch 8/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.4287 - accuracy: 0.8779 - val_loss: 1.5443 - val_accuracy: 0.1667\n",
      "Epoch 9/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.3255 - accuracy: 0.9282 - val_loss: 1.6487 - val_accuracy: 0.1667\n",
      "Epoch 10/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.2471 - accuracy: 0.9569 - val_loss: 1.5912 - val_accuracy: 0.1667\n",
      "Epoch 11/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.1764 - accuracy: 0.9856 - val_loss: 1.4724 - val_accuracy: 0.1667\n",
      "Epoch 12/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.1380 - accuracy: 0.9964 - val_loss: 1.4121 - val_accuracy: 0.1667\n",
      "Epoch 13/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.1037 - accuracy: 0.9982 - val_loss: 1.2943 - val_accuracy: 0.1667\n",
      "Epoch 14/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0914 - accuracy: 0.9982 - val_loss: 1.4264 - val_accuracy: 0.1667\n",
      "Epoch 15/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0741 - accuracy: 0.9982 - val_loss: 1.3477 - val_accuracy: 0.1667\n",
      "Epoch 16/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0515 - accuracy: 1.0000 - val_loss: 1.2517 - val_accuracy: 0.1667\n",
      "Epoch 17/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0437 - accuracy: 1.0000 - val_loss: 1.2401 - val_accuracy: 0.1833\n",
      "Epoch 18/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.9918 - val_accuracy: 0.5167\n",
      "Epoch 19/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.9032 - val_accuracy: 0.6333\n",
      "Epoch 20/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.8663 - val_accuracy: 0.6167\n",
      "Epoch 21/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.8784 - val_accuracy: 0.6333\n",
      "Epoch 22/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.7365 - val_accuracy: 0.6833\n",
      "Epoch 23/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.7851 - val_accuracy: 0.6833\n",
      "Epoch 24/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.7499 - val_accuracy: 0.6833\n",
      "Epoch 25/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.7479 - val_accuracy: 0.7000\n",
      "Epoch 26/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.7297 - val_accuracy: 0.6833\n",
      "Epoch 27/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.7390 - val_accuracy: 0.7000\n",
      "Epoch 28/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.7474 - val_accuracy: 0.7000\n",
      "Epoch 29/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.7376 - val_accuracy: 0.7167\n",
      "Epoch 30/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.7643 - val_accuracy: 0.7000\n",
      "Epoch 31/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.7487 - val_accuracy: 0.7000\n",
      "Epoch 32/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.7778 - val_accuracy: 0.7167\n",
      "Epoch 33/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.7519 - val_accuracy: 0.7167\n",
      "Epoch 34/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.7901 - val_accuracy: 0.6833\n",
      "Epoch 35/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.7809 - val_accuracy: 0.6833\n",
      "Epoch 36/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.8202 - val_accuracy: 0.6833\n",
      "Epoch 37/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.8017 - val_accuracy: 0.7167\n",
      "Epoch 38/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.8197 - val_accuracy: 0.7000\n",
      "Epoch 39/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.8605 - val_accuracy: 0.7000\n",
      "Epoch 40/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.8162 - val_accuracy: 0.6833\n",
      "Epoch 41/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.8199 - val_accuracy: 0.7000\n",
      "Epoch 42/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.8691 - val_accuracy: 0.6833\n",
      "Epoch 43/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.8658 - val_accuracy: 0.7167\n",
      "Epoch 44/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.8794 - val_accuracy: 0.7167\n",
      "Epoch 45/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.8403 - val_accuracy: 0.7000\n",
      "Epoch 46/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.8681 - val_accuracy: 0.7333\n",
      "Epoch 47/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.9358 - val_accuracy: 0.7000\n",
      "Epoch 48/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.8632 - val_accuracy: 0.7167\n",
      "Epoch 49/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.8599 - val_accuracy: 0.7000\n",
      "Epoch 50/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.9207 - val_accuracy: 0.7167\n",
      "Epoch 51/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.8933 - val_accuracy: 0.7167\n",
      "Epoch 52/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.8587 - val_accuracy: 0.7167\n",
      "Epoch 53/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.8909 - val_accuracy: 0.7167\n",
      "Epoch 54/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.8969 - val_accuracy: 0.7167\n",
      "Epoch 55/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.8862 - val_accuracy: 0.6833\n",
      "Epoch 56/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.9285 - val_accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.9107 - val_accuracy: 0.7000\n",
      "Epoch 58/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.9313 - val_accuracy: 0.7000\n",
      "Epoch 59/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.9225 - val_accuracy: 0.7167\n",
      "Epoch 60/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.9198 - val_accuracy: 0.7167\n",
      "Epoch 61/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.9309 - val_accuracy: 0.7167\n",
      "Epoch 62/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.9297 - val_accuracy: 0.6833\n",
      "Epoch 63/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.9214 - val_accuracy: 0.7167\n",
      "Epoch 64/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.9366 - val_accuracy: 0.7000\n",
      "Epoch 65/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.0057 - val_accuracy: 0.7000\n",
      "Epoch 66/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.9493 - val_accuracy: 0.7167\n",
      "Epoch 67/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.9192 - val_accuracy: 0.7167\n",
      "Epoch 68/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.9367 - val_accuracy: 0.7000\n",
      "Epoch 69/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.9726 - val_accuracy: 0.7167\n",
      "Epoch 70/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.9557 - val_accuracy: 0.7000\n",
      "Epoch 71/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.0436 - val_accuracy: 0.6833\n",
      "Epoch 72/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.9305 - val_accuracy: 0.7000\n",
      "Epoch 73/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.9660 - val_accuracy: 0.7167\n",
      "Epoch 74/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.9618 - val_accuracy: 0.7167\n",
      "Epoch 75/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.8902 - val_accuracy: 0.7333\n",
      "Epoch 76/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.9771 - val_accuracy: 0.7000\n",
      "Epoch 77/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.9221 - val_accuracy: 0.6833\n",
      "Epoch 78/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.9507 - val_accuracy: 0.7000\n",
      "Epoch 79/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.1047 - val_accuracy: 0.7167\n",
      "Epoch 80/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0125 - accuracy: 0.9982 - val_loss: 1.5453 - val_accuracy: 0.6500\n",
      "Epoch 81/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0298 - accuracy: 0.9946 - val_loss: 2.3954 - val_accuracy: 0.4500\n",
      "Epoch 82/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0270 - accuracy: 0.9946 - val_loss: 2.0218 - val_accuracy: 0.6333\n",
      "Epoch 83/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0297 - accuracy: 0.9946 - val_loss: 1.5565 - val_accuracy: 0.6000\n",
      "Epoch 84/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0422 - accuracy: 0.9838 - val_loss: 0.9327 - val_accuracy: 0.7500\n",
      "Epoch 85/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0362 - accuracy: 0.9910 - val_loss: 1.3752 - val_accuracy: 0.6667\n",
      "Epoch 86/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0178 - accuracy: 0.9982 - val_loss: 1.0354 - val_accuracy: 0.7333\n",
      "Epoch 87/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 1.5075 - val_accuracy: 0.6833\n",
      "Epoch 88/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.9012 - val_accuracy: 0.7333\n",
      "Epoch 89/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0138 - accuracy: 1.0000 - val_loss: 1.0203 - val_accuracy: 0.7333\n",
      "Epoch 90/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.9124 - val_accuracy: 0.7333\n",
      "Epoch 91/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.8518 - val_accuracy: 0.7167\n",
      "Epoch 92/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.8852 - val_accuracy: 0.7000\n",
      "Epoch 93/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.9915 - val_accuracy: 0.7000\n",
      "Epoch 94/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.9365 - val_accuracy: 0.7000\n",
      "Epoch 95/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.9456 - val_accuracy: 0.7333\n",
      "Epoch 96/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.9474 - val_accuracy: 0.7167\n",
      "Epoch 97/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.9990 - val_accuracy: 0.7000\n",
      "Epoch 98/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.0277 - val_accuracy: 0.7000\n",
      "Epoch 99/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.9101 - val_accuracy: 0.7000\n",
      "Epoch 100/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.9270 - val_accuracy: 0.7000\n",
      "accuracy for model 9 is 69.9999988079071\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 623198, 10)        110       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 623198, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 623191, 8)         648       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 623191, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 623186, 6)         294       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 623186, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 311593, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 311593, 6)         24        \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1869558)           0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 24)                44869416  \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 44,871,087\n",
      "Trainable params: 44,871,059\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 557 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      "557/557 [==============================] - 34s 61ms/sample - loss: 0.8959 - accuracy: 0.6355 - val_loss: 0.8933 - val_accuracy: 0.6500\n",
      "Epoch 2/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.7172 - accuracy: 0.7056 - val_loss: 1.1252 - val_accuracy: 0.3167\n",
      "Epoch 3/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.7595 - accuracy: 0.7002 - val_loss: 1.3386 - val_accuracy: 0.2167\n",
      "Epoch 4/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.7089 - accuracy: 0.7038 - val_loss: 1.4535 - val_accuracy: 0.2000\n",
      "Epoch 5/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.6723 - accuracy: 0.7217 - val_loss: 1.5159 - val_accuracy: 0.1667\n",
      "Epoch 6/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.6410 - accuracy: 0.7612 - val_loss: 1.5482 - val_accuracy: 0.1667\n",
      "Epoch 7/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.5919 - accuracy: 0.7630 - val_loss: 1.5275 - val_accuracy: 0.1667\n",
      "Epoch 8/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.5462 - accuracy: 0.8007 - val_loss: 1.5505 - val_accuracy: 0.1667\n",
      "Epoch 9/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.5055 - accuracy: 0.8402 - val_loss: 1.5674 - val_accuracy: 0.1667\n",
      "Epoch 10/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.4375 - accuracy: 0.8779 - val_loss: 1.5519 - val_accuracy: 0.1667\n",
      "Epoch 11/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.3985 - accuracy: 0.9066 - val_loss: 1.5336 - val_accuracy: 0.1667\n",
      "Epoch 12/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.3498 - accuracy: 0.9282 - val_loss: 1.5063 - val_accuracy: 0.1667\n",
      "Epoch 13/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.3044 - accuracy: 0.9336 - val_loss: 1.5271 - val_accuracy: 0.1667\n",
      "Epoch 14/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.2595 - accuracy: 0.9479 - val_loss: 1.7197 - val_accuracy: 0.1667\n",
      "Epoch 15/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.2160 - accuracy: 0.9461 - val_loss: 1.9826 - val_accuracy: 0.1667\n",
      "Epoch 16/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.1636 - accuracy: 0.9749 - val_loss: 2.0096 - val_accuracy: 0.1667\n",
      "Epoch 17/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.1251 - accuracy: 0.9856 - val_loss: 2.1152 - val_accuracy: 0.1667\n",
      "Epoch 18/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0993 - accuracy: 0.9928 - val_loss: 1.6303 - val_accuracy: 0.1667\n",
      "Epoch 19/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0801 - accuracy: 0.9946 - val_loss: 1.4904 - val_accuracy: 0.1667\n",
      "Epoch 20/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0638 - accuracy: 0.9982 - val_loss: 1.6494 - val_accuracy: 0.2000\n",
      "Epoch 21/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0415 - accuracy: 1.0000 - val_loss: 1.1506 - val_accuracy: 0.3500\n",
      "Epoch 22/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0393 - accuracy: 1.0000 - val_loss: 1.0589 - val_accuracy: 0.4667\n",
      "Epoch 23/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0367 - accuracy: 1.0000 - val_loss: 1.0064 - val_accuracy: 0.4500\n",
      "Epoch 24/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.9248 - val_accuracy: 0.6000\n",
      "Epoch 25/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0445 - accuracy: 0.9910 - val_loss: 1.0204 - val_accuracy: 0.4833\n",
      "Epoch 26/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0298 - accuracy: 0.9982 - val_loss: 0.9114 - val_accuracy: 0.6333\n",
      "Epoch 27/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0296 - accuracy: 0.9964 - val_loss: 0.9244 - val_accuracy: 0.6000\n",
      "Epoch 28/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0230 - accuracy: 1.0000 - val_loss: 1.0251 - val_accuracy: 0.5167\n",
      "Epoch 29/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0187 - accuracy: 1.0000 - val_loss: 0.9290 - val_accuracy: 0.6167\n",
      "Epoch 30/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.9084 - val_accuracy: 0.6500\n",
      "Epoch 31/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.9088 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.9551 - val_accuracy: 0.6000\n",
      "Epoch 33/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.9370 - val_accuracy: 0.6167\n",
      "Epoch 34/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.9679 - val_accuracy: 0.6167\n",
      "Epoch 35/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0119 - accuracy: 1.0000 - val_loss: 1.0347 - val_accuracy: 0.6167\n",
      "Epoch 36/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.9859 - val_accuracy: 0.6333\n",
      "Epoch 37/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0120 - accuracy: 1.0000 - val_loss: 1.0204 - val_accuracy: 0.6167\n",
      "Epoch 38/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0136 - accuracy: 1.0000 - val_loss: 1.0940 - val_accuracy: 0.6167\n",
      "Epoch 39/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0127 - accuracy: 1.0000 - val_loss: 1.0661 - val_accuracy: 0.6500\n",
      "Epoch 40/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0108 - accuracy: 1.0000 - val_loss: 1.1457 - val_accuracy: 0.6000\n",
      "Epoch 41/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.1193 - val_accuracy: 0.6333\n",
      "Epoch 42/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0114 - accuracy: 1.0000 - val_loss: 1.1586 - val_accuracy: 0.6167\n",
      "Epoch 43/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 1.1121 - val_accuracy: 0.6000\n",
      "Epoch 44/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0111 - accuracy: 0.9982 - val_loss: 1.2704 - val_accuracy: 0.6333\n",
      "Epoch 45/100\n",
      "557/557 [==============================] - 5s 10ms/sample - loss: 0.0100 - accuracy: 1.0000 - val_loss: 1.3587 - val_accuracy: 0.5833\n",
      "Epoch 46/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.3027 - val_accuracy: 0.6167\n",
      "Epoch 47/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0099 - accuracy: 1.0000 - val_loss: 1.2653 - val_accuracy: 0.6000\n",
      "Epoch 48/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.2779 - val_accuracy: 0.6167\n",
      "Epoch 49/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0097 - accuracy: 1.0000 - val_loss: 1.2244 - val_accuracy: 0.6333\n",
      "Epoch 50/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.3198 - val_accuracy: 0.6167\n",
      "Epoch 51/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.2716 - val_accuracy: 0.6167\n",
      "Epoch 52/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.3995 - val_accuracy: 0.6167\n",
      "Epoch 53/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.3500 - val_accuracy: 0.6167\n",
      "Epoch 54/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.3190 - val_accuracy: 0.6000\n",
      "Epoch 55/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 1.3074 - val_accuracy: 0.6333\n",
      "Epoch 56/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.3902 - val_accuracy: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2999 - val_accuracy: 0.6167\n",
      "Epoch 58/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0046 - accuracy: 1.0000 - val_loss: 1.2928 - val_accuracy: 0.6333\n",
      "Epoch 59/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.3744 - val_accuracy: 0.6333\n",
      "Epoch 60/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.3383 - val_accuracy: 0.6333\n",
      "Epoch 61/100\n",
      "557/557 [==============================] - 5s 10ms/sample - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.3497 - val_accuracy: 0.6333\n",
      "Epoch 62/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0064 - accuracy: 1.0000 - val_loss: 1.3027 - val_accuracy: 0.6500\n",
      "Epoch 63/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.4126 - val_accuracy: 0.6333\n",
      "Epoch 64/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.3119 - val_accuracy: 0.6333\n",
      "Epoch 65/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4039 - val_accuracy: 0.6333\n",
      "Epoch 66/100\n",
      "557/557 [==============================] - 5s 10ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.4043 - val_accuracy: 0.6500\n",
      "Epoch 67/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4252 - val_accuracy: 0.6333\n",
      "Epoch 68/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.3885 - val_accuracy: 0.6500\n",
      "Epoch 69/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.4251 - val_accuracy: 0.6500\n",
      "Epoch 70/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.4349 - val_accuracy: 0.6500\n",
      "Epoch 71/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.3902 - val_accuracy: 0.6333\n",
      "Epoch 72/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.3539 - val_accuracy: 0.6333\n",
      "Epoch 73/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.3725 - val_accuracy: 0.6333\n",
      "Epoch 74/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4180 - val_accuracy: 0.6333\n",
      "Epoch 75/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.4596 - val_accuracy: 0.6500\n",
      "Epoch 76/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.3788 - val_accuracy: 0.6333\n",
      "Epoch 77/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.5192 - val_accuracy: 0.6333\n",
      "Epoch 78/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.4953 - val_accuracy: 0.6167\n",
      "Epoch 79/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.4188 - val_accuracy: 0.6167\n",
      "Epoch 80/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.5216 - val_accuracy: 0.6500\n",
      "Epoch 81/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.4617 - val_accuracy: 0.6333\n",
      "Epoch 82/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.5047 - val_accuracy: 0.6000\n",
      "Epoch 83/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4477 - val_accuracy: 0.6333\n",
      "Epoch 84/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.6014 - val_accuracy: 0.6167\n",
      "Epoch 85/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.4561 - val_accuracy: 0.6167\n",
      "Epoch 86/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.5219 - val_accuracy: 0.6333\n",
      "Epoch 87/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.5484 - val_accuracy: 0.6333\n",
      "Epoch 88/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.5077 - val_accuracy: 0.6333\n",
      "Epoch 89/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.5705 - val_accuracy: 0.6167\n",
      "Epoch 90/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0139 - accuracy: 0.9964 - val_loss: 2.0785 - val_accuracy: 0.5000\n",
      "Epoch 91/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0327 - accuracy: 0.9910 - val_loss: 3.4535 - val_accuracy: 0.3667\n",
      "Epoch 92/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0810 - accuracy: 0.9695 - val_loss: 2.2259 - val_accuracy: 0.5500\n",
      "Epoch 93/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.1271 - accuracy: 0.9605 - val_loss: 2.0047 - val_accuracy: 0.6667\n",
      "Epoch 94/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0500 - accuracy: 0.9874 - val_loss: 2.1610 - val_accuracy: 0.6000\n",
      "Epoch 95/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0337 - accuracy: 0.9964 - val_loss: 1.6247 - val_accuracy: 0.5833\n",
      "Epoch 96/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0262 - accuracy: 0.9946 - val_loss: 3.1696 - val_accuracy: 0.6667\n",
      "Epoch 97/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0219 - accuracy: 0.9964 - val_loss: 1.6552 - val_accuracy: 0.6167\n",
      "Epoch 98/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0096 - accuracy: 0.9982 - val_loss: 1.2095 - val_accuracy: 0.6500\n",
      "Epoch 99/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0111 - accuracy: 0.9982 - val_loss: 1.4985 - val_accuracy: 0.6000\n",
      "Epoch 100/100\n",
      "557/557 [==============================] - 5s 9ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.5626 - val_accuracy: 0.6500\n",
      "accuracy for model 10 is 64.99999761581421\n",
      "Training Testing Accuracy: 76.07% (7.07%)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9814f90846e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_CNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt_vcf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt_pheno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_CNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PoC_CNN_model.pickle.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "best_CNN = eval_cnn(tt_vcf, tt_pheno, 10, mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4b2685813010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_CNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PoC_CNN_model.pickle.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(best_CNN, open(\"PoC_CNN_model.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout accuracy is 70.32257914543152\n"
     ]
    }
   ],
   "source": [
    "bs = ((ho_vcf.shape[0])/40)\n",
    "bs = round(bs)\n",
    "ho_vcf = ho_vcf.reshape(ho_vcf.shape[0], ho_vcf.shape[1],1)\n",
    "ho_pheno = mlb.transform(ho_pheno)\n",
    "_, accuracy = best_CNN.evaluate(ho_vcf, ho_pheno, batch_size=bs, verbose=0)\n",
    "print(\"Holdout accuracy is \" + str(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN (based off yield prediction paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "(617,)\n",
      "(617, 1)\n",
      "220000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "(155,)\n",
      "(155, 1)\n",
      "220000\n",
      "(617, 214899)\n",
      "(155, 214899)\n",
      "(617, 1)\n",
      "0.0\n",
      "(617, 1)\n",
      "(155, 1)\n"
     ]
    }
   ],
   "source": [
    "tt_vcf, ho_vcf, tt_pheno, ho_pheno = new_prep_data(\"PoC_Merged_filtered.csv_train_test.csv_5pcnt.csv\", \"PoC_Merged_filtered.csv_holdout.csv_5pcnt.csv\")\n",
    "r_t = tt_pheno.ravel()\n",
    "r_h = ho_pheno.ravel()\n",
    "print(r_t[10])\n",
    "i = 0\n",
    "for x in r_t:\n",
    "    if(x==0.5):\n",
    "        r_t[i]=2.0\n",
    "    i = i+1\n",
    "i = 0\n",
    "for x in r_h:\n",
    "    if(x==0.5):\n",
    "        r_h[i]=2.0\n",
    "    i = i+1\n",
    "r_t = np.reshape(r_t,(len(r_t),1))\n",
    "r_h = np.reshape(r_h,(len(r_h),1))\n",
    "tt_pheno = r_t\n",
    "ho_pheno = r_h\n",
    "print(tt_pheno.shape)\n",
    "print(ho_pheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 623207)\n",
      "(155, 214899)\n",
      "(155, 623207)\n"
     ]
    }
   ],
   "source": [
    "ohe = pickle.load(open(\"PoC_ohe.dat\", \"rb\"))\n",
    "tt_vcf = ohe.transform(tt_vcf)\n",
    "print(tt_vcf.shape)\n",
    "print(ho_vcf.shape)\n",
    "ho_vcf = ohe.transform(ho_vcf)\n",
    "print(ho_vcf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##how to mlb both tt and ho for same scheme? do i even need to?\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb = mlb.fit(tt_pheno)\n",
    "##print(tt_pheno.shape)\n",
    "#print(ho_pheno.shape)\n",
    "#tt_pheno = mlb.transform(tt_pheno)\n",
    "#print(tt_pheno.shape)\n",
    "#ho_pheno = mlb.transform(ho_pheno)\n",
    "#print(ho_pheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My own DNN model based upon paper\n",
    "#del model #incase its stored a previous model\n",
    "#del history #for redoing shit\n",
    "\n",
    "#do batch size as 64\n",
    "#reduce the inputs by half when you read it in\n",
    "#add XGboost and RF to the one notebook\n",
    "def build_DNN_model(x_len):\n",
    "    model = Sequential()\n",
    "\n",
    "    #add first input layer, with no normalization\n",
    "    model.add(Dense(192, input_dim = x_len))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.03))\n",
    "    \n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.02))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation('relu'))\n",
    "    #add output layer\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    opt = tf.keras.optimizers.Adamax(learning_rate=0.003)#, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dnn(x,y,k,mlb):\n",
    "    cv = StratifiedKFold(n_splits=k,shuffle=False)\n",
    "    best_model = []\n",
    "    results = []\n",
    "    highest = 0\n",
    "    i = 1\n",
    "    for train,test in cv.split(x,y):\n",
    "        print(y.shape)\n",
    "        print(y[train])\n",
    "        if(i==1):\n",
    "            y = mlb.transform(y)\n",
    "            print(y.shape)\n",
    "            print(y[train])\n",
    "        model = build_DNN_model(x[train].shape[1])\n",
    "        bs = ((x[train].shape[0])/20)\n",
    "        bs = round(bs)\n",
    "        history = model.fit(x[train], y[train], validation_data=(x[test], y[test]), epochs=100, batch_size=bs)\n",
    "        _, accuracy = model.evaluate(x[test], y[test], batch_size=bs, verbose=0)\n",
    "        accuracy = accuracy *100\n",
    "        print(\"accuracy for model \" + str(i) + \" is \" + str(accuracy))\n",
    "        if(accuracy > highest):\n",
    "            highest = accuracy\n",
    "            best_model = model\n",
    "        results.append(accuracy)\n",
    "        del model\n",
    "        i = i + 1\n",
    "    print(\"Training Testing Accuracy: %.2f%% (%.2f%%)\" % (np.mean(results), np.std(results))) \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 1)\n",
      "[[1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]]\n",
      "(617, 3)\n",
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 192)               119655936 \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 119,691,683\n",
      "Trainable params: 119,691,619\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 15s 27ms/sample - loss: 0.9935 - accuracy: 0.4982 - val_loss: 9.3928 - val_accuracy: 0.2381\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7916 - accuracy: 0.6715 - val_loss: 5.3864 - val_accuracy: 0.2381\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 2s 4ms/sample - loss: 0.7022 - accuracy: 0.7166 - val_loss: 3.1928 - val_accuracy: 0.2857\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7451 - accuracy: 0.6751 - val_loss: 2.3713 - val_accuracy: 0.3016\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7027 - accuracy: 0.7148 - val_loss: 1.9524 - val_accuracy: 0.3651\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6358 - accuracy: 0.7184 - val_loss: 1.6254 - val_accuracy: 0.4127\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6461 - accuracy: 0.7058 - val_loss: 2.8047 - val_accuracy: 0.3016\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.5697 - accuracy: 0.7455 - val_loss: 1.0271 - val_accuracy: 0.6349\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.5527 - accuracy: 0.7581 - val_loss: 0.9965 - val_accuracy: 0.5556\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 2s 4ms/sample - loss: 0.4797 - accuracy: 0.7996 - val_loss: 0.9048 - val_accuracy: 0.6190\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 2s 4ms/sample - loss: 0.4794 - accuracy: 0.8032 - val_loss: 0.8312 - val_accuracy: 0.6190\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3767 - accuracy: 0.8646 - val_loss: 0.6650 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3463 - accuracy: 0.8700 - val_loss: 0.4840 - val_accuracy: 0.8413\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3391 - accuracy: 0.8682 - val_loss: 0.4712 - val_accuracy: 0.7937\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 2s 4ms/sample - loss: 0.2680 - accuracy: 0.9170 - val_loss: 0.7048 - val_accuracy: 0.7460\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 2s 4ms/sample - loss: 0.2416 - accuracy: 0.8989 - val_loss: 0.4621 - val_accuracy: 0.8889\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.2029 - accuracy: 0.9332 - val_loss: 0.4503 - val_accuracy: 0.8571\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1802 - accuracy: 0.9404 - val_loss: 0.5786 - val_accuracy: 0.7778\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1590 - accuracy: 0.9458 - val_loss: 0.4243 - val_accuracy: 0.8730\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1334 - accuracy: 0.9513 - val_loss: 0.5417 - val_accuracy: 0.8254\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0961 - accuracy: 0.9675 - val_loss: 0.8145 - val_accuracy: 0.6984\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 2s 4ms/sample - loss: 0.0987 - accuracy: 0.9603 - val_loss: 0.6372 - val_accuracy: 0.7778\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0842 - accuracy: 0.9729 - val_loss: 0.7499 - val_accuracy: 0.7619\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0866 - accuracy: 0.9765 - val_loss: 0.5754 - val_accuracy: 0.7937\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0624 - accuracy: 0.9801 - val_loss: 0.5463 - val_accuracy: 0.7937\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0530 - accuracy: 0.9856 - val_loss: 0.5960 - val_accuracy: 0.8095\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0765 - accuracy: 0.9729 - val_loss: 0.6094 - val_accuracy: 0.8571\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0581 - accuracy: 0.9783 - val_loss: 0.8860 - val_accuracy: 0.7302\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0777 - accuracy: 0.9711 - val_loss: 0.7992 - val_accuracy: 0.7937\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0599 - accuracy: 0.9765 - val_loss: 0.7214 - val_accuracy: 0.8095\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0482 - accuracy: 0.9856 - val_loss: 0.5454 - val_accuracy: 0.8730\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0308 - accuracy: 0.9892 - val_loss: 0.5623 - val_accuracy: 0.8413\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0287 - accuracy: 0.9928 - val_loss: 0.4301 - val_accuracy: 0.8730\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0237 - accuracy: 0.9946 - val_loss: 0.6951 - val_accuracy: 0.7937\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.8082 - val_accuracy: 0.8095\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0179 - accuracy: 0.9910 - val_loss: 0.6769 - val_accuracy: 0.8730\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0132 - accuracy: 0.9982 - val_loss: 0.6215 - val_accuracy: 0.8571\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0141 - accuracy: 0.9982 - val_loss: 0.9595 - val_accuracy: 0.7778\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0301 - accuracy: 0.9892 - val_loss: 1.5140 - val_accuracy: 0.7143\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0330 - accuracy: 0.9874 - val_loss: 2.4334 - val_accuracy: 0.6508\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0377 - accuracy: 0.9928 - val_loss: 0.6381 - val_accuracy: 0.8571\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0380 - accuracy: 0.9892 - val_loss: 0.6334 - val_accuracy: 0.8254\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0177 - accuracy: 0.9946 - val_loss: 0.6742 - val_accuracy: 0.8413\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.8058 - val_accuracy: 0.8413\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0165 - accuracy: 0.9964 - val_loss: 0.7643 - val_accuracy: 0.8730\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0130 - accuracy: 0.9946 - val_loss: 0.8232 - val_accuracy: 0.8095\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.7715 - val_accuracy: 0.8413\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0128 - accuracy: 0.9946 - val_loss: 0.7112 - val_accuracy: 0.8571\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0254 - accuracy: 0.9946 - val_loss: 0.6784 - val_accuracy: 0.8413\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0488 - accuracy: 0.9856 - val_loss: 1.9436 - val_accuracy: 0.6984\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0207 - accuracy: 0.9946 - val_loss: 1.3269 - val_accuracy: 0.6349\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0409 - accuracy: 0.9856 - val_loss: 1.1859 - val_accuracy: 0.8095\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0120 - accuracy: 0.9964 - val_loss: 0.9685 - val_accuracy: 0.8095\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.7220 - val_accuracy: 0.8730\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0117 - accuracy: 0.9964 - val_loss: 0.6227 - val_accuracy: 0.8571\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0152 - accuracy: 0.9928 - val_loss: 1.4431 - val_accuracy: 0.6032\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0104 - accuracy: 0.9982 - val_loss: 0.9520 - val_accuracy: 0.8413\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0091 - accuracy: 0.9982 - val_loss: 1.0248 - val_accuracy: 0.7460\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0108 - accuracy: 0.9964 - val_loss: 1.4119 - val_accuracy: 0.8095\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0125 - accuracy: 0.9964 - val_loss: 1.0691 - val_accuracy: 0.7937\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0087 - accuracy: 0.9982 - val_loss: 1.0223 - val_accuracy: 0.7460\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0141 - accuracy: 0.9928 - val_loss: 2.5544 - val_accuracy: 0.6508\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0173 - accuracy: 0.9946 - val_loss: 0.9425 - val_accuracy: 0.8095\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.8295 - val_accuracy: 0.8254\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0097 - accuracy: 0.9964 - val_loss: 1.5220 - val_accuracy: 0.7619\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0089 - accuracy: 0.9982 - val_loss: 1.1659 - val_accuracy: 0.8413\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0060 - accuracy: 0.9982 - val_loss: 1.0168 - val_accuracy: 0.8413\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0107 - accuracy: 0.9946 - val_loss: 1.1153 - val_accuracy: 0.8413\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.6180 - val_accuracy: 0.8730\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0127 - accuracy: 0.9964 - val_loss: 0.7796 - val_accuracy: 0.8254\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0164 - accuracy: 0.9928 - val_loss: 0.7426 - val_accuracy: 0.8254\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.7392 - val_accuracy: 0.8254\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0089 - accuracy: 0.9964 - val_loss: 1.0393 - val_accuracy: 0.7619\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0101 - accuracy: 0.9982 - val_loss: 1.0124 - val_accuracy: 0.8413\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.0840 - val_accuracy: 0.8254\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.7645 - val_accuracy: 0.8730\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.9480 - val_accuracy: 0.8095\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0115 - accuracy: 0.9946 - val_loss: 1.5986 - val_accuracy: 0.6190\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5877 - val_accuracy: 0.8571\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.7091 - val_accuracy: 0.8254\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.8069 - val_accuracy: 0.7778\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0069 - accuracy: 0.9964 - val_loss: 1.2443 - val_accuracy: 0.6984\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.8905 - val_accuracy: 0.8254\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0114 - accuracy: 0.9946 - val_loss: 3.2812 - val_accuracy: 0.4921\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0480 - accuracy: 0.9838 - val_loss: 1.3505 - val_accuracy: 0.6825\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0255 - accuracy: 0.9928 - val_loss: 1.2196 - val_accuracy: 0.8095\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.5266 - val_accuracy: 0.7937\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0249 - accuracy: 0.9928 - val_loss: 1.0270 - val_accuracy: 0.7460\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.8745 - val_accuracy: 0.7778\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.8104 - val_accuracy: 0.7937\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6708 - val_accuracy: 0.8095\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6349 - val_accuracy: 0.8730\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.6543 - val_accuracy: 0.8730\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.8489 - val_accuracy: 0.8254\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.7156 - val_accuracy: 0.8730\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0076 - accuracy: 0.9982 - val_loss: 0.6472 - val_accuracy: 0.8571\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0116 - accuracy: 0.9964 - val_loss: 3.4230 - val_accuracy: 0.4127\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0356 - accuracy: 0.9838 - val_loss: 1.3631 - val_accuracy: 0.6190\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0239 - accuracy: 0.9910 - val_loss: 2.3095 - val_accuracy: 0.6984\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0103 - accuracy: 0.9964 - val_loss: 0.7331 - val_accuracy: 0.8413\n",
      "accuracy for model 1 is 84.1269850730896\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 192)               119655936 \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 119,691,683\n",
      "Trainable params: 119,691,619\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 16s 29ms/sample - loss: 0.8857 - accuracy: 0.6209 - val_loss: 11.2637 - val_accuracy: 0.5873\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7305 - accuracy: 0.7058 - val_loss: 5.9599 - val_accuracy: 0.5873\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6812 - accuracy: 0.7022 - val_loss: 3.6585 - val_accuracy: 0.5873\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6403 - accuracy: 0.7094 - val_loss: 2.1988 - val_accuracy: 0.5873\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.5901 - accuracy: 0.7274 - val_loss: 1.5895 - val_accuracy: 0.7302\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.4926 - accuracy: 0.7708 - val_loss: 0.7872 - val_accuracy: 0.7460\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.4327 - accuracy: 0.8285 - val_loss: 0.6174 - val_accuracy: 0.7460\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3732 - accuracy: 0.8682 - val_loss: 1.9836 - val_accuracy: 0.7302\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3034 - accuracy: 0.8989 - val_loss: 0.7157 - val_accuracy: 0.7302\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.2402 - accuracy: 0.9278 - val_loss: 0.5704 - val_accuracy: 0.7460\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1775 - accuracy: 0.9585 - val_loss: 0.6099 - val_accuracy: 0.7460\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1168 - accuracy: 0.9819 - val_loss: 0.7477 - val_accuracy: 0.7302\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0920 - accuracy: 0.9765 - val_loss: 0.6370 - val_accuracy: 0.7460\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0965 - accuracy: 0.9693 - val_loss: 0.6791 - val_accuracy: 0.7619\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0532 - accuracy: 0.9874 - val_loss: 0.6264 - val_accuracy: 0.7619\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0549 - accuracy: 0.9874 - val_loss: 0.9137 - val_accuracy: 0.6508\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0415 - accuracy: 0.9928 - val_loss: 0.9850 - val_accuracy: 0.6825\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0569 - accuracy: 0.9856 - val_loss: 1.2743 - val_accuracy: 0.7302\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0410 - accuracy: 0.9946 - val_loss: 0.6817 - val_accuracy: 0.7937\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0503 - accuracy: 0.9801 - val_loss: 0.6791 - val_accuracy: 0.7302\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0328 - accuracy: 0.9946 - val_loss: 0.5865 - val_accuracy: 0.7778\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0210 - accuracy: 0.9982 - val_loss: 0.8189 - val_accuracy: 0.7778\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0243 - accuracy: 0.9964 - val_loss: 0.9479 - val_accuracy: 0.7619\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0386 - accuracy: 0.9892 - val_loss: 1.8021 - val_accuracy: 0.6825\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0470 - accuracy: 0.9838 - val_loss: 1.2491 - val_accuracy: 0.7460\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0263 - accuracy: 0.9928 - val_loss: 1.2735 - val_accuracy: 0.7619\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0158 - accuracy: 0.9982 - val_loss: 0.9004 - val_accuracy: 0.7619\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0151 - accuracy: 0.9946 - val_loss: 1.0704 - val_accuracy: 0.7778\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0213 - accuracy: 0.9946 - val_loss: 1.3220 - val_accuracy: 0.7619\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 1.3819 - val_accuracy: 0.7619\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0125 - accuracy: 0.9982 - val_loss: 1.3135 - val_accuracy: 0.7619\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0121 - accuracy: 0.9964 - val_loss: 0.9584 - val_accuracy: 0.8095\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0114 - accuracy: 0.9982 - val_loss: 1.1076 - val_accuracy: 0.7778\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0096 - accuracy: 0.9982 - val_loss: 1.0288 - val_accuracy: 0.7619\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0174 - accuracy: 0.9946 - val_loss: 0.9144 - val_accuracy: 0.7778\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0126 - accuracy: 0.9964 - val_loss: 0.9184 - val_accuracy: 0.8095\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0210 - accuracy: 0.9910 - val_loss: 0.9605 - val_accuracy: 0.7619\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0075 - accuracy: 0.9982 - val_loss: 1.4530 - val_accuracy: 0.7778\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0173 - accuracy: 0.9946 - val_loss: 1.1085 - val_accuracy: 0.7619\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0152 - accuracy: 0.9964 - val_loss: 1.1959 - val_accuracy: 0.7619\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0172 - accuracy: 0.9928 - val_loss: 0.9064 - val_accuracy: 0.7778\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0123 - accuracy: 0.9946 - val_loss: 1.3911 - val_accuracy: 0.7937\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0131 - accuracy: 0.9964 - val_loss: 1.0310 - val_accuracy: 0.7778\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0186 - accuracy: 0.9964 - val_loss: 0.9168 - val_accuracy: 0.7937\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0226 - accuracy: 0.9928 - val_loss: 0.9779 - val_accuracy: 0.7778\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0080 - accuracy: 0.9982 - val_loss: 0.9006 - val_accuracy: 0.7778\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.9381 - val_accuracy: 0.7778\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.3162 - val_accuracy: 0.7619\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0075 - accuracy: 0.9982 - val_loss: 0.8382 - val_accuracy: 0.7937\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.8668 - val_accuracy: 0.7937\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 2s 4ms/sample - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.9409 - val_accuracy: 0.7778\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.9948 - val_accuracy: 0.8095\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.0715 - val_accuracy: 0.8095\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.2211 - val_accuracy: 0.7778\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2597 - val_accuracy: 0.7778\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.3885 - val_accuracy: 0.7460\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.4261 - val_accuracy: 0.7460\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.2640 - val_accuracy: 0.7778\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 0.9982 - val_loss: 1.2822 - val_accuracy: 0.7778\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.1815 - val_accuracy: 0.7778\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.3607 - val_accuracy: 0.7460\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0220 - accuracy: 0.9910 - val_loss: 1.1311 - val_accuracy: 0.7778\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0088 - accuracy: 0.9964 - val_loss: 1.2276 - val_accuracy: 0.7778\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0040 - accuracy: 0.9982 - val_loss: 1.0842 - val_accuracy: 0.7619\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0124 - accuracy: 0.9964 - val_loss: 1.2869 - val_accuracy: 0.7619\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0150 - accuracy: 0.9964 - val_loss: 1.0029 - val_accuracy: 0.8254\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0060 - accuracy: 0.9982 - val_loss: 1.3882 - val_accuracy: 0.7460\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.3676 - val_accuracy: 0.7619\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0054 - accuracy: 0.9982 - val_loss: 1.7149 - val_accuracy: 0.7778\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0072 - accuracy: 0.9982 - val_loss: 1.2482 - val_accuracy: 0.7937\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0071 - accuracy: 0.9964 - val_loss: 1.4225 - val_accuracy: 0.7778\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0144 - accuracy: 0.9964 - val_loss: 1.0984 - val_accuracy: 0.7778\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0231 - accuracy: 0.9928 - val_loss: 1.6790 - val_accuracy: 0.7460\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0218 - accuracy: 0.9946 - val_loss: 1.7837 - val_accuracy: 0.7619\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0056 - accuracy: 0.9982 - val_loss: 1.3056 - val_accuracy: 0.7778\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.9861 - val_accuracy: 0.7778\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0107 - accuracy: 0.9982 - val_loss: 1.3286 - val_accuracy: 0.7302\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0061 - accuracy: 0.9964 - val_loss: 1.2799 - val_accuracy: 0.7778\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3477 - val_accuracy: 0.7619\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0158 - accuracy: 0.9946 - val_loss: 0.9538 - val_accuracy: 0.7937\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0266 - accuracy: 0.9892 - val_loss: 1.0158 - val_accuracy: 0.8095\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0183 - accuracy: 0.9928 - val_loss: 1.0624 - val_accuracy: 0.7619\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.5904 - val_accuracy: 0.7460\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0099 - accuracy: 0.9964 - val_loss: 1.4799 - val_accuracy: 0.7778\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0127 - accuracy: 0.9964 - val_loss: 1.2685 - val_accuracy: 0.8095\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0125 - accuracy: 0.9982 - val_loss: 2.1017 - val_accuracy: 0.7778\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0162 - accuracy: 0.9910 - val_loss: 1.7113 - val_accuracy: 0.7460\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.4498 - val_accuracy: 0.7937\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.3563 - val_accuracy: 0.7937\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0033 - accuracy: 0.9982 - val_loss: 1.4414 - val_accuracy: 0.7937\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0060 - accuracy: 0.9982 - val_loss: 1.2622 - val_accuracy: 0.7619\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0097 - accuracy: 0.9946 - val_loss: 1.4567 - val_accuracy: 0.7619\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.8265 - val_accuracy: 0.7778\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.2588 - val_accuracy: 0.7778\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 8.5088e-04 - accuracy: 1.0000 - val_loss: 1.2984 - val_accuracy: 0.7778\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.6073 - val_accuracy: 0.7460\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0423 - accuracy: 0.9892 - val_loss: 1.2040 - val_accuracy: 0.7778\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0299 - accuracy: 0.9856 - val_loss: 1.2775 - val_accuracy: 0.7302\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0144 - accuracy: 0.9946 - val_loss: 1.7937 - val_accuracy: 0.7460\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0128 - accuracy: 0.9964 - val_loss: 1.2432 - val_accuracy: 0.7937\n",
      "accuracy for model 2 is 79.36508059501648\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 192)               119655936 \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 119,691,683\n",
      "Trainable params: 119,691,619\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 7s 12ms/sample - loss: 1.0152 - accuracy: 0.5126 - val_loss: 5.2877 - val_accuracy: 0.5873\n",
      "Epoch 2/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.8194 - accuracy: 0.6552 - val_loss: 3.4161 - val_accuracy: 0.5873\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7464 - accuracy: 0.7004 - val_loss: 1.7821 - val_accuracy: 0.6984\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7150 - accuracy: 0.6931 - val_loss: 1.5666 - val_accuracy: 0.6984\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6992 - accuracy: 0.7004 - val_loss: 1.8553 - val_accuracy: 0.7619\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6334 - accuracy: 0.7058 - val_loss: 1.4238 - val_accuracy: 0.7619\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6130 - accuracy: 0.7184 - val_loss: 0.8500 - val_accuracy: 0.7619\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.5038 - accuracy: 0.7834 - val_loss: 0.5761 - val_accuracy: 0.7619\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.4247 - accuracy: 0.8466 - val_loss: 0.3929 - val_accuracy: 0.8413\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3744 - accuracy: 0.8592 - val_loss: 0.5060 - val_accuracy: 0.7778\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3101 - accuracy: 0.8646 - val_loss: 0.3399 - val_accuracy: 0.8889\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.2563 - accuracy: 0.9206 - val_loss: 0.3528 - val_accuracy: 0.8413\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.2358 - accuracy: 0.9152 - val_loss: 0.4796 - val_accuracy: 0.7937\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1556 - accuracy: 0.9531 - val_loss: 0.3127 - val_accuracy: 0.8889\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1179 - accuracy: 0.9693 - val_loss: 0.3066 - val_accuracy: 0.8730\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1287 - accuracy: 0.9621 - val_loss: 0.2961 - val_accuracy: 0.8889\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1236 - accuracy: 0.9585 - val_loss: 0.4304 - val_accuracy: 0.8889\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0893 - accuracy: 0.9801 - val_loss: 0.3514 - val_accuracy: 0.8730\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0862 - accuracy: 0.9675 - val_loss: 0.4149 - val_accuracy: 0.8730\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0660 - accuracy: 0.9747 - val_loss: 0.4178 - val_accuracy: 0.8730\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0532 - accuracy: 0.9856 - val_loss: 0.3790 - val_accuracy: 0.9048\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0776 - accuracy: 0.9765 - val_loss: 0.4195 - val_accuracy: 0.8889\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0445 - accuracy: 0.9856 - val_loss: 0.4086 - val_accuracy: 0.8889\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0486 - accuracy: 0.9856 - val_loss: 0.4058 - val_accuracy: 0.8889\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0386 - accuracy: 0.9928 - val_loss: 0.4472 - val_accuracy: 0.8413\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0373 - accuracy: 0.9910 - val_loss: 0.4151 - val_accuracy: 0.8730\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0374 - accuracy: 0.9910 - val_loss: 0.3913 - val_accuracy: 0.8730\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0197 - accuracy: 0.9982 - val_loss: 0.5565 - val_accuracy: 0.8413\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0513 - accuracy: 0.9892 - val_loss: 0.3633 - val_accuracy: 0.8571\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0276 - accuracy: 0.9928 - val_loss: 0.4080 - val_accuracy: 0.8730\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0277 - accuracy: 0.9964 - val_loss: 0.4774 - val_accuracy: 0.8413\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0399 - accuracy: 0.9838 - val_loss: 0.6165 - val_accuracy: 0.8571\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0408 - accuracy: 0.9874 - val_loss: 0.7772 - val_accuracy: 0.7778\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0342 - accuracy: 0.9874 - val_loss: 0.4850 - val_accuracy: 0.8730\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0432 - accuracy: 0.9874 - val_loss: 0.5024 - val_accuracy: 0.8571\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0256 - accuracy: 0.9964 - val_loss: 0.5227 - val_accuracy: 0.8571\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0204 - accuracy: 0.9946 - val_loss: 0.4866 - val_accuracy: 0.8730\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0170 - accuracy: 0.9964 - val_loss: 0.5017 - val_accuracy: 0.8571\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0227 - accuracy: 0.9928 - val_loss: 0.5278 - val_accuracy: 0.8571\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0120 - accuracy: 0.9982 - val_loss: 0.6615 - val_accuracy: 0.8254\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0179 - accuracy: 0.9982 - val_loss: 0.4706 - val_accuracy: 0.8730\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0302 - accuracy: 0.9892 - val_loss: 0.5628 - val_accuracy: 0.8571\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0147 - accuracy: 0.9964 - val_loss: 0.5850 - val_accuracy: 0.8571\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0140 - accuracy: 0.9964 - val_loss: 0.5591 - val_accuracy: 0.8571\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0202 - accuracy: 0.9928 - val_loss: 0.5312 - val_accuracy: 0.8730\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0162 - accuracy: 0.9946 - val_loss: 0.5777 - val_accuracy: 0.8571\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.5745 - val_accuracy: 0.8730\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0331 - accuracy: 0.9928 - val_loss: 0.5902 - val_accuracy: 0.8571\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0141 - accuracy: 0.9946 - val_loss: 0.5510 - val_accuracy: 0.8730\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0219 - accuracy: 0.9946 - val_loss: 0.6391 - val_accuracy: 0.8571\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0113 - accuracy: 0.9946 - val_loss: 0.5602 - val_accuracy: 0.8889\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0109 - accuracy: 0.9982 - val_loss: 0.5555 - val_accuracy: 0.8889\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0266 - accuracy: 0.9910 - val_loss: 0.8350 - val_accuracy: 0.7937\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0148 - accuracy: 0.9946 - val_loss: 0.5804 - val_accuracy: 0.8889\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0113 - accuracy: 0.9982 - val_loss: 0.6066 - val_accuracy: 0.8730\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0286 - accuracy: 0.9928 - val_loss: 0.6747 - val_accuracy: 0.8413\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0191 - accuracy: 0.9964 - val_loss: 0.6328 - val_accuracy: 0.8730\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0152 - accuracy: 0.9946 - val_loss: 0.6516 - val_accuracy: 0.8889\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0214 - accuracy: 0.9946 - val_loss: 0.6288 - val_accuracy: 0.8571\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.6013 - val_accuracy: 0.8730\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0128 - accuracy: 0.9964 - val_loss: 0.6594 - val_accuracy: 0.8571\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0162 - accuracy: 0.9928 - val_loss: 0.5701 - val_accuracy: 0.8730\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 0.9964 - val_loss: 0.6097 - val_accuracy: 0.8730\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0204 - accuracy: 0.9946 - val_loss: 0.5009 - val_accuracy: 0.8571\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.6681 - val_accuracy: 0.8413\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.6526 - val_accuracy: 0.8730\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0120 - accuracy: 0.9982 - val_loss: 0.6526 - val_accuracy: 0.8571\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0067 - accuracy: 0.9964 - val_loss: 0.6652 - val_accuracy: 0.8571\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0106 - accuracy: 0.9946 - val_loss: 0.6897 - val_accuracy: 0.8730\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0123 - accuracy: 0.9946 - val_loss: 0.6179 - val_accuracy: 0.8571\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0089 - accuracy: 0.9964 - val_loss: 0.6362 - val_accuracy: 0.8730\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0085 - accuracy: 0.9964 - val_loss: 0.6586 - val_accuracy: 0.8571\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.6450 - val_accuracy: 0.8730\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0161 - accuracy: 0.9964 - val_loss: 0.5297 - val_accuracy: 0.8571\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.6316 - val_accuracy: 0.8889\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0099 - accuracy: 0.9964 - val_loss: 0.8059 - val_accuracy: 0.8571\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0063 - accuracy: 0.9964 - val_loss: 0.6283 - val_accuracy: 0.8730\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 0.8413\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0175 - accuracy: 0.9982 - val_loss: 0.5928 - val_accuracy: 0.8889\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0323 - accuracy: 0.9946 - val_loss: 0.6708 - val_accuracy: 0.8571\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5976 - val_accuracy: 0.8730\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0116 - accuracy: 0.9982 - val_loss: 0.6939 - val_accuracy: 0.8413\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0093 - accuracy: 0.9964 - val_loss: 0.6786 - val_accuracy: 0.8413\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0069 - accuracy: 0.9982 - val_loss: 0.7058 - val_accuracy: 0.8413\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0160 - accuracy: 0.9910 - val_loss: 0.7273 - val_accuracy: 0.8413\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0203 - accuracy: 0.9892 - val_loss: 0.7031 - val_accuracy: 0.8889\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.9336 - val_accuracy: 0.8095\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0116 - accuracy: 0.9946 - val_loss: 0.5745 - val_accuracy: 0.8730\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0070 - accuracy: 0.9982 - val_loss: 1.0132 - val_accuracy: 0.7302\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0313 - accuracy: 0.9874 - val_loss: 1.1154 - val_accuracy: 0.7143\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0111 - accuracy: 0.9964 - val_loss: 0.6220 - val_accuracy: 0.8254\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0107 - accuracy: 0.9982 - val_loss: 0.6583 - val_accuracy: 0.8413\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.6999 - val_accuracy: 0.8571\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.7324 - val_accuracy: 0.8571\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.7222 - val_accuracy: 0.8571\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0230 - accuracy: 0.9892 - val_loss: 0.6566 - val_accuracy: 0.8571\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.7017 - val_accuracy: 0.8571\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.6825 - val_accuracy: 0.8730\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0127 - accuracy: 0.9982 - val_loss: 0.7609 - val_accuracy: 0.8571\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0132 - accuracy: 0.9946 - val_loss: 0.9244 - val_accuracy: 0.8254\n",
      "accuracy for model 3 is 82.53968358039856\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 192)               119655936 \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 119,691,683\n",
      "Trainable params: 119,691,619\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 554 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "554/554 [==============================] - 7s 13ms/sample - loss: 1.1601 - accuracy: 0.3971 - val_loss: 5.0819 - val_accuracy: 0.5873\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.8861 - accuracy: 0.5903 - val_loss: 4.0881 - val_accuracy: 0.5873\n",
      "Epoch 3/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.8255 - accuracy: 0.6318 - val_loss: 2.7103 - val_accuracy: 0.5873\n",
      "Epoch 4/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7587 - accuracy: 0.6643 - val_loss: 2.0514 - val_accuracy: 0.6032\n",
      "Epoch 5/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.7126 - accuracy: 0.7004 - val_loss: 1.2836 - val_accuracy: 0.6032\n",
      "Epoch 6/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6554 - accuracy: 0.7365 - val_loss: 0.8246 - val_accuracy: 0.7302\n",
      "Epoch 7/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.6229 - accuracy: 0.7473 - val_loss: 0.7151 - val_accuracy: 0.7460\n",
      "Epoch 8/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.5779 - accuracy: 0.7690 - val_loss: 0.6556 - val_accuracy: 0.7460\n",
      "Epoch 9/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.4954 - accuracy: 0.8231 - val_loss: 0.5481 - val_accuracy: 0.8095\n",
      "Epoch 10/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.4473 - accuracy: 0.8339 - val_loss: 0.6232 - val_accuracy: 0.7619\n",
      "Epoch 11/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.3429 - accuracy: 0.8917 - val_loss: 0.7234 - val_accuracy: 0.7460\n",
      "Epoch 12/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.2602 - accuracy: 0.9260 - val_loss: 0.7198 - val_accuracy: 0.7937\n",
      "Epoch 13/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.2175 - accuracy: 0.9332 - val_loss: 1.1669 - val_accuracy: 0.5238\n",
      "Epoch 14/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1918 - accuracy: 0.9440 - val_loss: 0.6860 - val_accuracy: 0.7778\n",
      "Epoch 15/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1515 - accuracy: 0.9603 - val_loss: 1.6743 - val_accuracy: 0.3968\n",
      "Epoch 16/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1039 - accuracy: 0.9711 - val_loss: 1.1515 - val_accuracy: 0.5238\n",
      "Epoch 17/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.1130 - accuracy: 0.9711 - val_loss: 0.5470 - val_accuracy: 0.7778\n",
      "Epoch 18/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0795 - accuracy: 0.9838 - val_loss: 0.8273 - val_accuracy: 0.6508\n",
      "Epoch 19/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0612 - accuracy: 0.9838 - val_loss: 0.6555 - val_accuracy: 0.8413\n",
      "Epoch 20/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0541 - accuracy: 0.9856 - val_loss: 0.6696 - val_accuracy: 0.7619\n",
      "Epoch 21/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0462 - accuracy: 0.9892 - val_loss: 0.6351 - val_accuracy: 0.7937\n",
      "Epoch 22/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0382 - accuracy: 0.9892 - val_loss: 0.7031 - val_accuracy: 0.8095\n",
      "Epoch 23/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0381 - accuracy: 0.9892 - val_loss: 0.7298 - val_accuracy: 0.7937\n",
      "Epoch 24/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0263 - accuracy: 0.9910 - val_loss: 1.2581 - val_accuracy: 0.7460\n",
      "Epoch 25/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0403 - accuracy: 0.9856 - val_loss: 1.1453 - val_accuracy: 0.7778\n",
      "Epoch 26/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0380 - accuracy: 0.9892 - val_loss: 1.3580 - val_accuracy: 0.7937\n",
      "Epoch 27/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0331 - accuracy: 0.9928 - val_loss: 1.3505 - val_accuracy: 0.6032\n",
      "Epoch 28/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0280 - accuracy: 0.9946 - val_loss: 0.8028 - val_accuracy: 0.7778\n",
      "Epoch 29/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0184 - accuracy: 0.9982 - val_loss: 1.0877 - val_accuracy: 0.7937\n",
      "Epoch 30/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0103 - accuracy: 1.0000 - val_loss: 1.0975 - val_accuracy: 0.8095\n",
      "Epoch 31/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0188 - accuracy: 0.9964 - val_loss: 0.9753 - val_accuracy: 0.7937\n",
      "Epoch 32/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0206 - accuracy: 0.9910 - val_loss: 0.8294 - val_accuracy: 0.8095\n",
      "Epoch 33/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0098 - accuracy: 0.9964 - val_loss: 0.9014 - val_accuracy: 0.7302\n",
      "Epoch 34/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0131 - accuracy: 0.9964 - val_loss: 1.0975 - val_accuracy: 0.8095\n",
      "Epoch 35/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0124 - accuracy: 0.9982 - val_loss: 0.8245 - val_accuracy: 0.7937\n",
      "Epoch 36/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 1.1709 - val_accuracy: 0.8095\n",
      "Epoch 37/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0111 - accuracy: 0.9946 - val_loss: 0.6795 - val_accuracy: 0.8413\n",
      "Epoch 38/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.2634 - val_accuracy: 0.7937\n",
      "Epoch 39/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.9587 - val_accuracy: 0.8095\n",
      "Epoch 40/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0079 - accuracy: 0.9982 - val_loss: 1.0768 - val_accuracy: 0.8254\n",
      "Epoch 41/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.1117 - val_accuracy: 0.8254\n",
      "Epoch 42/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0100 - accuracy: 0.9964 - val_loss: 0.8353 - val_accuracy: 0.7937\n",
      "Epoch 43/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0176 - accuracy: 0.9928 - val_loss: 0.7698 - val_accuracy: 0.7937\n",
      "Epoch 44/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0143 - accuracy: 0.9946 - val_loss: 0.7240 - val_accuracy: 0.8254\n",
      "Epoch 45/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0075 - accuracy: 0.9964 - val_loss: 1.1795 - val_accuracy: 0.7778\n",
      "Epoch 46/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.7801 - val_accuracy: 0.7937\n",
      "Epoch 47/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0223 - accuracy: 0.9946 - val_loss: 1.0425 - val_accuracy: 0.8254\n",
      "Epoch 48/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0199 - accuracy: 0.9928 - val_loss: 0.9718 - val_accuracy: 0.7302\n",
      "Epoch 49/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0214 - accuracy: 0.9928 - val_loss: 0.8358 - val_accuracy: 0.8095\n",
      "Epoch 50/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0098 - accuracy: 0.9964 - val_loss: 0.7712 - val_accuracy: 0.8095\n",
      "Epoch 51/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.8770 - val_accuracy: 0.8095\n",
      "Epoch 52/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0061 - accuracy: 0.9982 - val_loss: 1.1221 - val_accuracy: 0.8413\n",
      "Epoch 53/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0059 - accuracy: 0.9982 - val_loss: 1.5230 - val_accuracy: 0.7937\n",
      "Epoch 54/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0086 - accuracy: 0.9982 - val_loss: 1.2194 - val_accuracy: 0.6667\n",
      "Epoch 55/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0086 - accuracy: 0.9982 - val_loss: 1.0795 - val_accuracy: 0.8095\n",
      "Epoch 56/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0051 - accuracy: 0.9982 - val_loss: 1.0664 - val_accuracy: 0.8254\n",
      "Epoch 57/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0117 - accuracy: 0.9946 - val_loss: 1.4261 - val_accuracy: 0.8254\n",
      "Epoch 58/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0075 - accuracy: 0.9982 - val_loss: 1.1248 - val_accuracy: 0.8254\n",
      "Epoch 59/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.7588 - val_accuracy: 0.8095\n",
      "Epoch 60/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0158 - accuracy: 0.9964 - val_loss: 1.5727 - val_accuracy: 0.7937\n",
      "Epoch 61/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0104 - accuracy: 0.9964 - val_loss: 0.8906 - val_accuracy: 0.7937\n",
      "Epoch 62/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0403 - accuracy: 0.9892 - val_loss: 1.2754 - val_accuracy: 0.8095\n",
      "Epoch 63/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0094 - accuracy: 0.9964 - val_loss: 1.2183 - val_accuracy: 0.6825\n",
      "Epoch 64/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.7888 - val_accuracy: 0.7937\n",
      "Epoch 65/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0054 - accuracy: 0.9982 - val_loss: 1.3044 - val_accuracy: 0.8095\n",
      "Epoch 66/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0158 - accuracy: 0.9964 - val_loss: 0.7728 - val_accuracy: 0.8095\n",
      "Epoch 67/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.4719 - val_accuracy: 0.8254\n",
      "Epoch 68/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.6597 - val_accuracy: 0.8095\n",
      "Epoch 69/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.4201 - val_accuracy: 0.8254\n",
      "Epoch 70/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.2699 - val_accuracy: 0.8413\n",
      "Epoch 71/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0085 - accuracy: 0.9982 - val_loss: 0.7102 - val_accuracy: 0.8413\n",
      "Epoch 72/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.8776 - val_accuracy: 0.8254\n",
      "Epoch 73/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0150 - accuracy: 0.9928 - val_loss: 1.0828 - val_accuracy: 0.6667\n",
      "Epoch 74/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0077 - accuracy: 0.9964 - val_loss: 1.6974 - val_accuracy: 0.7778\n",
      "Epoch 75/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0125 - accuracy: 0.9964 - val_loss: 0.8650 - val_accuracy: 0.7619\n",
      "Epoch 76/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0095 - accuracy: 0.9982 - val_loss: 0.8690 - val_accuracy: 0.7302\n",
      "Epoch 77/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.6715 - val_accuracy: 0.8413\n",
      "Epoch 78/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.7112 - val_accuracy: 0.8254\n",
      "Epoch 79/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0379 - accuracy: 0.9910 - val_loss: 1.0733 - val_accuracy: 0.8254\n",
      "Epoch 80/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0296 - accuracy: 0.9892 - val_loss: 0.9384 - val_accuracy: 0.8413\n",
      "Epoch 81/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0117 - accuracy: 0.9946 - val_loss: 0.6482 - val_accuracy: 0.8254\n",
      "Epoch 82/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.7982 - val_accuracy: 0.8254\n",
      "Epoch 83/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0178 - accuracy: 0.9946 - val_loss: 1.6751 - val_accuracy: 0.8095\n",
      "Epoch 84/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0085 - accuracy: 0.9982 - val_loss: 1.9845 - val_accuracy: 0.7937\n",
      "Epoch 85/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0301 - accuracy: 0.9892 - val_loss: 1.3861 - val_accuracy: 0.6984\n",
      "Epoch 86/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0104 - accuracy: 0.9946 - val_loss: 1.8865 - val_accuracy: 0.7937\n",
      "Epoch 87/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.6731 - val_accuracy: 0.8095\n",
      "Epoch 88/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0055 - accuracy: 0.9964 - val_loss: 1.0647 - val_accuracy: 0.8254\n",
      "Epoch 89/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.9688 - val_accuracy: 0.8095\n",
      "Epoch 90/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0057 - accuracy: 0.9982 - val_loss: 1.1949 - val_accuracy: 0.7460\n",
      "Epoch 91/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0063 - accuracy: 0.9982 - val_loss: 1.0762 - val_accuracy: 0.7143\n",
      "Epoch 92/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.9025 - val_accuracy: 0.8095\n",
      "Epoch 93/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0104 - accuracy: 0.9982 - val_loss: 0.8287 - val_accuracy: 0.8254\n",
      "Epoch 94/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 9.5024e-04 - accuracy: 1.0000 - val_loss: 0.8545 - val_accuracy: 0.7937\n",
      "Epoch 95/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0087 - accuracy: 0.9964 - val_loss: 1.8542 - val_accuracy: 0.6190\n",
      "Epoch 96/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0335 - accuracy: 0.9910 - val_loss: 1.0145 - val_accuracy: 0.7937\n",
      "Epoch 97/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 0.9982 - val_loss: 1.4997 - val_accuracy: 0.8095\n",
      "Epoch 98/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0091 - accuracy: 0.9982 - val_loss: 1.4557 - val_accuracy: 0.7937\n",
      "Epoch 99/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0035 - accuracy: 0.9982 - val_loss: 1.1020 - val_accuracy: 0.8254\n",
      "Epoch 100/100\n",
      "554/554 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.6261 - val_accuracy: 0.8254\n",
      "accuracy for model 4 is 82.53968358039856\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 192)               119655936 \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 119,691,683\n",
      "Trainable params: 119,691,619\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 555 samples, validate on 62 samples\n",
      "Epoch 1/100\n",
      "555/555 [==============================] - 13s 23ms/sample - loss: 1.0042 - accuracy: 0.5694 - val_loss: 1.1496 - val_accuracy: 0.6613\n",
      "Epoch 2/100\n",
      "555/555 [==============================] - 2s 4ms/sample - loss: 0.8099 - accuracy: 0.6775 - val_loss: 1.3682 - val_accuracy: 0.6290\n",
      "Epoch 3/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.7354 - accuracy: 0.7009 - val_loss: 1.6920 - val_accuracy: 0.5806\n",
      "Epoch 4/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.6947 - accuracy: 0.7243 - val_loss: 0.8774 - val_accuracy: 0.6935\n",
      "Epoch 5/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.6379 - accuracy: 0.7514 - val_loss: 0.6489 - val_accuracy: 0.7097\n",
      "Epoch 6/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.5660 - accuracy: 0.7748 - val_loss: 0.5922 - val_accuracy: 0.7742\n",
      "Epoch 7/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.4716 - accuracy: 0.8144 - val_loss: 0.7180 - val_accuracy: 0.7419\n",
      "Epoch 8/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.4212 - accuracy: 0.8505 - val_loss: 0.7482 - val_accuracy: 0.7903\n",
      "Epoch 9/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.3552 - accuracy: 0.8811 - val_loss: 0.8849 - val_accuracy: 0.5968\n",
      "Epoch 10/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.2913 - accuracy: 0.8991 - val_loss: 0.8428 - val_accuracy: 0.7097\n",
      "Epoch 11/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.2045 - accuracy: 0.9369 - val_loss: 1.4245 - val_accuracy: 0.3065\n",
      "Epoch 12/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.2250 - accuracy: 0.9387 - val_loss: 1.0596 - val_accuracy: 0.5968\n",
      "Epoch 13/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.1588 - accuracy: 0.9532 - val_loss: 1.7205 - val_accuracy: 0.4839\n",
      "Epoch 14/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.1487 - accuracy: 0.9568 - val_loss: 1.7870 - val_accuracy: 0.4355\n",
      "Epoch 15/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.1630 - accuracy: 0.9459 - val_loss: 0.9492 - val_accuracy: 0.7258\n",
      "Epoch 16/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.1027 - accuracy: 0.9676 - val_loss: 1.1045 - val_accuracy: 0.6935\n",
      "Epoch 17/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0909 - accuracy: 0.9712 - val_loss: 1.4040 - val_accuracy: 0.7581\n",
      "Epoch 18/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0666 - accuracy: 0.9856 - val_loss: 1.1746 - val_accuracy: 0.6613\n",
      "Epoch 19/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0681 - accuracy: 0.9820 - val_loss: 0.9483 - val_accuracy: 0.6935\n",
      "Epoch 20/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0618 - accuracy: 0.9838 - val_loss: 1.2420 - val_accuracy: 0.5484\n",
      "Epoch 21/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0440 - accuracy: 0.9874 - val_loss: 1.1688 - val_accuracy: 0.6774\n",
      "Epoch 22/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0272 - accuracy: 0.9964 - val_loss: 1.2510 - val_accuracy: 0.6774\n",
      "Epoch 23/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0270 - accuracy: 0.9964 - val_loss: 1.1174 - val_accuracy: 0.7097\n",
      "Epoch 24/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0262 - accuracy: 0.9928 - val_loss: 1.1412 - val_accuracy: 0.7903\n",
      "Epoch 25/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0262 - accuracy: 0.9964 - val_loss: 1.1887 - val_accuracy: 0.7258\n",
      "Epoch 26/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0156 - accuracy: 0.9982 - val_loss: 1.4173 - val_accuracy: 0.7903\n",
      "Epoch 27/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0236 - accuracy: 0.9928 - val_loss: 1.0764 - val_accuracy: 0.7097\n",
      "Epoch 28/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0222 - accuracy: 0.9910 - val_loss: 1.1273 - val_accuracy: 0.7258\n",
      "Epoch 29/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0290 - accuracy: 0.9946 - val_loss: 1.3348 - val_accuracy: 0.7903\n",
      "Epoch 30/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0401 - accuracy: 0.9892 - val_loss: 1.0819 - val_accuracy: 0.6935\n",
      "Epoch 31/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0187 - accuracy: 0.9964 - val_loss: 1.3144 - val_accuracy: 0.7419\n",
      "Epoch 32/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0250 - accuracy: 0.9946 - val_loss: 1.1985 - val_accuracy: 0.7742\n",
      "Epoch 33/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0132 - accuracy: 0.9982 - val_loss: 1.1911 - val_accuracy: 0.7903\n",
      "Epoch 34/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0338 - accuracy: 0.9892 - val_loss: 1.0625 - val_accuracy: 0.7419\n",
      "Epoch 35/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0240 - accuracy: 0.9928 - val_loss: 1.6511 - val_accuracy: 0.8065\n",
      "Epoch 36/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0185 - accuracy: 0.9964 - val_loss: 1.3544 - val_accuracy: 0.8065\n",
      "Epoch 37/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0339 - accuracy: 0.9928 - val_loss: 1.2875 - val_accuracy: 0.7581\n",
      "Epoch 38/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0182 - accuracy: 0.9946 - val_loss: 1.4099 - val_accuracy: 0.8065\n",
      "Epoch 39/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0302 - accuracy: 0.9928 - val_loss: 1.2303 - val_accuracy: 0.6774\n",
      "Epoch 40/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0234 - accuracy: 0.9946 - val_loss: 1.4718 - val_accuracy: 0.7903\n",
      "Epoch 41/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0110 - accuracy: 0.9982 - val_loss: 1.4947 - val_accuracy: 0.8065\n",
      "Epoch 42/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0355 - accuracy: 0.9856 - val_loss: 1.6235 - val_accuracy: 0.8065\n",
      "Epoch 43/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0076 - accuracy: 0.9982 - val_loss: 1.1645 - val_accuracy: 0.7581\n",
      "Epoch 44/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.8532 - val_accuracy: 0.7903\n",
      "Epoch 45/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0161 - accuracy: 0.9928 - val_loss: 0.9329 - val_accuracy: 0.7903\n",
      "Epoch 46/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0492 - accuracy: 0.9820 - val_loss: 1.0903 - val_accuracy: 0.6774\n",
      "Epoch 47/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0499 - accuracy: 0.9838 - val_loss: 1.4617 - val_accuracy: 0.7742\n",
      "Epoch 48/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0366 - accuracy: 0.9910 - val_loss: 1.0373 - val_accuracy: 0.6290\n",
      "Epoch 49/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0344 - accuracy: 0.9892 - val_loss: 1.3313 - val_accuracy: 0.6935\n",
      "Epoch 50/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0162 - accuracy: 0.9964 - val_loss: 0.9661 - val_accuracy: 0.6935\n",
      "Epoch 51/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0466 - accuracy: 0.9874 - val_loss: 0.9062 - val_accuracy: 0.7742\n",
      "Epoch 52/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0067 - accuracy: 0.9982 - val_loss: 1.5962 - val_accuracy: 0.6774\n",
      "Epoch 53/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0113 - accuracy: 0.9964 - val_loss: 1.2196 - val_accuracy: 0.7419\n",
      "Epoch 54/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0056 - accuracy: 1.0000 - val_loss: 1.2875 - val_accuracy: 0.8226\n",
      "Epoch 55/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0132 - accuracy: 0.9982 - val_loss: 1.3964 - val_accuracy: 0.8065\n",
      "Epoch 56/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0151 - accuracy: 0.9964 - val_loss: 1.3778 - val_accuracy: 0.6613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0098 - accuracy: 0.9946 - val_loss: 1.1330 - val_accuracy: 0.7903\n",
      "Epoch 58/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0180 - accuracy: 0.9928 - val_loss: 1.2189 - val_accuracy: 0.7097\n",
      "Epoch 59/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0065 - accuracy: 0.9982 - val_loss: 1.7107 - val_accuracy: 0.8226\n",
      "Epoch 60/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0162 - accuracy: 0.9964 - val_loss: 2.0989 - val_accuracy: 0.7903\n",
      "Epoch 61/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.2631 - val_accuracy: 0.7903\n",
      "Epoch 62/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.6759 - val_accuracy: 0.8065\n",
      "Epoch 63/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.7694 - val_accuracy: 0.8065\n",
      "Epoch 64/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0069 - accuracy: 0.9964 - val_loss: 1.6419 - val_accuracy: 0.7258\n",
      "Epoch 65/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0209 - accuracy: 0.9910 - val_loss: 1.9103 - val_accuracy: 0.6129\n",
      "Epoch 66/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.3011 - val_accuracy: 0.6935\n",
      "Epoch 67/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0055 - accuracy: 0.9982 - val_loss: 1.2940 - val_accuracy: 0.7419\n",
      "Epoch 68/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0127 - accuracy: 0.9982 - val_loss: 1.3810 - val_accuracy: 0.8065\n",
      "Epoch 69/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0071 - accuracy: 0.9982 - val_loss: 1.2620 - val_accuracy: 0.7097\n",
      "Epoch 70/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.2997 - val_accuracy: 0.6935\n",
      "Epoch 71/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0136 - accuracy: 0.9964 - val_loss: 1.2488 - val_accuracy: 0.8065\n",
      "Epoch 72/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0052 - accuracy: 0.9982 - val_loss: 1.6451 - val_accuracy: 0.8065\n",
      "Epoch 73/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0067 - accuracy: 0.9982 - val_loss: 1.2228 - val_accuracy: 0.8065\n",
      "Epoch 74/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.4006 - val_accuracy: 0.8065\n",
      "Epoch 75/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0065 - accuracy: 0.9964 - val_loss: 1.1311 - val_accuracy: 0.8065\n",
      "Epoch 76/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0066 - accuracy: 0.9982 - val_loss: 1.8209 - val_accuracy: 0.5645\n",
      "Epoch 77/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0126 - accuracy: 0.9964 - val_loss: 2.1161 - val_accuracy: 0.7581\n",
      "Epoch 78/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0102 - accuracy: 0.9964 - val_loss: 2.5624 - val_accuracy: 0.5484\n",
      "Epoch 79/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0090 - accuracy: 0.9946 - val_loss: 1.9047 - val_accuracy: 0.5968\n",
      "Epoch 80/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0123 - accuracy: 0.9946 - val_loss: 1.7363 - val_accuracy: 0.7903\n",
      "Epoch 81/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0192 - accuracy: 0.9946 - val_loss: 1.3511 - val_accuracy: 0.7097\n",
      "Epoch 82/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0169 - accuracy: 0.9964 - val_loss: 1.4122 - val_accuracy: 0.7903\n",
      "Epoch 83/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0270 - accuracy: 0.9946 - val_loss: 1.4678 - val_accuracy: 0.8065\n",
      "Epoch 84/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.2530 - val_accuracy: 0.8065\n",
      "Epoch 85/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0062 - accuracy: 0.9964 - val_loss: 1.2995 - val_accuracy: 0.8065\n",
      "Epoch 86/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.5238 - val_accuracy: 0.8065\n",
      "Epoch 87/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.4205 - val_accuracy: 0.8065\n",
      "Epoch 88/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0082 - accuracy: 0.9964 - val_loss: 1.6282 - val_accuracy: 0.8065\n",
      "Epoch 89/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 0.9982 - val_loss: 1.5368 - val_accuracy: 0.8065\n",
      "Epoch 90/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.3068 - val_accuracy: 0.8065\n",
      "Epoch 91/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.4979 - val_accuracy: 0.7097\n",
      "Epoch 92/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.4198 - val_accuracy: 0.7742\n",
      "Epoch 93/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.4881 - val_accuracy: 0.7903\n",
      "Epoch 94/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 0.9982 - val_loss: 1.3144 - val_accuracy: 0.7097\n",
      "Epoch 95/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0218 - accuracy: 0.9910 - val_loss: 1.2598 - val_accuracy: 0.8065\n",
      "Epoch 96/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0177 - accuracy: 0.9982 - val_loss: 1.3328 - val_accuracy: 0.8065\n",
      "Epoch 97/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.3277 - val_accuracy: 0.7419\n",
      "Epoch 98/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.3968 - val_accuracy: 0.7903\n",
      "Epoch 99/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.4415 - val_accuracy: 0.8065\n",
      "Epoch 100/100\n",
      "555/555 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.5500 - val_accuracy: 0.7097\n",
      "accuracy for model 5 is 70.96773982048035\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 192)               119655936 \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 119,691,683\n",
      "Trainable params: 119,691,619\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "556/556 [==============================] - 31s 55ms/sample - loss: 1.3301 - accuracy: 0.3993 - val_loss: 5.5957 - val_accuracy: 0.5902\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.9280 - accuracy: 0.5827 - val_loss: 2.3965 - val_accuracy: 0.7213\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.7958 - accuracy: 0.6583 - val_loss: 0.8434 - val_accuracy: 0.7541\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.7429 - accuracy: 0.6799 - val_loss: 2.1247 - val_accuracy: 0.3934\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.6874 - accuracy: 0.7014 - val_loss: 2.4478 - val_accuracy: 0.3934\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.6368 - accuracy: 0.7266 - val_loss: 2.0107 - val_accuracy: 0.3934\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.5636 - accuracy: 0.7536 - val_loss: 2.3676 - val_accuracy: 0.3607\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.5103 - accuracy: 0.7878 - val_loss: 1.5869 - val_accuracy: 0.3934\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.4788 - accuracy: 0.8022 - val_loss: 1.4808 - val_accuracy: 0.3770\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.4519 - accuracy: 0.8309 - val_loss: 1.6673 - val_accuracy: 0.4098\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.3769 - accuracy: 0.8561 - val_loss: 0.7866 - val_accuracy: 0.6393\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.3476 - accuracy: 0.8633 - val_loss: 1.5492 - val_accuracy: 0.4754\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2705 - accuracy: 0.8867 - val_loss: 1.4345 - val_accuracy: 0.5574\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2656 - accuracy: 0.9101 - val_loss: 1.3669 - val_accuracy: 0.5410\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2485 - accuracy: 0.9011 - val_loss: 1.1055 - val_accuracy: 0.5902\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2070 - accuracy: 0.9227 - val_loss: 0.6400 - val_accuracy: 0.7541\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1852 - accuracy: 0.9299 - val_loss: 0.7404 - val_accuracy: 0.7377\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1544 - accuracy: 0.9496 - val_loss: 0.9119 - val_accuracy: 0.6885\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1786 - accuracy: 0.9478 - val_loss: 0.8389 - val_accuracy: 0.6721\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1385 - accuracy: 0.9496 - val_loss: 0.7449 - val_accuracy: 0.7705\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1100 - accuracy: 0.9604 - val_loss: 0.7670 - val_accuracy: 0.7705\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0856 - accuracy: 0.9748 - val_loss: 0.7618 - val_accuracy: 0.7705\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0832 - accuracy: 0.9766 - val_loss: 0.7701 - val_accuracy: 0.7705\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0906 - accuracy: 0.9694 - val_loss: 1.1196 - val_accuracy: 0.6721\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0729 - accuracy: 0.9712 - val_loss: 0.7406 - val_accuracy: 0.8033\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0691 - accuracy: 0.9730 - val_loss: 1.2012 - val_accuracy: 0.6230\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0527 - accuracy: 0.9838 - val_loss: 1.0110 - val_accuracy: 0.7869\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0540 - accuracy: 0.9802 - val_loss: 0.8243 - val_accuracy: 0.7869\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0367 - accuracy: 0.9910 - val_loss: 0.8113 - val_accuracy: 0.7869\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0316 - accuracy: 0.9910 - val_loss: 0.9146 - val_accuracy: 0.7869\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0522 - accuracy: 0.9928 - val_loss: 0.8020 - val_accuracy: 0.7705\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0296 - accuracy: 0.9928 - val_loss: 1.1185 - val_accuracy: 0.7869\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0357 - accuracy: 0.9892 - val_loss: 0.9952 - val_accuracy: 0.7377\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0656 - accuracy: 0.9766 - val_loss: 1.0622 - val_accuracy: 0.7705\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0283 - accuracy: 0.9928 - val_loss: 0.9566 - val_accuracy: 0.7213\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0242 - accuracy: 0.9946 - val_loss: 1.0629 - val_accuracy: 0.7705\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0362 - accuracy: 0.9928 - val_loss: 1.0904 - val_accuracy: 0.7377\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0389 - accuracy: 0.9874 - val_loss: 0.9431 - val_accuracy: 0.7869\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0157 - accuracy: 0.9982 - val_loss: 1.2094 - val_accuracy: 0.7049\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0216 - accuracy: 0.9946 - val_loss: 0.8731 - val_accuracy: 0.7869\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0213 - accuracy: 0.9928 - val_loss: 0.8762 - val_accuracy: 0.7705\n",
      "Epoch 42/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0129 - accuracy: 0.9964 - val_loss: 1.1248 - val_accuracy: 0.7049\n",
      "Epoch 43/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0131 - accuracy: 0.9982 - val_loss: 1.1220 - val_accuracy: 0.8033\n",
      "Epoch 44/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0133 - accuracy: 0.9982 - val_loss: 1.2070 - val_accuracy: 0.7213\n",
      "Epoch 45/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0149 - accuracy: 0.9982 - val_loss: 1.3261 - val_accuracy: 0.6885\n",
      "Epoch 46/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0115 - accuracy: 1.0000 - val_loss: 1.1313 - val_accuracy: 0.7049\n",
      "Epoch 47/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0169 - accuracy: 0.9946 - val_loss: 0.5997 - val_accuracy: 0.8197\n",
      "Epoch 48/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0227 - accuracy: 0.9910 - val_loss: 1.2079 - val_accuracy: 0.6557\n",
      "Epoch 49/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0202 - accuracy: 0.9910 - val_loss: 0.8310 - val_accuracy: 0.8033\n",
      "Epoch 50/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0091 - accuracy: 0.9964 - val_loss: 1.1273 - val_accuracy: 0.7213\n",
      "Epoch 51/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0261 - accuracy: 0.9892 - val_loss: 0.9802 - val_accuracy: 0.8033\n",
      "Epoch 52/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0176 - accuracy: 0.9946 - val_loss: 1.4417 - val_accuracy: 0.6885\n",
      "Epoch 53/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0162 - accuracy: 0.9928 - val_loss: 1.5554 - val_accuracy: 0.6885\n",
      "Epoch 54/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0161 - accuracy: 0.9946 - val_loss: 1.6044 - val_accuracy: 0.6885\n",
      "Epoch 55/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0154 - accuracy: 0.9964 - val_loss: 1.0216 - val_accuracy: 0.7705\n",
      "Epoch 56/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0156 - accuracy: 0.9946 - val_loss: 1.1756 - val_accuracy: 0.7541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0174 - accuracy: 0.9946 - val_loss: 1.2072 - val_accuracy: 0.7213\n",
      "Epoch 58/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0098 - accuracy: 0.9982 - val_loss: 1.1291 - val_accuracy: 0.7377\n",
      "Epoch 59/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0086 - accuracy: 0.9964 - val_loss: 1.3004 - val_accuracy: 0.7869\n",
      "Epoch 60/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.1293 - val_accuracy: 0.7541\n",
      "Epoch 61/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0152 - accuracy: 0.9946 - val_loss: 1.2302 - val_accuracy: 0.7541\n",
      "Epoch 62/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0088 - accuracy: 0.9964 - val_loss: 1.0830 - val_accuracy: 0.7869\n",
      "Epoch 63/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.4397 - val_accuracy: 0.7049\n",
      "Epoch 64/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0203 - accuracy: 0.9928 - val_loss: 1.3242 - val_accuracy: 0.7705\n",
      "Epoch 65/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0131 - accuracy: 0.9928 - val_loss: 1.3561 - val_accuracy: 0.7213\n",
      "Epoch 66/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0142 - accuracy: 0.9982 - val_loss: 1.1495 - val_accuracy: 0.7377\n",
      "Epoch 67/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.1719 - val_accuracy: 0.7377\n",
      "Epoch 68/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.2325 - val_accuracy: 0.7377\n",
      "Epoch 69/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0075 - accuracy: 0.9982 - val_loss: 1.1783 - val_accuracy: 0.7377\n",
      "Epoch 70/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.1397 - val_accuracy: 0.7541\n",
      "Epoch 71/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0120 - accuracy: 0.9964 - val_loss: 1.3412 - val_accuracy: 0.6885\n",
      "Epoch 72/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0180 - accuracy: 0.9928 - val_loss: 1.3012 - val_accuracy: 0.7049\n",
      "Epoch 73/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0199 - accuracy: 0.9946 - val_loss: 1.0773 - val_accuracy: 0.7705\n",
      "Epoch 74/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0087 - accuracy: 0.9928 - val_loss: 1.1051 - val_accuracy: 0.7705\n",
      "Epoch 75/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0060 - accuracy: 0.9982 - val_loss: 1.2074 - val_accuracy: 0.8197\n",
      "Epoch 76/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2396 - val_accuracy: 0.7869\n",
      "Epoch 77/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.2693 - val_accuracy: 0.7705\n",
      "Epoch 78/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 0.9982 - val_loss: 1.3139 - val_accuracy: 0.7377\n",
      "Epoch 79/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.2613 - val_accuracy: 0.8033\n",
      "Epoch 80/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.2983 - val_accuracy: 0.7705\n",
      "Epoch 81/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.2661 - val_accuracy: 0.7869\n",
      "Epoch 82/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0135 - accuracy: 0.9946 - val_loss: 1.3767 - val_accuracy: 0.7705\n",
      "Epoch 83/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0101 - accuracy: 0.9964 - val_loss: 1.0188 - val_accuracy: 0.8197\n",
      "Epoch 84/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.0050 - val_accuracy: 0.8197\n",
      "Epoch 85/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.2144 - val_accuracy: 0.7705\n",
      "Epoch 86/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.3290 - val_accuracy: 0.7541\n",
      "Epoch 87/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0074 - accuracy: 0.9982 - val_loss: 1.5118 - val_accuracy: 0.7705\n",
      "Epoch 88/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0177 - accuracy: 0.9946 - val_loss: 1.3911 - val_accuracy: 0.7213\n",
      "Epoch 89/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.3936 - val_accuracy: 0.7049\n",
      "Epoch 90/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0070 - accuracy: 0.9964 - val_loss: 1.5416 - val_accuracy: 0.7541\n",
      "Epoch 91/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.3666 - val_accuracy: 0.7541\n",
      "Epoch 92/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0081 - accuracy: 0.9964 - val_loss: 2.0814 - val_accuracy: 0.6393\n",
      "Epoch 93/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.4451 - val_accuracy: 0.6721\n",
      "Epoch 94/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0063 - accuracy: 0.9964 - val_loss: 1.2438 - val_accuracy: 0.7213\n",
      "Epoch 95/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0128 - accuracy: 0.9946 - val_loss: 1.4227 - val_accuracy: 0.7213\n",
      "Epoch 96/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0134 - accuracy: 0.9982 - val_loss: 1.2905 - val_accuracy: 0.6721\n",
      "Epoch 97/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0110 - accuracy: 0.9946 - val_loss: 1.2702 - val_accuracy: 0.7869\n",
      "Epoch 98/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0143 - accuracy: 0.9946 - val_loss: 1.4034 - val_accuracy: 0.7213\n",
      "Epoch 99/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.5595 - val_accuracy: 0.7705\n",
      "Epoch 100/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 0.9982 - val_loss: 1.5411 - val_accuracy: 0.7705\n",
      "accuracy for model 6 is 77.04917788505554\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 192)               119655936 \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 119,691,683\n",
      "Trainable params: 119,691,619\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "556/556 [==============================] - 16s 29ms/sample - loss: 0.9715 - accuracy: 0.5576 - val_loss: 3.9339 - val_accuracy: 0.5902\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.8552 - accuracy: 0.6295 - val_loss: 3.3486 - val_accuracy: 0.5902\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.7641 - accuracy: 0.6799 - val_loss: 2.0272 - val_accuracy: 0.5902\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.7309 - accuracy: 0.6978 - val_loss: 1.4020 - val_accuracy: 0.7705\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.6604 - accuracy: 0.7248 - val_loss: 0.8447 - val_accuracy: 0.7705\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.6136 - accuracy: 0.7590 - val_loss: 0.7917 - val_accuracy: 0.7705\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.5793 - accuracy: 0.7644 - val_loss: 0.7144 - val_accuracy: 0.7705\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.5135 - accuracy: 0.8147 - val_loss: 0.8952 - val_accuracy: 0.7377\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.4608 - accuracy: 0.8417 - val_loss: 0.6091 - val_accuracy: 0.7869\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.3773 - accuracy: 0.8633 - val_loss: 0.6959 - val_accuracy: 0.7705\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.3672 - accuracy: 0.8597 - val_loss: 1.3591 - val_accuracy: 0.7541\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.3390 - accuracy: 0.8615 - val_loss: 0.6633 - val_accuracy: 0.7705\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2953 - accuracy: 0.8939 - val_loss: 0.9733 - val_accuracy: 0.7705\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2762 - accuracy: 0.9065 - val_loss: 0.7715 - val_accuracy: 0.7869\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2014 - accuracy: 0.9371 - val_loss: 0.6939 - val_accuracy: 0.8033\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1997 - accuracy: 0.9317 - val_loss: 0.6492 - val_accuracy: 0.7705\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1607 - accuracy: 0.9478 - val_loss: 0.7430 - val_accuracy: 0.7869\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1635 - accuracy: 0.9406 - val_loss: 0.6987 - val_accuracy: 0.7869\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1026 - accuracy: 0.9712 - val_loss: 1.0511 - val_accuracy: 0.7705\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1111 - accuracy: 0.9568 - val_loss: 0.8283 - val_accuracy: 0.7705\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0753 - accuracy: 0.9748 - val_loss: 0.8668 - val_accuracy: 0.7869\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0827 - accuracy: 0.9766 - val_loss: 0.8557 - val_accuracy: 0.8033\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0643 - accuracy: 0.9784 - val_loss: 0.9922 - val_accuracy: 0.7869\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0579 - accuracy: 0.9838 - val_loss: 0.7776 - val_accuracy: 0.7869\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0645 - accuracy: 0.9802 - val_loss: 0.9528 - val_accuracy: 0.8033\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0603 - accuracy: 0.9820 - val_loss: 0.8097 - val_accuracy: 0.7213\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0636 - accuracy: 0.9748 - val_loss: 0.8707 - val_accuracy: 0.8033\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0522 - accuracy: 0.9820 - val_loss: 0.8104 - val_accuracy: 0.8033\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0381 - accuracy: 0.9892 - val_loss: 0.8829 - val_accuracy: 0.7705\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0264 - accuracy: 0.9928 - val_loss: 0.9656 - val_accuracy: 0.8033\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0260 - accuracy: 0.9910 - val_loss: 1.0093 - val_accuracy: 0.8033\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0330 - accuracy: 0.9892 - val_loss: 0.8904 - val_accuracy: 0.8033\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0367 - accuracy: 0.9874 - val_loss: 0.9464 - val_accuracy: 0.8033\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0307 - accuracy: 0.9928 - val_loss: 0.9888 - val_accuracy: 0.7705\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0305 - accuracy: 0.9874 - val_loss: 1.0089 - val_accuracy: 0.7869\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0199 - accuracy: 0.9928 - val_loss: 1.0581 - val_accuracy: 0.7705\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0254 - accuracy: 0.9928 - val_loss: 1.2336 - val_accuracy: 0.7869\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0280 - accuracy: 0.9928 - val_loss: 1.4423 - val_accuracy: 0.8033\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0262 - accuracy: 0.9910 - val_loss: 1.1153 - val_accuracy: 0.8033\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0296 - accuracy: 0.9946 - val_loss: 1.0847 - val_accuracy: 0.7869\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0117 - accuracy: 0.9964 - val_loss: 1.4171 - val_accuracy: 0.8033\n",
      "Epoch 42/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0196 - accuracy: 0.9964 - val_loss: 1.1845 - val_accuracy: 0.7869\n",
      "Epoch 43/100\n",
      "556/556 [==============================] - 2s 4ms/sample - loss: 0.0153 - accuracy: 0.9946 - val_loss: 1.2181 - val_accuracy: 0.7869\n",
      "Epoch 44/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0106 - accuracy: 0.9964 - val_loss: 1.2942 - val_accuracy: 0.7869\n",
      "Epoch 45/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0172 - accuracy: 0.9928 - val_loss: 1.2209 - val_accuracy: 0.8033\n",
      "Epoch 46/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0059 - accuracy: 0.9982 - val_loss: 1.2478 - val_accuracy: 0.8033\n",
      "Epoch 47/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0139 - accuracy: 0.9964 - val_loss: 1.1772 - val_accuracy: 0.8033\n",
      "Epoch 48/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0109 - accuracy: 0.9964 - val_loss: 1.3453 - val_accuracy: 0.8033\n",
      "Epoch 49/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0121 - accuracy: 0.9964 - val_loss: 1.1543 - val_accuracy: 0.7869\n",
      "Epoch 50/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.1567 - val_accuracy: 0.8033\n",
      "Epoch 51/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2099 - val_accuracy: 0.8033\n",
      "Epoch 52/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0188 - accuracy: 0.9946 - val_loss: 1.2802 - val_accuracy: 0.8033\n",
      "Epoch 53/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.3626 - val_accuracy: 0.8033\n",
      "Epoch 54/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.2583 - val_accuracy: 0.7869\n",
      "Epoch 55/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.2168 - val_accuracy: 0.7705\n",
      "Epoch 56/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.2384 - val_accuracy: 0.7869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0061 - accuracy: 0.9982 - val_loss: 1.3168 - val_accuracy: 0.8033\n",
      "Epoch 58/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0089 - accuracy: 0.9982 - val_loss: 1.5574 - val_accuracy: 0.8033\n",
      "Epoch 59/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0112 - accuracy: 0.9982 - val_loss: 1.1681 - val_accuracy: 0.7869\n",
      "Epoch 60/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0112 - accuracy: 0.9964 - val_loss: 1.5611 - val_accuracy: 0.8033\n",
      "Epoch 61/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0142 - accuracy: 0.9982 - val_loss: 1.4152 - val_accuracy: 0.8033\n",
      "Epoch 62/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0140 - accuracy: 0.9964 - val_loss: 1.0930 - val_accuracy: 0.8033\n",
      "Epoch 63/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0068 - accuracy: 0.9982 - val_loss: 1.1146 - val_accuracy: 0.8197\n",
      "Epoch 64/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0131 - accuracy: 0.9946 - val_loss: 1.0341 - val_accuracy: 0.7541\n",
      "Epoch 65/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0087 - accuracy: 0.9982 - val_loss: 1.3116 - val_accuracy: 0.8033\n",
      "Epoch 66/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 0.9982 - val_loss: 1.4369 - val_accuracy: 0.8033\n",
      "Epoch 67/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0038 - accuracy: 0.9982 - val_loss: 1.3572 - val_accuracy: 0.8197\n",
      "Epoch 68/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.4700 - val_accuracy: 0.8033\n",
      "Epoch 69/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0187 - accuracy: 0.9982 - val_loss: 1.2261 - val_accuracy: 0.7705\n",
      "Epoch 70/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.3013 - val_accuracy: 0.8197\n",
      "Epoch 71/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.3836 - val_accuracy: 0.8197\n",
      "Epoch 72/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.3772 - val_accuracy: 0.8197\n",
      "Epoch 73/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0185 - accuracy: 0.9946 - val_loss: 1.7373 - val_accuracy: 0.7705\n",
      "Epoch 74/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0098 - accuracy: 0.9946 - val_loss: 1.6540 - val_accuracy: 0.7869\n",
      "Epoch 75/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.4838 - val_accuracy: 0.8197\n",
      "Epoch 76/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.4324 - val_accuracy: 0.8033\n",
      "Epoch 77/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.2782 - val_accuracy: 0.7049\n",
      "Epoch 78/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.2882 - val_accuracy: 0.8197\n",
      "Epoch 79/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0043 - accuracy: 0.9982 - val_loss: 1.7644 - val_accuracy: 0.7869\n",
      "Epoch 80/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.6782 - val_accuracy: 0.7869\n",
      "Epoch 81/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 0.9982 - val_loss: 1.5135 - val_accuracy: 0.8033\n",
      "Epoch 82/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.4559 - val_accuracy: 0.8033\n",
      "Epoch 83/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.3195 - val_accuracy: 0.8033\n",
      "Epoch 84/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0033 - accuracy: 0.9982 - val_loss: 1.4811 - val_accuracy: 0.7869\n",
      "Epoch 85/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0098 - accuracy: 0.9946 - val_loss: 1.6041 - val_accuracy: 0.8033\n",
      "Epoch 86/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.5626 - val_accuracy: 0.8033\n",
      "Epoch 87/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0032 - accuracy: 0.9982 - val_loss: 1.3069 - val_accuracy: 0.7869\n",
      "Epoch 88/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.3085 - val_accuracy: 0.8033\n",
      "Epoch 89/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 7.8148e-04 - accuracy: 1.0000 - val_loss: 1.3424 - val_accuracy: 0.8033\n",
      "Epoch 90/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0132 - accuracy: 0.9964 - val_loss: 1.6135 - val_accuracy: 0.8033\n",
      "Epoch 91/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0199 - accuracy: 0.9946 - val_loss: 1.2731 - val_accuracy: 0.7541\n",
      "Epoch 92/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0115 - accuracy: 0.9964 - val_loss: 1.6146 - val_accuracy: 0.8033\n",
      "Epoch 93/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0108 - accuracy: 0.9964 - val_loss: 1.4717 - val_accuracy: 0.7049\n",
      "Epoch 94/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0052 - accuracy: 0.9964 - val_loss: 1.7262 - val_accuracy: 0.8033\n",
      "Epoch 95/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0051 - accuracy: 0.9982 - val_loss: 1.4303 - val_accuracy: 0.8033\n",
      "Epoch 96/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0075 - accuracy: 0.9964 - val_loss: 1.3102 - val_accuracy: 0.8033\n",
      "Epoch 97/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.3973 - val_accuracy: 0.7049\n",
      "Epoch 98/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0155 - accuracy: 0.9910 - val_loss: 1.4925 - val_accuracy: 0.8197\n",
      "Epoch 99/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0128 - accuracy: 0.9982 - val_loss: 1.5243 - val_accuracy: 0.6557\n",
      "Epoch 100/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0144 - accuracy: 0.9946 - val_loss: 1.6255 - val_accuracy: 0.8033\n",
      "accuracy for model 7 is 80.32786846160889\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 192)               119655936 \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 119,691,683\n",
      "Trainable params: 119,691,619\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 556 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "556/556 [==============================] - 11s 19ms/sample - loss: 0.9566 - accuracy: 0.5360 - val_loss: 4.9641 - val_accuracy: 0.5902\n",
      "Epoch 2/100\n",
      "556/556 [==============================] - 3s 6ms/sample - loss: 0.7472 - accuracy: 0.6835 - val_loss: 2.4456 - val_accuracy: 0.6066\n",
      "Epoch 3/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.6725 - accuracy: 0.7248 - val_loss: 1.3720 - val_accuracy: 0.6066\n",
      "Epoch 4/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.6336 - accuracy: 0.7176 - val_loss: 1.1074 - val_accuracy: 0.6230\n",
      "Epoch 5/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.6030 - accuracy: 0.7338 - val_loss: 1.1249 - val_accuracy: 0.6230\n",
      "Epoch 6/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.5445 - accuracy: 0.7608 - val_loss: 1.0676 - val_accuracy: 0.3934\n",
      "Epoch 7/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.5058 - accuracy: 0.7842 - val_loss: 1.2498 - val_accuracy: 0.3443\n",
      "Epoch 8/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.4739 - accuracy: 0.8165 - val_loss: 1.0825 - val_accuracy: 0.4098\n",
      "Epoch 9/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.4055 - accuracy: 0.8381 - val_loss: 1.2978 - val_accuracy: 0.6393\n",
      "Epoch 10/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.3180 - accuracy: 0.8867 - val_loss: 1.1326 - val_accuracy: 0.6885\n",
      "Epoch 11/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2534 - accuracy: 0.9227 - val_loss: 1.9725 - val_accuracy: 0.6393\n",
      "Epoch 12/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2212 - accuracy: 0.9281 - val_loss: 1.7054 - val_accuracy: 0.6393\n",
      "Epoch 13/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.2097 - accuracy: 0.9263 - val_loss: 1.3831 - val_accuracy: 0.4426\n",
      "Epoch 14/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1106 - accuracy: 0.9676 - val_loss: 1.3971 - val_accuracy: 0.6885\n",
      "Epoch 15/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.1058 - accuracy: 0.9694 - val_loss: 2.1366 - val_accuracy: 0.6721\n",
      "Epoch 16/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0685 - accuracy: 0.9766 - val_loss: 2.4131 - val_accuracy: 0.6230\n",
      "Epoch 17/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0540 - accuracy: 0.9856 - val_loss: 1.5690 - val_accuracy: 0.7049\n",
      "Epoch 18/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0398 - accuracy: 0.9892 - val_loss: 1.5585 - val_accuracy: 0.6885\n",
      "Epoch 19/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0452 - accuracy: 0.9874 - val_loss: 1.8517 - val_accuracy: 0.7049\n",
      "Epoch 20/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0524 - accuracy: 0.9874 - val_loss: 2.9373 - val_accuracy: 0.6393\n",
      "Epoch 21/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0421 - accuracy: 0.9874 - val_loss: 2.5658 - val_accuracy: 0.6230\n",
      "Epoch 22/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0477 - accuracy: 0.9856 - val_loss: 1.3288 - val_accuracy: 0.7049\n",
      "Epoch 23/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0366 - accuracy: 0.9892 - val_loss: 2.1979 - val_accuracy: 0.6721\n",
      "Epoch 24/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0653 - accuracy: 0.9802 - val_loss: 2.0663 - val_accuracy: 0.7049\n",
      "Epoch 25/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0239 - accuracy: 0.9964 - val_loss: 2.5873 - val_accuracy: 0.6721\n",
      "Epoch 26/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0258 - accuracy: 0.9892 - val_loss: 2.1372 - val_accuracy: 0.6885\n",
      "Epoch 27/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0215 - accuracy: 0.9946 - val_loss: 2.5104 - val_accuracy: 0.6885\n",
      "Epoch 28/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0237 - accuracy: 0.9964 - val_loss: 1.8659 - val_accuracy: 0.6066\n",
      "Epoch 29/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0291 - accuracy: 0.9928 - val_loss: 2.6360 - val_accuracy: 0.6393\n",
      "Epoch 30/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0171 - accuracy: 0.9946 - val_loss: 1.6302 - val_accuracy: 0.6230\n",
      "Epoch 31/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0105 - accuracy: 0.9982 - val_loss: 2.7287 - val_accuracy: 0.6557\n",
      "Epoch 32/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0351 - accuracy: 0.9856 - val_loss: 1.6243 - val_accuracy: 0.4918\n",
      "Epoch 33/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0210 - accuracy: 0.9982 - val_loss: 2.3360 - val_accuracy: 0.6885\n",
      "Epoch 34/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0160 - accuracy: 0.9910 - val_loss: 2.1341 - val_accuracy: 0.6885\n",
      "Epoch 35/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0691 - accuracy: 0.9766 - val_loss: 1.9734 - val_accuracy: 0.6885\n",
      "Epoch 36/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0165 - accuracy: 0.9964 - val_loss: 2.6033 - val_accuracy: 0.6885\n",
      "Epoch 37/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0429 - accuracy: 0.9820 - val_loss: 1.5923 - val_accuracy: 0.7213\n",
      "Epoch 38/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0132 - accuracy: 0.9982 - val_loss: 2.0184 - val_accuracy: 0.6885\n",
      "Epoch 39/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.0633 - val_accuracy: 0.7049\n",
      "Epoch 40/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.3950 - val_accuracy: 0.6885\n",
      "Epoch 41/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 2.5111 - val_accuracy: 0.6721\n",
      "Epoch 42/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0064 - accuracy: 0.9982 - val_loss: 2.1208 - val_accuracy: 0.5574\n",
      "Epoch 43/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0166 - accuracy: 0.9910 - val_loss: 2.0286 - val_accuracy: 0.6885\n",
      "Epoch 44/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0365 - accuracy: 0.9856 - val_loss: 3.4288 - val_accuracy: 0.6393\n",
      "Epoch 45/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0301 - accuracy: 0.9874 - val_loss: 3.4760 - val_accuracy: 0.2951\n",
      "Epoch 46/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0155 - accuracy: 0.9946 - val_loss: 2.9401 - val_accuracy: 0.6721\n",
      "Epoch 47/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0171 - accuracy: 0.9928 - val_loss: 1.9104 - val_accuracy: 0.7049\n",
      "Epoch 48/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0070 - accuracy: 0.9982 - val_loss: 2.5221 - val_accuracy: 0.6885\n",
      "Epoch 49/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0097 - accuracy: 0.9982 - val_loss: 2.3468 - val_accuracy: 0.6885\n",
      "Epoch 50/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.3886 - val_accuracy: 0.7049\n",
      "Epoch 51/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.1868 - val_accuracy: 0.7049\n",
      "Epoch 52/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0219 - accuracy: 0.9928 - val_loss: 1.6895 - val_accuracy: 0.6885\n",
      "Epoch 53/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0065 - accuracy: 0.9982 - val_loss: 2.5366 - val_accuracy: 0.6885\n",
      "Epoch 54/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.7212 - val_accuracy: 0.6885\n",
      "Epoch 55/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.6093 - val_accuracy: 0.6885\n",
      "Epoch 56/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0135 - accuracy: 0.9964 - val_loss: 2.1116 - val_accuracy: 0.6885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 0.9982 - val_loss: 2.1348 - val_accuracy: 0.6557\n",
      "Epoch 58/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0248 - accuracy: 0.9874 - val_loss: 3.1938 - val_accuracy: 0.6230\n",
      "Epoch 59/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0063 - accuracy: 0.9982 - val_loss: 2.3073 - val_accuracy: 0.6885\n",
      "Epoch 60/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.2853 - val_accuracy: 0.6885\n",
      "Epoch 61/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0088 - accuracy: 0.9982 - val_loss: 2.0508 - val_accuracy: 0.6066\n",
      "Epoch 62/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0046 - accuracy: 0.9982 - val_loss: 2.3556 - val_accuracy: 0.6885\n",
      "Epoch 63/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0039 - accuracy: 0.9982 - val_loss: 1.9092 - val_accuracy: 0.6721\n",
      "Epoch 64/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.4095 - val_accuracy: 0.7049\n",
      "Epoch 65/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.5358 - val_accuracy: 0.6885\n",
      "Epoch 66/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 2.4162 - val_accuracy: 0.7049\n",
      "Epoch 67/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.4748 - val_accuracy: 0.6885\n",
      "Epoch 68/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0156 - accuracy: 0.9964 - val_loss: 2.5283 - val_accuracy: 0.6885\n",
      "Epoch 69/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0258 - accuracy: 0.9910 - val_loss: 3.3285 - val_accuracy: 0.6393\n",
      "Epoch 70/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0148 - accuracy: 0.9946 - val_loss: 2.2793 - val_accuracy: 0.4918\n",
      "Epoch 71/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0232 - accuracy: 0.9892 - val_loss: 3.2826 - val_accuracy: 0.6393\n",
      "Epoch 72/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0103 - accuracy: 0.9982 - val_loss: 2.3910 - val_accuracy: 0.6885\n",
      "Epoch 73/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0047 - accuracy: 0.9982 - val_loss: 2.0098 - val_accuracy: 0.6230\n",
      "Epoch 74/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0101 - accuracy: 0.9964 - val_loss: 2.7938 - val_accuracy: 0.6721\n",
      "Epoch 75/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.2471 - val_accuracy: 0.7049\n",
      "Epoch 76/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 0.9982 - val_loss: 1.8209 - val_accuracy: 0.6885\n",
      "Epoch 77/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.1342 - val_accuracy: 0.7377\n",
      "Epoch 78/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0035 - accuracy: 0.9982 - val_loss: 2.1860 - val_accuracy: 0.7377\n",
      "Epoch 79/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0057 - accuracy: 0.9982 - val_loss: 2.2266 - val_accuracy: 0.6885\n",
      "Epoch 80/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.2683 - val_accuracy: 0.6885\n",
      "Epoch 81/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0035 - accuracy: 0.9982 - val_loss: 2.5231 - val_accuracy: 0.7049\n",
      "Epoch 82/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 0.9982 - val_loss: 2.3183 - val_accuracy: 0.6885\n",
      "Epoch 83/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0116 - accuracy: 0.9964 - val_loss: 1.7003 - val_accuracy: 0.6885\n",
      "Epoch 84/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0131 - accuracy: 0.9982 - val_loss: 1.9675 - val_accuracy: 0.6721\n",
      "Epoch 85/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.1023 - val_accuracy: 0.7213\n",
      "Epoch 86/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 0.9982 - val_loss: 2.3258 - val_accuracy: 0.6721\n",
      "Epoch 87/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0057 - accuracy: 0.9964 - val_loss: 3.2565 - val_accuracy: 0.4754\n",
      "Epoch 88/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0155 - accuracy: 0.9946 - val_loss: 4.9607 - val_accuracy: 0.6066\n",
      "Epoch 89/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0072 - accuracy: 0.9982 - val_loss: 2.4469 - val_accuracy: 0.6721\n",
      "Epoch 90/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0057 - accuracy: 0.9982 - val_loss: 4.5932 - val_accuracy: 0.6066\n",
      "Epoch 91/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0071 - accuracy: 0.9964 - val_loss: 2.5204 - val_accuracy: 0.6721\n",
      "Epoch 92/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0176 - accuracy: 0.9964 - val_loss: 2.3852 - val_accuracy: 0.4918\n",
      "Epoch 93/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0049 - accuracy: 0.9982 - val_loss: 3.8054 - val_accuracy: 0.6393\n",
      "Epoch 94/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0135 - accuracy: 0.9964 - val_loss: 3.4435 - val_accuracy: 0.4590\n",
      "Epoch 95/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0084 - accuracy: 0.9964 - val_loss: 3.2127 - val_accuracy: 0.6557\n",
      "Epoch 96/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.1398 - val_accuracy: 0.6557\n",
      "Epoch 97/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.8338 - val_accuracy: 0.6557\n",
      "Epoch 98/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.8168 - val_accuracy: 0.6393\n",
      "Epoch 99/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 0.9982 - val_loss: 2.4504 - val_accuracy: 0.6885\n",
      "Epoch 100/100\n",
      "556/556 [==============================] - 2s 3ms/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.4373 - val_accuracy: 0.6885\n",
      "accuracy for model 8 is 68.8524603843689\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 192)               119655936 \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 119,691,683\n",
      "Trainable params: 119,691,619\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 557 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      "557/557 [==============================] - 13s 23ms/sample - loss: 1.1914 - accuracy: 0.3824 - val_loss: 7.3876 - val_accuracy: 0.2333\n",
      "Epoch 2/100\n",
      "557/557 [==============================] - 3s 6ms/sample - loss: 0.8834 - accuracy: 0.6212 - val_loss: 2.7763 - val_accuracy: 0.2333\n",
      "Epoch 3/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.7478 - accuracy: 0.7074 - val_loss: 2.5175 - val_accuracy: 0.2333\n",
      "Epoch 4/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.6898 - accuracy: 0.6966 - val_loss: 2.5012 - val_accuracy: 0.2333\n",
      "Epoch 5/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.6383 - accuracy: 0.7217 - val_loss: 1.9399 - val_accuracy: 0.2500\n",
      "Epoch 6/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.5872 - accuracy: 0.7702 - val_loss: 1.4851 - val_accuracy: 0.2833\n",
      "Epoch 7/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.5183 - accuracy: 0.8025 - val_loss: 1.1099 - val_accuracy: 0.4667\n",
      "Epoch 8/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.4291 - accuracy: 0.8510 - val_loss: 0.8340 - val_accuracy: 0.6833\n",
      "Epoch 9/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.3484 - accuracy: 0.8797 - val_loss: 2.0341 - val_accuracy: 0.3333\n",
      "Epoch 10/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.3057 - accuracy: 0.8959 - val_loss: 1.1643 - val_accuracy: 0.6500\n",
      "Epoch 11/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.2453 - accuracy: 0.9156 - val_loss: 0.9944 - val_accuracy: 0.6500\n",
      "Epoch 12/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1929 - accuracy: 0.9372 - val_loss: 0.8519 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1618 - accuracy: 0.9587 - val_loss: 1.1889 - val_accuracy: 0.6167\n",
      "Epoch 14/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1326 - accuracy: 0.9569 - val_loss: 1.0650 - val_accuracy: 0.5833\n",
      "Epoch 15/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1356 - accuracy: 0.9443 - val_loss: 1.7868 - val_accuracy: 0.6333\n",
      "Epoch 16/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0818 - accuracy: 0.9820 - val_loss: 1.9209 - val_accuracy: 0.6167\n",
      "Epoch 17/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0798 - accuracy: 0.9767 - val_loss: 1.5139 - val_accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0512 - accuracy: 0.9892 - val_loss: 1.3815 - val_accuracy: 0.6500\n",
      "Epoch 19/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0416 - accuracy: 0.9892 - val_loss: 1.0839 - val_accuracy: 0.6833\n",
      "Epoch 20/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0303 - accuracy: 0.9910 - val_loss: 1.2229 - val_accuracy: 0.7000\n",
      "Epoch 21/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0514 - accuracy: 0.9856 - val_loss: 1.6127 - val_accuracy: 0.6500\n",
      "Epoch 22/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0335 - accuracy: 0.9928 - val_loss: 1.2829 - val_accuracy: 0.6667\n",
      "Epoch 23/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0280 - accuracy: 0.9892 - val_loss: 1.3268 - val_accuracy: 0.6833\n",
      "Epoch 24/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0225 - accuracy: 0.9946 - val_loss: 1.6639 - val_accuracy: 0.6500\n",
      "Epoch 25/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0186 - accuracy: 0.9964 - val_loss: 1.5165 - val_accuracy: 0.6833\n",
      "Epoch 26/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0376 - accuracy: 0.9838 - val_loss: 1.1523 - val_accuracy: 0.7333\n",
      "Epoch 27/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0164 - accuracy: 0.9982 - val_loss: 1.2267 - val_accuracy: 0.6833\n",
      "Epoch 28/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0528 - accuracy: 0.9874 - val_loss: 1.1069 - val_accuracy: 0.7000\n",
      "Epoch 29/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0335 - accuracy: 0.9910 - val_loss: 1.5585 - val_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0270 - accuracy: 0.9964 - val_loss: 1.5342 - val_accuracy: 0.6833\n",
      "Epoch 31/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0230 - accuracy: 0.9946 - val_loss: 1.6346 - val_accuracy: 0.6667\n",
      "Epoch 32/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0362 - accuracy: 0.9910 - val_loss: 1.6782 - val_accuracy: 0.5833\n",
      "Epoch 33/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0248 - accuracy: 0.9910 - val_loss: 2.1308 - val_accuracy: 0.6167\n",
      "Epoch 34/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0230 - accuracy: 0.9928 - val_loss: 1.7611 - val_accuracy: 0.6667\n",
      "Epoch 35/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0271 - accuracy: 0.9928 - val_loss: 1.6549 - val_accuracy: 0.6500\n",
      "Epoch 36/100\n",
      "557/557 [==============================] - 2s 4ms/sample - loss: 0.0270 - accuracy: 0.9910 - val_loss: 1.2634 - val_accuracy: 0.7000\n",
      "Epoch 37/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0133 - accuracy: 0.9946 - val_loss: 1.6741 - val_accuracy: 0.6833\n",
      "Epoch 38/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0233 - accuracy: 0.9946 - val_loss: 1.5970 - val_accuracy: 0.6167\n",
      "Epoch 39/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0139 - accuracy: 0.9946 - val_loss: 2.3520 - val_accuracy: 0.6333\n",
      "Epoch 40/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0139 - accuracy: 0.9964 - val_loss: 1.3683 - val_accuracy: 0.7167\n",
      "Epoch 41/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.5899 - val_accuracy: 0.7000\n",
      "Epoch 42/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0192 - accuracy: 0.9946 - val_loss: 1.9341 - val_accuracy: 0.6500\n",
      "Epoch 43/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0213 - accuracy: 0.9928 - val_loss: 1.5089 - val_accuracy: 0.7000\n",
      "Epoch 44/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.6253 - val_accuracy: 0.6833\n",
      "Epoch 45/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0141 - accuracy: 0.9964 - val_loss: 1.3705 - val_accuracy: 0.7167\n",
      "Epoch 46/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0073 - accuracy: 0.9982 - val_loss: 2.1858 - val_accuracy: 0.6500\n",
      "Epoch 47/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0234 - accuracy: 0.9928 - val_loss: 1.7787 - val_accuracy: 0.6167\n",
      "Epoch 48/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0067 - accuracy: 0.9982 - val_loss: 2.3809 - val_accuracy: 0.6333\n",
      "Epoch 49/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0102 - accuracy: 0.9964 - val_loss: 1.6774 - val_accuracy: 0.7000\n",
      "Epoch 50/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0339 - accuracy: 0.9874 - val_loss: 1.8464 - val_accuracy: 0.6833\n",
      "Epoch 51/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0084 - accuracy: 0.9982 - val_loss: 1.9734 - val_accuracy: 0.6500\n",
      "Epoch 52/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0272 - accuracy: 0.9892 - val_loss: 1.6696 - val_accuracy: 0.6833\n",
      "Epoch 53/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0186 - accuracy: 0.9946 - val_loss: 2.0868 - val_accuracy: 0.6500\n",
      "Epoch 54/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0257 - accuracy: 0.9910 - val_loss: 1.6566 - val_accuracy: 0.6333\n",
      "Epoch 55/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0264 - accuracy: 0.9874 - val_loss: 1.3153 - val_accuracy: 0.7167\n",
      "Epoch 56/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0191 - accuracy: 0.9928 - val_loss: 1.6427 - val_accuracy: 0.6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0190 - accuracy: 0.9928 - val_loss: 1.5561 - val_accuracy: 0.7167\n",
      "Epoch 58/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0245 - accuracy: 0.9910 - val_loss: 1.8405 - val_accuracy: 0.6833\n",
      "Epoch 59/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0235 - accuracy: 0.9856 - val_loss: 2.1972 - val_accuracy: 0.6000\n",
      "Epoch 60/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0244 - accuracy: 0.9910 - val_loss: 1.5048 - val_accuracy: 0.6833\n",
      "Epoch 61/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0187 - accuracy: 0.9946 - val_loss: 2.0801 - val_accuracy: 0.6167\n",
      "Epoch 62/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0364 - accuracy: 0.9820 - val_loss: 1.6859 - val_accuracy: 0.6833\n",
      "Epoch 63/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0124 - accuracy: 0.9964 - val_loss: 1.4801 - val_accuracy: 0.6333\n",
      "Epoch 64/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0124 - accuracy: 0.9946 - val_loss: 2.7233 - val_accuracy: 0.6500\n",
      "Epoch 65/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0198 - accuracy: 0.9928 - val_loss: 1.6385 - val_accuracy: 0.6000\n",
      "Epoch 66/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0145 - accuracy: 0.9946 - val_loss: 1.5538 - val_accuracy: 0.7000\n",
      "Epoch 67/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0086 - accuracy: 0.9982 - val_loss: 2.2267 - val_accuracy: 0.6500\n",
      "Epoch 68/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 1.4413 - val_accuracy: 0.7167\n",
      "Epoch 69/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.7232 - val_accuracy: 0.7167\n",
      "Epoch 70/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 0.9982 - val_loss: 1.7177 - val_accuracy: 0.7333\n",
      "Epoch 71/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.5729 - val_accuracy: 0.6833\n",
      "Epoch 72/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.5043 - val_accuracy: 0.7500\n",
      "Epoch 73/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.5576 - val_accuracy: 0.7500\n",
      "Epoch 74/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.7371 - val_accuracy: 0.7500\n",
      "Epoch 75/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0088 - accuracy: 0.9964 - val_loss: 1.6808 - val_accuracy: 0.7500\n",
      "Epoch 76/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0117 - accuracy: 0.9982 - val_loss: 2.4338 - val_accuracy: 0.6500\n",
      "Epoch 77/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0245 - accuracy: 0.9928 - val_loss: 1.9842 - val_accuracy: 0.6333\n",
      "Epoch 78/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0119 - accuracy: 0.9964 - val_loss: 1.4439 - val_accuracy: 0.7333\n",
      "Epoch 79/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0148 - accuracy: 0.9964 - val_loss: 1.8170 - val_accuracy: 0.6833\n",
      "Epoch 80/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0141 - accuracy: 0.9910 - val_loss: 1.9514 - val_accuracy: 0.6333\n",
      "Epoch 81/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0059 - accuracy: 0.9982 - val_loss: 2.0138 - val_accuracy: 0.6333\n",
      "Epoch 82/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0080 - accuracy: 0.9982 - val_loss: 2.0183 - val_accuracy: 0.6500\n",
      "Epoch 83/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0209 - accuracy: 0.9910 - val_loss: 1.9459 - val_accuracy: 0.6167\n",
      "Epoch 84/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0093 - accuracy: 0.9982 - val_loss: 2.4679 - val_accuracy: 0.6167\n",
      "Epoch 85/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0193 - accuracy: 0.9946 - val_loss: 1.5294 - val_accuracy: 0.6333\n",
      "Epoch 86/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.5540 - val_accuracy: 0.7000\n",
      "Epoch 87/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0187 - accuracy: 0.9964 - val_loss: 2.5407 - val_accuracy: 0.5167\n",
      "Epoch 88/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0139 - accuracy: 0.9964 - val_loss: 1.7206 - val_accuracy: 0.6833\n",
      "Epoch 89/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.0522 - val_accuracy: 0.6500\n",
      "Epoch 90/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0277 - accuracy: 0.9892 - val_loss: 2.3125 - val_accuracy: 0.5333\n",
      "Epoch 91/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0177 - accuracy: 0.9946 - val_loss: 1.5699 - val_accuracy: 0.7167\n",
      "Epoch 92/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0110 - accuracy: 0.9946 - val_loss: 1.5843 - val_accuracy: 0.6833\n",
      "Epoch 93/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.7141 - val_accuracy: 0.6667\n",
      "Epoch 94/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0081 - accuracy: 0.9964 - val_loss: 1.5311 - val_accuracy: 0.6667\n",
      "Epoch 95/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0069 - accuracy: 0.9964 - val_loss: 1.7340 - val_accuracy: 0.7000\n",
      "Epoch 96/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0067 - accuracy: 0.9982 - val_loss: 1.8729 - val_accuracy: 0.6667\n",
      "Epoch 97/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.0348 - val_accuracy: 0.6500\n",
      "Epoch 98/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 7.2937e-04 - accuracy: 1.0000 - val_loss: 2.0135 - val_accuracy: 0.6667\n",
      "Epoch 99/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0077 - accuracy: 0.9946 - val_loss: 1.9125 - val_accuracy: 0.6667\n",
      "Epoch 100/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 6.7115e-04 - accuracy: 1.0000 - val_loss: 2.7132 - val_accuracy: 0.6833\n",
      "accuracy for model 9 is 68.33333373069763\n",
      "(617, 3)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 192)               119655936 \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 119,691,683\n",
      "Trainable params: 119,691,619\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 557 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      "557/557 [==============================] - 8s 15ms/sample - loss: 1.0257 - accuracy: 0.4865 - val_loss: 6.6695 - val_accuracy: 0.6000\n",
      "Epoch 2/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.7514 - accuracy: 0.7074 - val_loss: 3.8829 - val_accuracy: 0.6000\n",
      "Epoch 3/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.6962 - accuracy: 0.6984 - val_loss: 2.5751 - val_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.6453 - accuracy: 0.7217 - val_loss: 1.8522 - val_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.5671 - accuracy: 0.7630 - val_loss: 1.8050 - val_accuracy: 0.3833\n",
      "Epoch 6/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.5025 - accuracy: 0.8025 - val_loss: 2.0069 - val_accuracy: 0.3000\n",
      "Epoch 7/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.4177 - accuracy: 0.8492 - val_loss: 1.9706 - val_accuracy: 0.2833\n",
      "Epoch 8/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.3444 - accuracy: 0.8779 - val_loss: 1.0470 - val_accuracy: 0.5667\n",
      "Epoch 9/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.2723 - accuracy: 0.9120 - val_loss: 1.0258 - val_accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1899 - accuracy: 0.9390 - val_loss: 1.3599 - val_accuracy: 0.6333\n",
      "Epoch 11/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1666 - accuracy: 0.9479 - val_loss: 1.3160 - val_accuracy: 0.5667\n",
      "Epoch 12/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.1105 - accuracy: 0.9731 - val_loss: 1.5240 - val_accuracy: 0.5667\n",
      "Epoch 13/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0941 - accuracy: 0.9695 - val_loss: 2.3392 - val_accuracy: 0.4000\n",
      "Epoch 14/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0633 - accuracy: 0.9874 - val_loss: 1.4132 - val_accuracy: 0.6500\n",
      "Epoch 15/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0456 - accuracy: 0.9928 - val_loss: 1.3820 - val_accuracy: 0.6667\n",
      "Epoch 16/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0427 - accuracy: 0.9910 - val_loss: 1.6173 - val_accuracy: 0.6167\n",
      "Epoch 17/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0370 - accuracy: 0.9892 - val_loss: 2.3749 - val_accuracy: 0.4500\n",
      "Epoch 18/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0534 - accuracy: 0.9874 - val_loss: 5.5867 - val_accuracy: 0.2333\n",
      "Epoch 19/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0359 - accuracy: 0.9910 - val_loss: 1.4416 - val_accuracy: 0.6000\n",
      "Epoch 20/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0371 - accuracy: 0.9928 - val_loss: 1.7946 - val_accuracy: 0.5500\n",
      "Epoch 21/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0396 - accuracy: 0.9874 - val_loss: 1.6775 - val_accuracy: 0.5500\n",
      "Epoch 22/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0229 - accuracy: 0.9946 - val_loss: 1.6245 - val_accuracy: 0.6500\n",
      "Epoch 23/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0275 - accuracy: 0.9964 - val_loss: 1.5309 - val_accuracy: 0.6333\n",
      "Epoch 24/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0179 - accuracy: 0.9946 - val_loss: 2.1931 - val_accuracy: 0.5167\n",
      "Epoch 25/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0175 - accuracy: 0.9982 - val_loss: 1.5492 - val_accuracy: 0.6667\n",
      "Epoch 26/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0318 - accuracy: 0.9910 - val_loss: 3.8794 - val_accuracy: 0.3000\n",
      "Epoch 27/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0320 - accuracy: 0.9946 - val_loss: 2.0093 - val_accuracy: 0.5333\n",
      "Epoch 28/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0277 - accuracy: 0.9928 - val_loss: 2.0258 - val_accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0175 - accuracy: 0.9964 - val_loss: 2.3923 - val_accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0099 - accuracy: 0.9964 - val_loss: 1.7981 - val_accuracy: 0.6167\n",
      "Epoch 31/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0151 - accuracy: 0.9964 - val_loss: 1.8837 - val_accuracy: 0.6000\n",
      "Epoch 32/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0108 - accuracy: 0.9982 - val_loss: 2.0432 - val_accuracy: 0.6500\n",
      "Epoch 33/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0142 - accuracy: 0.9964 - val_loss: 2.2482 - val_accuracy: 0.5833\n",
      "Epoch 34/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0204 - accuracy: 0.9946 - val_loss: 2.8242 - val_accuracy: 0.4333\n",
      "Epoch 35/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0126 - accuracy: 0.9982 - val_loss: 3.6245 - val_accuracy: 0.3833\n",
      "Epoch 36/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0136 - accuracy: 0.9982 - val_loss: 1.9384 - val_accuracy: 0.6167\n",
      "Epoch 37/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0112 - accuracy: 0.9964 - val_loss: 2.3054 - val_accuracy: 0.5500\n",
      "Epoch 38/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0083 - accuracy: 0.9982 - val_loss: 2.0998 - val_accuracy: 0.5833\n",
      "Epoch 39/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0098 - accuracy: 0.9964 - val_loss: 2.0939 - val_accuracy: 0.6000\n",
      "Epoch 40/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0053 - accuracy: 0.9982 - val_loss: 2.0100 - val_accuracy: 0.5667\n",
      "Epoch 41/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0169 - accuracy: 0.9946 - val_loss: 2.3028 - val_accuracy: 0.5500\n",
      "Epoch 42/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0125 - accuracy: 0.9982 - val_loss: 1.9943 - val_accuracy: 0.6000\n",
      "Epoch 43/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0063 - accuracy: 0.9982 - val_loss: 1.9010 - val_accuracy: 0.6833\n",
      "Epoch 44/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0057 - accuracy: 0.9982 - val_loss: 2.1066 - val_accuracy: 0.6000\n",
      "Epoch 45/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.3488 - val_accuracy: 0.5667\n",
      "Epoch 46/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0049 - accuracy: 0.9982 - val_loss: 2.8019 - val_accuracy: 0.5167\n",
      "Epoch 47/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0077 - accuracy: 0.9982 - val_loss: 2.4826 - val_accuracy: 0.5667\n",
      "Epoch 48/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.1873 - val_accuracy: 0.6167\n",
      "Epoch 49/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.3140 - val_accuracy: 0.5833\n",
      "Epoch 50/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0036 - accuracy: 0.9982 - val_loss: 2.6413 - val_accuracy: 0.5500\n",
      "Epoch 51/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.6691 - val_accuracy: 0.5500\n",
      "Epoch 52/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.1811 - val_accuracy: 0.6167\n",
      "Epoch 53/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0048 - accuracy: 0.9982 - val_loss: 2.6133 - val_accuracy: 0.5833\n",
      "Epoch 54/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0110 - accuracy: 0.9964 - val_loss: 2.1483 - val_accuracy: 0.5833\n",
      "Epoch 55/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.2618 - val_accuracy: 0.6167\n",
      "Epoch 56/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0036 - accuracy: 0.9982 - val_loss: 2.3609 - val_accuracy: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0167 - accuracy: 0.9964 - val_loss: 2.9250 - val_accuracy: 0.5500\n",
      "Epoch 58/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0194 - accuracy: 0.9964 - val_loss: 2.4048 - val_accuracy: 0.6500\n",
      "Epoch 59/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0064 - accuracy: 0.9982 - val_loss: 3.1841 - val_accuracy: 0.5500\n",
      "Epoch 60/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0128 - accuracy: 0.9964 - val_loss: 2.7745 - val_accuracy: 0.5667\n",
      "Epoch 61/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.3271 - val_accuracy: 0.6167\n",
      "Epoch 62/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.3573 - val_accuracy: 0.5833\n",
      "Epoch 63/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.5133 - val_accuracy: 0.5833\n",
      "Epoch 64/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.6513 - val_accuracy: 0.6167\n",
      "Epoch 65/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.3797 - val_accuracy: 0.5833\n",
      "Epoch 66/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.6839 - val_accuracy: 0.5667\n",
      "Epoch 67/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.7252 - val_accuracy: 0.6000\n",
      "Epoch 68/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0083 - accuracy: 0.9964 - val_loss: 2.5696 - val_accuracy: 0.5833\n",
      "Epoch 69/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0032 - accuracy: 0.9982 - val_loss: 5.0858 - val_accuracy: 0.4000\n",
      "Epoch 70/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.3825 - val_accuracy: 0.5333\n",
      "Epoch 71/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.5546 - val_accuracy: 0.6167\n",
      "Epoch 72/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0138 - accuracy: 0.9964 - val_loss: 2.1781 - val_accuracy: 0.6167\n",
      "Epoch 73/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0069 - accuracy: 0.9982 - val_loss: 2.5412 - val_accuracy: 0.5833\n",
      "Epoch 74/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.8548 - val_accuracy: 0.5500\n",
      "Epoch 75/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.5751 - val_accuracy: 0.6000\n",
      "Epoch 76/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.9010 - val_accuracy: 0.5833\n",
      "Epoch 77/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0062 - accuracy: 0.9964 - val_loss: 2.4431 - val_accuracy: 0.6667\n",
      "Epoch 78/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.6967 - val_accuracy: 0.6167\n",
      "Epoch 79/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.4265 - val_accuracy: 0.5167\n",
      "Epoch 80/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0034 - accuracy: 0.9982 - val_loss: 2.7294 - val_accuracy: 0.5833\n",
      "Epoch 81/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.3334 - val_accuracy: 0.6500\n",
      "Epoch 82/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 9.5766e-04 - accuracy: 1.0000 - val_loss: 2.3141 - val_accuracy: 0.6833\n",
      "Epoch 83/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.4437 - val_accuracy: 0.6333\n",
      "Epoch 84/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 3.2133e-04 - accuracy: 1.0000 - val_loss: 2.6278 - val_accuracy: 0.6000\n",
      "Epoch 85/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 8.5372e-04 - accuracy: 1.0000 - val_loss: 2.8751 - val_accuracy: 0.5667\n",
      "Epoch 86/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0080 - accuracy: 0.9982 - val_loss: 2.6766 - val_accuracy: 0.6167\n",
      "Epoch 87/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0056 - accuracy: 0.9982 - val_loss: 3.0595 - val_accuracy: 0.5333\n",
      "Epoch 88/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.7995 - val_accuracy: 0.5667\n",
      "Epoch 89/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.2779 - val_accuracy: 0.5667\n",
      "Epoch 90/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 3.8892e-04 - accuracy: 1.0000 - val_loss: 3.2932 - val_accuracy: 0.5500\n",
      "Epoch 91/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0127 - accuracy: 0.9964 - val_loss: 2.7837 - val_accuracy: 0.6333\n",
      "Epoch 92/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0029 - accuracy: 0.9982 - val_loss: 3.8746 - val_accuracy: 0.5167\n",
      "Epoch 93/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0084 - accuracy: 0.9946 - val_loss: 2.5414 - val_accuracy: 0.6500\n",
      "Epoch 94/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.5329 - val_accuracy: 0.5500\n",
      "Epoch 95/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.1241 - val_accuracy: 0.5500\n",
      "Epoch 96/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0068 - accuracy: 0.9982 - val_loss: 4.0456 - val_accuracy: 0.6167\n",
      "Epoch 97/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0037 - accuracy: 0.9964 - val_loss: 2.9360 - val_accuracy: 0.5667\n",
      "Epoch 98/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0063 - accuracy: 0.9982 - val_loss: 2.3900 - val_accuracy: 0.6167\n",
      "Epoch 99/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0026 - accuracy: 0.9982 - val_loss: 2.8329 - val_accuracy: 0.6667\n",
      "Epoch 100/100\n",
      "557/557 [==============================] - 2s 3ms/sample - loss: 0.0153 - accuracy: 0.9928 - val_loss: 3.1024 - val_accuracy: 0.5833\n",
      "accuracy for model 10 is 58.33333134651184\n",
      "Training Testing Accuracy: 75.24% (7.90%)\n"
     ]
    }
   ],
   "source": [
    "best_DNN = eval_dnn(tt_vcf, tt_pheno, 10, mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9b43f49bc18e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_DNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PoC_DNN_model.pickle.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(best_DNN, open(\"PoC_DNN_model.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout accuracy is 70.32257914543152\n"
     ]
    }
   ],
   "source": [
    "bs = ((ho_vcf.shape[0])/40)\n",
    "bs = round(bs)\n",
    "ho_pheno = mlb.transform(ho_pheno)\n",
    "_, accuracy = best_DNN.evaluate(ho_vcf, ho_pheno, batch_size=bs, verbose=0)\n",
    "print(\"Holdout accuracy is \" + str(accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
